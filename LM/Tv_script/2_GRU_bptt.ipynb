{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j1THn1bC6jEF"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from io import open\n",
    "import torch\n",
    "\n",
    "class Dictionary(object):\n",
    "    def __init__(self):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = []\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2idx:\n",
    "            self.idx2word.append(word)\n",
    "            self.word2idx[word] = len(self.idx2word) - 1\n",
    "        return self.word2idx[word]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx2word)\n",
    "\n",
    "\n",
    "class Corpus(object):\n",
    "    def __init__(self, path):\n",
    "        self.dictionary = Dictionary()\n",
    "        self.train = self.tokenize(os.path.join(path, 'train.txt'))\n",
    "\n",
    "    def tokenize(self, path):\n",
    "        \"\"\"Tokenizes a text file.\"\"\"\n",
    "        assert os.path.exists(path)\n",
    "        # Add words to the dictionary\n",
    "        with open(path, 'r', encoding=\"utf8\") as f:\n",
    "            for line in f:\n",
    "                words = line.split() + ['<eos>']\n",
    "                for word in words:\n",
    "                    self.dictionary.add_word(word)\n",
    "\n",
    "        # Tokenize file content\n",
    "        with open(path, 'r', encoding=\"utf8\") as f:\n",
    "            idss = []\n",
    "            for line in f:\n",
    "                words = line.split() + ['<eos>']\n",
    "                ids = []\n",
    "                for word in words:\n",
    "                    ids.append(self.dictionary.word2idx[word])\n",
    "                idss.append(torch.tensor(ids).type(torch.int64))\n",
    "            ids = torch.cat(idss)\n",
    "\n",
    "        return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lapqnCzd6jEQ"
   },
   "outputs": [],
   "source": [
    "corpus = Corpus(path='data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K5Wko5pK6jEV"
   },
   "outputs": [],
   "source": [
    "def batchify(data, bsz):\n",
    "    # Work out how cleanly we can divide the dataset into bsz parts.\n",
    "    nbatch = data.size(0) // bsz\n",
    "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
    "    data = data.narrow(0, 0, nbatch * bsz)\n",
    "    # Evenly divide the data across the bsz batches.\n",
    "    data = data.view(bsz, -1).t().contiguous()\n",
    "    return data.to(device)\n",
    "\n",
    "def get_batch(source, i):\n",
    "    seq_len = min(bptt, len(source) - 1 - i)\n",
    "    data = source[i:i+seq_len]\n",
    "    target = source[i+1:i+1+seq_len].view(-1)\n",
    "    return data, target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KIPGqdT66jEf"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class RNNModel(nn.Module):\n",
    "    \"\"\"Container module with an encoder, a recurrent module, and a decoder.\"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, embed_dim, hid_dim, n_layers, dropout=0.5):\n",
    "        super(RNNModel, self).__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.encoder = nn.Embedding(input_dim, embed_dim)\n",
    "    \n",
    "        self.rnn = nn.GRU(embed_dim,\n",
    "                          hid_dim,\n",
    "                          n_layers,\n",
    "                          dropout = 0 if n_layers < 2 else dropout)\n",
    "        \n",
    "        self.decoder = nn.Linear(hid_dim, input_dim)\n",
    "\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.init_weights()\n",
    "\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, inputs, hidden):\n",
    "        emb = self.drop(self.encoder(inputs))\n",
    "        output, hidden = self.rnn(emb, hidden)\n",
    "        output = self.drop(output)\n",
    "        decoded = self.decoder(output)\n",
    "        decoded = decoded.view(-1, self.input_dim)\n",
    "        return F.log_softmax(decoded, dim=1), hidden\n",
    "\n",
    "    def init_hidden(self, bsz):\n",
    "        weight = next(self.parameters())\n",
    "\n",
    "        return weight.new_zeros(self.n_layers, bsz, self.hid_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6UvK6CF26jEk"
   },
   "outputs": [],
   "source": [
    "vocab_size = len(corpus.dictionary)\n",
    "emb_dim = 200\n",
    "hid_dim = 250\n",
    "n_layers = 2\n",
    "dropout = 0.5\n",
    "eval_batch_size = 10\n",
    "batch_size = 128\n",
    "bptt = 10\n",
    "\n",
    "log_interval = 100\n",
    "\n",
    "model = RNNModel(vocab_size, emb_dim, hid_dim, n_layers, dropout)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2721,
     "status": "ok",
     "timestamp": 1591633205445,
     "user": {
      "displayName": "Daniel Cuiñas Vázquez",
      "photoUrl": "",
      "userId": "11552885780691803973"
     },
     "user_tz": -120
    },
    "id": "k4PPk6cP6jEq",
    "outputId": "f522f8ab-d685-472c-8666-f2464bad9fed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 21,627,468 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1994,
     "status": "ok",
     "timestamp": 1591633205447,
     "user": {
      "displayName": "Daniel Cuiñas Vázquez",
      "photoUrl": "",
      "userId": "11552885780691803973"
     },
     "user_tz": -120
    },
    "id": "-VqKFc1q6jEw",
    "outputId": "57dbffda-0b53-470c-8430-da983208fa5e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LMZAbCH1Enf3"
   },
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "bptt = 10\n",
    "\n",
    "train_loader = batchify(corpus.train, batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2-eTrMxuu2_V"
   },
   "outputs": [],
   "source": [
    "i,t = get_batch(train_loader,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 829,
     "status": "ok",
     "timestamp": 1591633737762,
     "user": {
      "displayName": "Daniel Cuiñas Vázquez",
      "photoUrl": "",
      "userId": "11552885780691803973"
     },
     "user_tz": -120
    },
    "id": "tkWneYPzu3E7",
    "outputId": "8fdb7c3e-8111-43f8-ea80-7aa78acf7bc3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0,   146],\n",
       "        [    1,   209],\n",
       "        [    2,    74],\n",
       "        [    3,   479],\n",
       "        [    4,    13],\n",
       "        [    5,     1],\n",
       "        [    6,   136],\n",
       "        [    7,  1006],\n",
       "        [    8, 27416],\n",
       "        [    1,   111]], device='cuda:0')"
      ]
     },
     "execution_count": 29,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 844,
     "status": "ok",
     "timestamp": 1591633740623,
     "user": {
      "displayName": "Daniel Cuiñas Vázquez",
      "photoUrl": "",
      "userId": "11552885780691803973"
     },
     "user_tz": -120
    },
    "id": "u79w0yxUu3NQ",
    "outputId": "556fd9fe-01d9-4b59-df6a-823e41d93571"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    1,   209,     2,    74,     3,   479,     4,    13,     5,     1,\n",
       "            6,   136,     7,  1006,     8, 27416,     1,   111,     2,   111],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 30,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ufa0_QGF6jE0"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "lr = 0.001\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o0c4vDJM6jE4"
   },
   "outputs": [],
   "source": [
    "model=model.to(device)\n",
    "criterion=criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cZB8W44P6jFE"
   },
   "outputs": [],
   "source": [
    "def train(model, iterator, criterion):\n",
    "    clip = 0.25\n",
    "    total_loss = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    hidden = model.init_hidden(batch_size)\n",
    "    \n",
    "    \n",
    "    for batch, i in enumerate(range(0, iterator.size(0) - 1, bptt)):\n",
    "        data, targets = get_batch(iterator, i)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        hidden = hidden.detach()\n",
    "      \n",
    "        output, hidden = model(data, hidden)    \n",
    "        \n",
    "        loss = criterion(output, targets)\n",
    "                \n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += len(data)*loss.item()\n",
    "        \n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            cur_loss = total_loss / log_interval\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.2f} | '\n",
    "                    'loss {:5.2f} | ppl {:8.2f}'.format(\n",
    "                epoch, batch, len(iterator) // bptt, lr,\n",
    "                cur_loss, math.exp(cur_loss)))\n",
    "            # total_loss = 0\n",
    "        return total_loss / (len(iterator) - 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-56ZmxmF6jFI"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    total_loss = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    hidden = model.init_hidden(eval_batch_size)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for i in range(0, iterator.size(0) - 1, bptt):\n",
    "            data, targets = get_batch(iterator, i)\n",
    "            output, hidden = model(data, hidden)\n",
    "            hidden = hidden.detach()\n",
    "            \n",
    "            total_loss += len(data) * criterion(output, targets).item()\n",
    "\n",
    "        \n",
    "    return total_loss / (len(iterator) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "isTs0LXT6jFL"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2519693,
     "status": "ok",
     "timestamp": 1591550498324,
     "user": {
      "displayName": "Daniel Cuiñas Vázquez",
      "photoUrl": "",
      "userId": "11552885780691803973"
     },
     "user_tz": -120
    },
    "id": "YT0NG4yW6jFP",
    "outputId": "e2bbbf78-8daa-445a-9869-9fa9762182a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   0 |   100/  558 batches | lr 0.00 | loss  7.23 | ppl  1373.41\n",
      "| epoch   0 |   200/  558 batches | lr 0.00 | loss  6.15 | ppl   470.69\n",
      "| epoch   0 |   300/  558 batches | lr 0.00 | loss  5.94 | ppl   378.06\n",
      "| epoch   0 |   400/  558 batches | lr 0.00 | loss  5.82 | ppl   338.20\n",
      "| epoch   0 |   500/  558 batches | lr 0.00 | loss  5.69 | ppl   297.06\n",
      "Epoch: 01 | Epoch Time: 0m 49s\n",
      "| epoch   1 |   100/  558 batches | lr 0.00 | loss  5.53 | ppl   253.35\n",
      "| epoch   1 |   200/  558 batches | lr 0.00 | loss  5.44 | ppl   231.24\n",
      "| epoch   1 |   300/  558 batches | lr 0.00 | loss  5.38 | ppl   217.16\n",
      "| epoch   1 |   400/  558 batches | lr 0.00 | loss  5.37 | ppl   215.16\n",
      "| epoch   1 |   500/  558 batches | lr 0.00 | loss  5.32 | ppl   203.41\n",
      "Epoch: 02 | Epoch Time: 0m 50s\n",
      "| epoch   2 |   100/  558 batches | lr 0.00 | loss  5.26 | ppl   191.89\n",
      "| epoch   2 |   200/  558 batches | lr 0.00 | loss  5.19 | ppl   179.65\n",
      "| epoch   2 |   300/  558 batches | lr 0.00 | loss  5.15 | ppl   171.74\n",
      "| epoch   2 |   400/  558 batches | lr 0.00 | loss  5.15 | ppl   172.85\n",
      "| epoch   2 |   500/  558 batches | lr 0.00 | loss  5.11 | ppl   166.12\n",
      "Epoch: 03 | Epoch Time: 0m 50s\n",
      "| epoch   3 |   100/  558 batches | lr 0.00 | loss  5.08 | ppl   160.67\n",
      "| epoch   3 |   200/  558 batches | lr 0.00 | loss  5.03 | ppl   152.59\n",
      "| epoch   3 |   300/  558 batches | lr 0.00 | loss  4.99 | ppl   146.32\n",
      "| epoch   3 |   400/  558 batches | lr 0.00 | loss  4.99 | ppl   147.37\n",
      "| epoch   3 |   500/  558 batches | lr 0.00 | loss  4.96 | ppl   142.08\n",
      "Epoch: 04 | Epoch Time: 0m 50s\n",
      "| epoch   4 |   100/  558 batches | lr 0.00 | loss  4.94 | ppl   140.33\n",
      "| epoch   4 |   200/  558 batches | lr 0.00 | loss  4.89 | ppl   133.02\n",
      "| epoch   4 |   300/  558 batches | lr 0.00 | loss  4.86 | ppl   128.71\n",
      "| epoch   4 |   400/  558 batches | lr 0.00 | loss  4.87 | ppl   129.72\n",
      "| epoch   4 |   500/  558 batches | lr 0.00 | loss  4.84 | ppl   126.17\n",
      "Epoch: 05 | Epoch Time: 0m 50s\n",
      "| epoch   5 |   100/  558 batches | lr 0.00 | loss  4.83 | ppl   125.42\n",
      "| epoch   5 |   200/  558 batches | lr 0.00 | loss  4.77 | ppl   118.35\n",
      "| epoch   5 |   300/  558 batches | lr 0.00 | loss  4.75 | ppl   115.26\n",
      "| epoch   5 |   400/  558 batches | lr 0.00 | loss  4.75 | ppl   116.13\n",
      "| epoch   5 |   500/  558 batches | lr 0.00 | loss  4.73 | ppl   113.08\n",
      "Epoch: 06 | Epoch Time: 0m 50s\n",
      "| epoch   6 |   100/  558 batches | lr 0.00 | loss  4.73 | ppl   113.37\n",
      "| epoch   6 |   200/  558 batches | lr 0.00 | loss  4.67 | ppl   106.98\n",
      "| epoch   6 |   300/  558 batches | lr 0.00 | loss  4.65 | ppl   104.43\n",
      "| epoch   6 |   400/  558 batches | lr 0.00 | loss  4.66 | ppl   105.12\n",
      "| epoch   6 |   500/  558 batches | lr 0.00 | loss  4.63 | ppl   102.69\n",
      "Epoch: 07 | Epoch Time: 0m 50s\n",
      "| epoch   7 |   100/  558 batches | lr 0.00 | loss  4.64 | ppl   103.36\n",
      "| epoch   7 |   200/  558 batches | lr 0.00 | loss  4.58 | ppl    97.99\n",
      "| epoch   7 |   300/  558 batches | lr 0.00 | loss  4.56 | ppl    95.58\n",
      "| epoch   7 |   400/  558 batches | lr 0.00 | loss  4.56 | ppl    95.85\n",
      "| epoch   7 |   500/  558 batches | lr 0.00 | loss  4.54 | ppl    93.88\n",
      "Epoch: 08 | Epoch Time: 0m 50s\n",
      "| epoch   8 |   100/  558 batches | lr 0.00 | loss  4.56 | ppl    95.94\n",
      "| epoch   8 |   200/  558 batches | lr 0.00 | loss  4.50 | ppl    90.05\n",
      "| epoch   8 |   300/  558 batches | lr 0.00 | loss  4.47 | ppl    87.43\n",
      "| epoch   8 |   400/  558 batches | lr 0.00 | loss  4.48 | ppl    88.47\n",
      "| epoch   8 |   500/  558 batches | lr 0.00 | loss  4.46 | ppl    86.64\n",
      "Epoch: 09 | Epoch Time: 0m 50s\n",
      "| epoch   9 |   100/  558 batches | lr 0.00 | loss  4.49 | ppl    88.97\n",
      "| epoch   9 |   200/  558 batches | lr 0.00 | loss  4.42 | ppl    83.34\n",
      "| epoch   9 |   300/  558 batches | lr 0.00 | loss  4.40 | ppl    81.50\n",
      "| epoch   9 |   400/  558 batches | lr 0.00 | loss  4.41 | ppl    81.88\n",
      "| epoch   9 |   500/  558 batches | lr 0.00 | loss  4.40 | ppl    81.13\n",
      "Epoch: 10 | Epoch Time: 0m 50s\n",
      "| epoch  10 |   100/  558 batches | lr 0.00 | loss  4.42 | ppl    82.96\n",
      "| epoch  10 |   200/  558 batches | lr 0.00 | loss  4.36 | ppl    77.97\n",
      "| epoch  10 |   300/  558 batches | lr 0.00 | loss  4.33 | ppl    76.23\n",
      "| epoch  10 |   400/  558 batches | lr 0.00 | loss  4.34 | ppl    76.75\n",
      "| epoch  10 |   500/  558 batches | lr 0.00 | loss  4.32 | ppl    75.54\n",
      "Epoch: 11 | Epoch Time: 0m 50s\n",
      "| epoch  11 |   100/  558 batches | lr 0.00 | loss  4.35 | ppl    77.15\n",
      "| epoch  11 |   200/  558 batches | lr 0.00 | loss  4.29 | ppl    72.74\n",
      "| epoch  11 |   300/  558 batches | lr 0.00 | loss  4.27 | ppl    71.30\n",
      "| epoch  11 |   400/  558 batches | lr 0.00 | loss  4.27 | ppl    71.85\n",
      "| epoch  11 |   500/  558 batches | lr 0.00 | loss  4.26 | ppl    71.15\n",
      "Epoch: 12 | Epoch Time: 0m 50s\n",
      "| epoch  12 |   100/  558 batches | lr 0.00 | loss  4.29 | ppl    73.20\n",
      "| epoch  12 |   200/  558 batches | lr 0.00 | loss  4.23 | ppl    68.75\n",
      "| epoch  12 |   300/  558 batches | lr 0.00 | loss  4.21 | ppl    67.40\n",
      "| epoch  12 |   400/  558 batches | lr 0.00 | loss  4.22 | ppl    67.88\n",
      "| epoch  12 |   500/  558 batches | lr 0.00 | loss  4.21 | ppl    67.48\n",
      "Epoch: 13 | Epoch Time: 0m 50s\n",
      "| epoch  13 |   100/  558 batches | lr 0.00 | loss  4.23 | ppl    69.02\n",
      "| epoch  13 |   200/  558 batches | lr 0.00 | loss  4.18 | ppl    65.11\n",
      "| epoch  13 |   300/  558 batches | lr 0.00 | loss  4.16 | ppl    63.97\n",
      "| epoch  13 |   400/  558 batches | lr 0.00 | loss  4.16 | ppl    64.37\n",
      "| epoch  13 |   500/  558 batches | lr 0.00 | loss  4.17 | ppl    64.40\n",
      "Epoch: 14 | Epoch Time: 0m 50s\n",
      "| epoch  14 |   100/  558 batches | lr 0.00 | loss  4.18 | ppl    65.41\n",
      "| epoch  14 |   200/  558 batches | lr 0.00 | loss  4.13 | ppl    61.94\n",
      "| epoch  14 |   300/  558 batches | lr 0.00 | loss  4.11 | ppl    61.22\n",
      "| epoch  14 |   400/  558 batches | lr 0.00 | loss  4.12 | ppl    61.30\n",
      "| epoch  14 |   500/  558 batches | lr 0.00 | loss  4.11 | ppl    60.92\n",
      "Epoch: 15 | Epoch Time: 0m 50s\n",
      "| epoch  15 |   100/  558 batches | lr 0.00 | loss  4.14 | ppl    62.60\n",
      "| epoch  15 |   200/  558 batches | lr 0.00 | loss  4.08 | ppl    59.08\n",
      "| epoch  15 |   300/  558 batches | lr 0.00 | loss  4.06 | ppl    58.07\n",
      "| epoch  15 |   400/  558 batches | lr 0.00 | loss  4.07 | ppl    58.59\n",
      "| epoch  15 |   500/  558 batches | lr 0.00 | loss  4.06 | ppl    58.17\n",
      "Epoch: 16 | Epoch Time: 0m 50s\n",
      "| epoch  16 |   100/  558 batches | lr 0.00 | loss  4.09 | ppl    59.82\n",
      "| epoch  16 |   200/  558 batches | lr 0.00 | loss  4.04 | ppl    56.68\n",
      "| epoch  16 |   300/  558 batches | lr 0.00 | loss  4.02 | ppl    55.73\n",
      "| epoch  16 |   400/  558 batches | lr 0.00 | loss  4.02 | ppl    55.95\n",
      "| epoch  16 |   500/  558 batches | lr 0.00 | loss  4.03 | ppl    56.45\n",
      "Epoch: 17 | Epoch Time: 0m 50s\n",
      "| epoch  17 |   100/  558 batches | lr 0.00 | loss  4.05 | ppl    57.31\n",
      "| epoch  17 |   200/  558 batches | lr 0.00 | loss  3.99 | ppl    54.32\n",
      "| epoch  17 |   300/  558 batches | lr 0.00 | loss  3.99 | ppl    53.93\n",
      "| epoch  17 |   400/  558 batches | lr 0.00 | loss  3.99 | ppl    54.04\n",
      "| epoch  17 |   500/  558 batches | lr 0.00 | loss  3.99 | ppl    53.89\n",
      "Epoch: 18 | Epoch Time: 0m 50s\n",
      "| epoch  18 |   100/  558 batches | lr 0.00 | loss  4.01 | ppl    55.41\n",
      "| epoch  18 |   200/  558 batches | lr 0.00 | loss  3.96 | ppl    52.51\n",
      "| epoch  18 |   300/  558 batches | lr 0.00 | loss  3.95 | ppl    51.68\n",
      "| epoch  18 |   400/  558 batches | lr 0.00 | loss  3.96 | ppl    52.20\n",
      "| epoch  18 |   500/  558 batches | lr 0.00 | loss  3.95 | ppl    52.03\n",
      "Epoch: 19 | Epoch Time: 0m 50s\n",
      "| epoch  19 |   100/  558 batches | lr 0.00 | loss  3.99 | ppl    53.92\n",
      "| epoch  19 |   200/  558 batches | lr 0.00 | loss  3.93 | ppl    50.70\n",
      "| epoch  19 |   300/  558 batches | lr 0.00 | loss  3.91 | ppl    50.12\n",
      "| epoch  19 |   400/  558 batches | lr 0.00 | loss  3.93 | ppl    50.68\n",
      "| epoch  19 |   500/  558 batches | lr 0.00 | loss  3.92 | ppl    50.64\n",
      "Epoch: 20 | Epoch Time: 0m 50s\n",
      "| epoch  20 |   100/  558 batches | lr 0.00 | loss  3.95 | ppl    52.18\n",
      "| epoch  20 |   200/  558 batches | lr 0.00 | loss  3.90 | ppl    49.24\n",
      "| epoch  20 |   300/  558 batches | lr 0.00 | loss  3.89 | ppl    48.84\n",
      "| epoch  20 |   400/  558 batches | lr 0.00 | loss  3.89 | ppl    48.88\n",
      "| epoch  20 |   500/  558 batches | lr 0.00 | loss  3.90 | ppl    49.23\n",
      "Epoch: 21 | Epoch Time: 0m 50s\n",
      "| epoch  21 |   100/  558 batches | lr 0.00 | loss  3.92 | ppl    50.39\n",
      "| epoch  21 |   200/  558 batches | lr 0.00 | loss  3.87 | ppl    47.77\n",
      "| epoch  21 |   300/  558 batches | lr 0.00 | loss  3.86 | ppl    47.34\n",
      "| epoch  21 |   400/  558 batches | lr 0.00 | loss  3.86 | ppl    47.55\n",
      "| epoch  21 |   500/  558 batches | lr 0.00 | loss  3.87 | ppl    47.83\n",
      "Epoch: 22 | Epoch Time: 0m 50s\n",
      "| epoch  22 |   100/  558 batches | lr 0.00 | loss  3.89 | ppl    49.05\n",
      "| epoch  22 |   200/  558 batches | lr 0.00 | loss  3.83 | ppl    46.19\n",
      "| epoch  22 |   300/  558 batches | lr 0.00 | loss  3.83 | ppl    45.89\n",
      "| epoch  22 |   400/  558 batches | lr 0.00 | loss  3.84 | ppl    46.37\n",
      "| epoch  22 |   500/  558 batches | lr 0.00 | loss  3.84 | ppl    46.69\n",
      "Epoch: 23 | Epoch Time: 0m 50s\n",
      "| epoch  23 |   100/  558 batches | lr 0.00 | loss  3.87 | ppl    47.75\n",
      "| epoch  23 |   200/  558 batches | lr 0.00 | loss  3.81 | ppl    45.25\n",
      "| epoch  23 |   300/  558 batches | lr 0.00 | loss  3.80 | ppl    44.80\n",
      "| epoch  23 |   400/  558 batches | lr 0.00 | loss  3.82 | ppl    45.41\n",
      "| epoch  23 |   500/  558 batches | lr 0.00 | loss  3.81 | ppl    45.34\n",
      "Epoch: 24 | Epoch Time: 0m 50s\n",
      "| epoch  24 |   100/  558 batches | lr 0.00 | loss  3.84 | ppl    46.69\n",
      "| epoch  24 |   200/  558 batches | lr 0.00 | loss  3.79 | ppl    44.22\n",
      "| epoch  24 |   300/  558 batches | lr 0.00 | loss  3.78 | ppl    43.93\n",
      "| epoch  24 |   400/  558 batches | lr 0.00 | loss  3.79 | ppl    44.21\n",
      "| epoch  24 |   500/  558 batches | lr 0.00 | loss  3.80 | ppl    44.69\n",
      "Epoch: 25 | Epoch Time: 0m 50s\n",
      "| epoch  25 |   100/  558 batches | lr 0.00 | loss  3.83 | ppl    46.04\n",
      "| epoch  25 |   200/  558 batches | lr 0.00 | loss  3.77 | ppl    43.17\n",
      "| epoch  25 |   300/  558 batches | lr 0.00 | loss  3.76 | ppl    43.02\n",
      "| epoch  25 |   400/  558 batches | lr 0.00 | loss  3.77 | ppl    43.33\n",
      "| epoch  25 |   500/  558 batches | lr 0.00 | loss  3.77 | ppl    43.43\n",
      "Epoch: 26 | Epoch Time: 0m 50s\n",
      "| epoch  26 |   100/  558 batches | lr 0.00 | loss  3.81 | ppl    44.98\n",
      "| epoch  26 |   200/  558 batches | lr 0.00 | loss  3.74 | ppl    42.13\n",
      "| epoch  26 |   300/  558 batches | lr 0.00 | loss  3.74 | ppl    42.18\n",
      "| epoch  26 |   400/  558 batches | lr 0.00 | loss  3.75 | ppl    42.41\n",
      "| epoch  26 |   500/  558 batches | lr 0.00 | loss  3.75 | ppl    42.68\n",
      "Epoch: 27 | Epoch Time: 0m 50s\n",
      "| epoch  27 |   100/  558 batches | lr 0.00 | loss  3.79 | ppl    44.06\n",
      "| epoch  27 |   200/  558 batches | lr 0.00 | loss  3.73 | ppl    41.54\n",
      "| epoch  27 |   300/  558 batches | lr 0.00 | loss  3.72 | ppl    41.41\n",
      "| epoch  27 |   400/  558 batches | lr 0.00 | loss  3.73 | ppl    41.83\n",
      "| epoch  27 |   500/  558 batches | lr 0.00 | loss  3.74 | ppl    42.06\n",
      "Epoch: 28 | Epoch Time: 0m 50s\n",
      "| epoch  28 |   100/  558 batches | lr 0.00 | loss  3.77 | ppl    43.54\n",
      "| epoch  28 |   200/  558 batches | lr 0.00 | loss  3.70 | ppl    40.64\n",
      "| epoch  28 |   300/  558 batches | lr 0.00 | loss  3.70 | ppl    40.50\n",
      "| epoch  28 |   400/  558 batches | lr 0.00 | loss  3.72 | ppl    41.12\n",
      "| epoch  28 |   500/  558 batches | lr 0.00 | loss  3.72 | ppl    41.43\n",
      "Epoch: 29 | Epoch Time: 0m 50s\n",
      "| epoch  29 |   100/  558 batches | lr 0.00 | loss  3.75 | ppl    42.65\n",
      "| epoch  29 |   200/  558 batches | lr 0.00 | loss  3.69 | ppl    40.21\n",
      "| epoch  29 |   300/  558 batches | lr 0.00 | loss  3.69 | ppl    39.98\n",
      "| epoch  29 |   400/  558 batches | lr 0.00 | loss  3.70 | ppl    40.51\n",
      "| epoch  29 |   500/  558 batches | lr 0.00 | loss  3.70 | ppl    40.62\n",
      "Epoch: 30 | Epoch Time: 0m 50s\n",
      "| epoch  30 |   100/  558 batches | lr 0.00 | loss  3.74 | ppl    41.99\n",
      "| epoch  30 |   200/  558 batches | lr 0.00 | loss  3.68 | ppl    39.71\n",
      "| epoch  30 |   300/  558 batches | lr 0.00 | loss  3.67 | ppl    39.35\n",
      "| epoch  30 |   400/  558 batches | lr 0.00 | loss  3.68 | ppl    39.80\n",
      "| epoch  30 |   500/  558 batches | lr 0.00 | loss  3.69 | ppl    40.11\n",
      "Epoch: 31 | Epoch Time: 0m 50s\n",
      "| epoch  31 |   100/  558 batches | lr 0.00 | loss  3.73 | ppl    41.53\n",
      "| epoch  31 |   200/  558 batches | lr 0.00 | loss  3.67 | ppl    39.11\n",
      "| epoch  31 |   300/  558 batches | lr 0.00 | loss  3.66 | ppl    38.98\n",
      "| epoch  31 |   400/  558 batches | lr 0.00 | loss  3.66 | ppl    39.05\n",
      "| epoch  31 |   500/  558 batches | lr 0.00 | loss  3.67 | ppl    39.35\n",
      "Epoch: 32 | Epoch Time: 0m 50s\n",
      "| epoch  32 |   100/  558 batches | lr 0.00 | loss  3.71 | ppl    40.85\n",
      "| epoch  32 |   200/  558 batches | lr 0.00 | loss  3.65 | ppl    38.55\n",
      "| epoch  32 |   300/  558 batches | lr 0.00 | loss  3.64 | ppl    38.24\n",
      "| epoch  32 |   400/  558 batches | lr 0.00 | loss  3.65 | ppl    38.57\n",
      "| epoch  32 |   500/  558 batches | lr 0.00 | loss  3.66 | ppl    39.02\n",
      "Epoch: 33 | Epoch Time: 0m 50s\n",
      "| epoch  33 |   100/  558 batches | lr 0.00 | loss  3.70 | ppl    40.55\n",
      "| epoch  33 |   200/  558 batches | lr 0.00 | loss  3.63 | ppl    37.89\n",
      "| epoch  33 |   300/  558 batches | lr 0.00 | loss  3.63 | ppl    37.80\n",
      "| epoch  33 |   400/  558 batches | lr 0.00 | loss  3.64 | ppl    38.14\n",
      "| epoch  33 |   500/  558 batches | lr 0.00 | loss  3.65 | ppl    38.44\n",
      "Epoch: 34 | Epoch Time: 0m 50s\n",
      "| epoch  34 |   100/  558 batches | lr 0.00 | loss  3.68 | ppl    39.71\n",
      "| epoch  34 |   200/  558 batches | lr 0.00 | loss  3.62 | ppl    37.36\n",
      "| epoch  34 |   300/  558 batches | lr 0.00 | loss  3.62 | ppl    37.29\n",
      "| epoch  34 |   400/  558 batches | lr 0.00 | loss  3.64 | ppl    37.99\n",
      "| epoch  34 |   500/  558 batches | lr 0.00 | loss  3.64 | ppl    38.09\n",
      "Epoch: 35 | Epoch Time: 0m 50s\n",
      "| epoch  35 |   100/  558 batches | lr 0.00 | loss  3.67 | ppl    39.45\n",
      "| epoch  35 |   200/  558 batches | lr 0.00 | loss  3.61 | ppl    37.11\n",
      "| epoch  35 |   300/  558 batches | lr 0.00 | loss  3.61 | ppl    36.86\n",
      "| epoch  35 |   400/  558 batches | lr 0.00 | loss  3.62 | ppl    37.20\n",
      "| epoch  35 |   500/  558 batches | lr 0.00 | loss  3.63 | ppl    37.70\n",
      "Epoch: 36 | Epoch Time: 0m 50s\n",
      "| epoch  36 |   100/  558 batches | lr 0.00 | loss  3.66 | ppl    38.97\n",
      "| epoch  36 |   200/  558 batches | lr 0.00 | loss  3.60 | ppl    36.53\n",
      "| epoch  36 |   300/  558 batches | lr 0.00 | loss  3.60 | ppl    36.56\n",
      "| epoch  36 |   400/  558 batches | lr 0.00 | loss  3.61 | ppl    37.08\n",
      "| epoch  36 |   500/  558 batches | lr 0.00 | loss  3.62 | ppl    37.27\n",
      "Epoch: 37 | Epoch Time: 0m 50s\n",
      "| epoch  37 |   100/  558 batches | lr 0.00 | loss  3.65 | ppl    38.53\n",
      "| epoch  37 |   200/  558 batches | lr 0.00 | loss  3.59 | ppl    36.23\n",
      "| epoch  37 |   300/  558 batches | lr 0.00 | loss  3.59 | ppl    36.29\n",
      "| epoch  37 |   400/  558 batches | lr 0.00 | loss  3.60 | ppl    36.53\n",
      "| epoch  37 |   500/  558 batches | lr 0.00 | loss  3.61 | ppl    36.86\n",
      "Epoch: 38 | Epoch Time: 0m 50s\n",
      "| epoch  38 |   100/  558 batches | lr 0.00 | loss  3.64 | ppl    38.16\n",
      "| epoch  38 |   200/  558 batches | lr 0.00 | loss  3.58 | ppl    35.91\n",
      "| epoch  38 |   300/  558 batches | lr 0.00 | loss  3.57 | ppl    35.59\n",
      "| epoch  38 |   400/  558 batches | lr 0.00 | loss  3.58 | ppl    35.86\n",
      "| epoch  38 |   500/  558 batches | lr 0.00 | loss  3.59 | ppl    36.25\n",
      "Epoch: 39 | Epoch Time: 0m 50s\n",
      "| epoch  39 |   100/  558 batches | lr 0.00 | loss  3.63 | ppl    37.75\n",
      "| epoch  39 |   200/  558 batches | lr 0.00 | loss  3.57 | ppl    35.43\n",
      "| epoch  39 |   300/  558 batches | lr 0.00 | loss  3.57 | ppl    35.39\n",
      "| epoch  39 |   400/  558 batches | lr 0.00 | loss  3.57 | ppl    35.49\n",
      "| epoch  39 |   500/  558 batches | lr 0.00 | loss  3.59 | ppl    36.17\n",
      "Epoch: 40 | Epoch Time: 0m 50s\n",
      "| epoch  40 |   100/  558 batches | lr 0.00 | loss  3.62 | ppl    37.37\n",
      "| epoch  40 |   200/  558 batches | lr 0.00 | loss  3.56 | ppl    35.24\n",
      "| epoch  40 |   300/  558 batches | lr 0.00 | loss  3.56 | ppl    35.20\n",
      "| epoch  40 |   400/  558 batches | lr 0.00 | loss  3.57 | ppl    35.37\n",
      "| epoch  40 |   500/  558 batches | lr 0.00 | loss  3.58 | ppl    35.86\n",
      "Epoch: 41 | Epoch Time: 0m 50s\n",
      "| epoch  41 |   100/  558 batches | lr 0.00 | loss  3.61 | ppl    36.96\n",
      "| epoch  41 |   200/  558 batches | lr 0.00 | loss  3.55 | ppl    34.91\n",
      "| epoch  41 |   300/  558 batches | lr 0.00 | loss  3.55 | ppl    34.69\n",
      "| epoch  41 |   400/  558 batches | lr 0.00 | loss  3.56 | ppl    35.10\n",
      "| epoch  41 |   500/  558 batches | lr 0.00 | loss  3.57 | ppl    35.65\n",
      "Epoch: 42 | Epoch Time: 0m 49s\n",
      "| epoch  42 |   100/  558 batches | lr 0.00 | loss  3.60 | ppl    36.70\n",
      "| epoch  42 |   200/  558 batches | lr 0.00 | loss  3.54 | ppl    34.50\n",
      "| epoch  42 |   300/  558 batches | lr 0.00 | loss  3.54 | ppl    34.31\n",
      "| epoch  42 |   400/  558 batches | lr 0.00 | loss  3.56 | ppl    35.02\n",
      "| epoch  42 |   500/  558 batches | lr 0.00 | loss  3.56 | ppl    35.31\n",
      "Epoch: 43 | Epoch Time: 0m 49s\n",
      "| epoch  43 |   100/  558 batches | lr 0.00 | loss  3.60 | ppl    36.62\n",
      "| epoch  43 |   200/  558 batches | lr 0.00 | loss  3.53 | ppl    34.24\n",
      "| epoch  43 |   300/  558 batches | lr 0.00 | loss  3.53 | ppl    34.22\n",
      "| epoch  43 |   400/  558 batches | lr 0.00 | loss  3.54 | ppl    34.45\n",
      "| epoch  43 |   500/  558 batches | lr 0.00 | loss  3.56 | ppl    35.03\n",
      "Epoch: 44 | Epoch Time: 0m 49s\n",
      "| epoch  44 |   100/  558 batches | lr 0.00 | loss  3.59 | ppl    36.24\n",
      "| epoch  44 |   200/  558 batches | lr 0.00 | loss  3.53 | ppl    33.99\n",
      "| epoch  44 |   300/  558 batches | lr 0.00 | loss  3.52 | ppl    33.87\n",
      "| epoch  44 |   400/  558 batches | lr 0.00 | loss  3.53 | ppl    34.15\n",
      "| epoch  44 |   500/  558 batches | lr 0.00 | loss  3.54 | ppl    34.60\n",
      "Epoch: 45 | Epoch Time: 0m 49s\n",
      "| epoch  45 |   100/  558 batches | lr 0.00 | loss  3.58 | ppl    35.94\n",
      "| epoch  45 |   200/  558 batches | lr 0.00 | loss  3.52 | ppl    33.82\n",
      "| epoch  45 |   300/  558 batches | lr 0.00 | loss  3.51 | ppl    33.61\n",
      "| epoch  45 |   400/  558 batches | lr 0.00 | loss  3.53 | ppl    33.96\n",
      "| epoch  45 |   500/  558 batches | lr 0.00 | loss  3.54 | ppl    34.32\n",
      "Epoch: 46 | Epoch Time: 0m 49s\n",
      "| epoch  46 |   100/  558 batches | lr 0.00 | loss  3.58 | ppl    35.81\n",
      "| epoch  46 |   200/  558 batches | lr 0.00 | loss  3.51 | ppl    33.38\n",
      "| epoch  46 |   300/  558 batches | lr 0.00 | loss  3.51 | ppl    33.30\n",
      "| epoch  46 |   400/  558 batches | lr 0.00 | loss  3.52 | ppl    33.73\n",
      "| epoch  46 |   500/  558 batches | lr 0.00 | loss  3.53 | ppl    34.13\n",
      "Epoch: 47 | Epoch Time: 0m 49s\n",
      "| epoch  47 |   100/  558 batches | lr 0.00 | loss  3.57 | ppl    35.61\n",
      "| epoch  47 |   200/  558 batches | lr 0.00 | loss  3.50 | ppl    33.13\n",
      "| epoch  47 |   300/  558 batches | lr 0.00 | loss  3.50 | ppl    33.19\n",
      "| epoch  47 |   400/  558 batches | lr 0.00 | loss  3.51 | ppl    33.40\n",
      "| epoch  47 |   500/  558 batches | lr 0.00 | loss  3.52 | ppl    33.75\n",
      "Epoch: 48 | Epoch Time: 0m 49s\n",
      "| epoch  48 |   100/  558 batches | lr 0.00 | loss  3.56 | ppl    35.18\n",
      "| epoch  48 |   200/  558 batches | lr 0.00 | loss  3.50 | ppl    33.05\n",
      "| epoch  48 |   300/  558 batches | lr 0.00 | loss  3.49 | ppl    32.81\n",
      "| epoch  48 |   400/  558 batches | lr 0.00 | loss  3.50 | ppl    33.19\n",
      "| epoch  48 |   500/  558 batches | lr 0.00 | loss  3.51 | ppl    33.52\n",
      "Epoch: 49 | Epoch Time: 0m 49s\n",
      "| epoch  49 |   100/  558 batches | lr 0.00 | loss  3.55 | ppl    34.96\n",
      "| epoch  49 |   200/  558 batches | lr 0.00 | loss  3.48 | ppl    32.59\n",
      "| epoch  49 |   300/  558 batches | lr 0.00 | loss  3.49 | ppl    32.73\n",
      "| epoch  49 |   400/  558 batches | lr 0.00 | loss  3.50 | ppl    32.98\n",
      "| epoch  49 |   500/  558 batches | lr 0.00 | loss  3.51 | ppl    33.45\n",
      "Epoch: 50 | Epoch Time: 0m 49s\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 50\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "counter = 0\n",
    "patience = 2\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train(model, train_loader, criterion)\n",
    "    # valid_loss = evaluate(model, valid_loader, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    # print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):.2f}')\n",
    "\n",
    "    # if valid_loss < best_valid_loss:\n",
    "    #     best_valid_loss = valid_loss\n",
    "    #     #torch.save(model.state_dict(), 'tut2-model.pt')\n",
    "    #     counter = 0 \n",
    "    # else:\n",
    "    #     lr /= 4.0\n",
    "    #     counter += 1\n",
    "    #     if counter >= patience:\n",
    "    #         break\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GY1UfO7l6jFT"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0zu3lUJ86jFX"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "GRU_bptt.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
