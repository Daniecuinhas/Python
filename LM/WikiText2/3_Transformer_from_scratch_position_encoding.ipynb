{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Transformer_torchtext_scratch_position_encoding.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOnWB6v8gtCJL9iUZDvvRAR"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"aV4ez212dFrL","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593258144735,"user_tz":-120,"elapsed":8184,"user":{"displayName":"Daniel Cuiñas Vázquez","photoUrl":"","userId":"11552885780691803973"}}},"source":["import torch\n","from torchtext import data\n","import spacy\n","from spacy.symbols import ORTH\n","from torchtext.datasets import WikiText2\n","\n","my_tok = spacy.load('en')\n"," \n","def spacy_tok(x):\n","    return [tok.text for tok in my_tok.tokenizer(x)]\n"," \n","TEXT = data.Field(lower=True, tokenize=spacy_tok)\n"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"16xGo4uRdOBa","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593258144736,"user_tz":-120,"elapsed":8177,"user":{"displayName":"Daniel Cuiñas Vázquez","photoUrl":"","userId":"11552885780691803973"}}},"source":["my_tok.tokenizer.add_special_case(\"don't\", [{ORTH: \"do\"}, {ORTH: \"n't\"}])"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"adE-xUIIdPjs","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"executionInfo":{"status":"ok","timestamp":1593258165250,"user_tz":-120,"elapsed":28640,"user":{"displayName":"Daniel Cuiñas Vázquez","photoUrl":"","userId":"11552885780691803973"}},"outputId":"86299303-94ca-456b-c5b2-c4fa56331278"},"source":["train, valid, test = WikiText2.splits(TEXT)"],"execution_count":3,"outputs":[{"output_type":"stream","text":["downloading wikitext-2-v1.zip\n"],"name":"stdout"},{"output_type":"stream","text":["wikitext-2-v1.zip: 100%|██████████| 4.48M/4.48M [00:00<00:00, 20.6MB/s]\n"],"name":"stderr"},{"output_type":"stream","text":["extracting\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"WnslyWK5dQ8i","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593258165593,"user_tz":-120,"elapsed":28978,"user":{"displayName":"Daniel Cuiñas Vázquez","photoUrl":"","userId":"11552885780691803973"}}},"source":["TEXT.build_vocab(train)"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"LV13ObSddSoK","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593258165595,"user_tz":-120,"elapsed":28975,"user":{"displayName":"Daniel Cuiñas Vázquez","photoUrl":"","userId":"11552885780691803973"}}},"source":["batch_size = 50\n","bptt = 200"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"5i5oUng3dUIx","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1593258165596,"user_tz":-120,"elapsed":28965,"user":{"displayName":"Daniel Cuiñas Vázquez","photoUrl":"","userId":"11552885780691803973"}},"outputId":"ab3807ef-7dce-401d-8491-ba7f83ee30de"},"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(device)"],"execution_count":6,"outputs":[{"output_type":"stream","text":["cuda\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"sPHoqwA_dVuc","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593258165597,"user_tz":-120,"elapsed":28960,"user":{"displayName":"Daniel Cuiñas Vázquez","photoUrl":"","userId":"11552885780691803973"}}},"source":["train_iter, valid_iter, test_iter = data.BPTTIterator.splits(\n","    (train, valid, test),\n","    batch_size=batch_size,\n","    bptt_len=bptt, \n","    device=device,\n","    repeat=False)"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"MyK0kfyDdXj1","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593258166034,"user_tz":-120,"elapsed":29393,"user":{"displayName":"Daniel Cuiñas Vázquez","photoUrl":"","userId":"11552885780691803973"}}},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import math\n","\n","class PositionalEncoding(nn.Module):\n","    r\"\"\"Inject some information about the relative or absolute position of the tokens\n","        in the sequence. The positional encodings have the same dimension as\n","        the embeddings, so that the two can be summed. Here, we use sine and cosine\n","        functions of different frequencies.\n","    .. math::\n","        \\text{PosEncoder}(pos, 2i) = sin(pos/10000^(2i/d_model))\n","        \\text{PosEncoder}(pos, 2i+1) = cos(pos/10000^(2i/d_model))\n","        \\text{where pos is the word position and i is the embed idx)\n","    Args:\n","        d_model: the embed dim (required).\n","        dropout: the dropout value (default=0.1).\n","        max_len: the max. length of the incoming sequence (default=5000).\n","    Examples:\n","        >>> pos_encoder = PositionalEncoding(d_model)\n","    \"\"\"\n","\n","    def __init__(self, d_model, dropout=0.1, max_len = bptt):\n","        super(PositionalEncoding, self).__init__()\n","        self.dropout = nn.Dropout(p=dropout)\n","\n","        pe = torch.zeros(max_len, d_model)\n","        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n","        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n","        pe[:, 0::2] = torch.sin(position * div_term)\n","        pe[:, 1::2] = torch.cos(position * div_term)\n","        pe = pe.unsqueeze(0).transpose(0, 1)\n","        self.register_buffer('pe', pe)\n","\n","    def forward(self, x):\n","        r\"\"\"Inputs of forward function\n","        Args:\n","            x: the sequence fed to the positional encoder model (required).\n","        Shape:\n","            x: [sequence length, batch size, embed dim]\n","            output: [sequence length, batch size, embed dim]\n","        Examples:\n","            >>> output = pos_encoder(x)\n","        \"\"\"\n","\n","        x = x + self.pe[:x.size(0), :]\n","        return self.dropout(x)\n","\n","\n","class PositionwiseFeedforwardLayer(nn.Module):\n","    def __init__(self, hid_dim, pf_dim, dropout):\n","        super().__init__()\n","        \n","        self.fc_1 = nn.Linear(hid_dim, pf_dim)\n","        self.fc_2 = nn.Linear(pf_dim, hid_dim)\n","        \n","        self.dropout = nn.Dropout(dropout)\n","        \n","    def forward(self, x):\n","        \n","        #x = [batch size, seq len, hid dim]\n","        \n","        x = self.dropout(torch.relu(self.fc_1(x)))\n","        \n","        #x = [batch size, seq len, pf dim]\n","        \n","        x = self.fc_2(x)\n","        \n","        #x = [batch size, seq len, hid dim]\n","        \n","        return x\n","\n","class MultiHeadAttentionLayer(nn.Module):\n","    def __init__(self, hid_dim, n_heads, dropout, device):\n","        super().__init__()\n","        \n","        assert hid_dim % n_heads == 0\n","        \n","        self.hid_dim = hid_dim\n","        self.n_heads = n_heads\n","        self.head_dim = hid_dim // n_heads\n","        \n","        self.fc_q = nn.Linear(hid_dim, hid_dim)\n","        self.fc_k = nn.Linear(hid_dim, hid_dim)\n","        self.fc_v = nn.Linear(hid_dim, hid_dim)\n","        \n","        self.fc_o = nn.Linear(hid_dim, hid_dim)\n","        \n","        self.dropout = nn.Dropout(dropout)\n","        \n","        self.scale = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)\n","        \n","    def forward(self, query, key, value, mask = None):\n","        \n","        batch_size = query.shape[0]\n","        \n","        #query = [batch size, query len, hid dim]\n","        #key = [batch size, key len, hid dim]\n","        #value = [batch size, value len, hid dim]\n","                \n","        Q = self.fc_q(query)\n","        K = self.fc_k(key)\n","        V = self.fc_v(value)\n","        \n","        #Q = [batch size, query len, hid dim]\n","        #K = [batch size, key len, hid dim]\n","        #V = [batch size, value len, hid dim]\n","                \n","        Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n","        K = K.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n","        V = V.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n","        \n","        #Q = [batch size, n heads, query len, head dim]\n","        #K = [batch size, n heads, key len, head dim]\n","        #V = [batch size, n heads, value len, head dim]\n","                \n","        energy = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.scale\n","        \n","        #energy = [batch size, n heads, seq len, seq len]\n","        \n","        if mask is not None:\n","            energy = energy.masked_fill(mask == 0, -1e10)\n","        \n","        attention = torch.softmax(energy, dim = -1)\n","                \n","        #attention = [batch size, n heads, query len, key len]\n","        \n","        x = torch.matmul(self.dropout(attention), V) \n","\n","        # x = torch.matmul(attention, V) \n","        \n","        #x = [batch size, n heads, seq len, head dim]\n","        \n","        x = x.permute(0, 2, 1, 3).contiguous()\n","        \n","        #x = [batch size, seq len, n heads, head dim]\n","        \n","        x = x.view(batch_size, -1, self.hid_dim)\n","        \n","        #x = [batch size, seq len, hid dim]\n","        \n","        x = self.fc_o(x)\n","        \n","        #x = [batch size, seq len, hid dim]\n","        \n","        return x, attention\n","\n","\n","\n","class Transformer(nn.Module):\n","    def __init__(self, \n","                 hid_dim, \n","                 n_heads, \n","                 pf_dim,  \n","                 dropout, \n","                 device):\n","        super().__init__()\n","        \n","        self.self_att_layer_norm = nn.LayerNorm(hid_dim)\n","        self.ff_layer_norm = nn.LayerNorm(hid_dim)\n","        self.self_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n","        self.positionwise_feedforward = PositionwiseFeedforwardLayer(hid_dim, \n","                                                                     pf_dim, \n","                                                                     dropout)\n","        self.dropout = nn.Dropout(dropout)\n","        \n","    def forward(self, src, src_mask):\n","        \n","        #src = [batch size, src len, hid dim]\n","        #src_mask = [batch size, src len]\n","                \n","        #self attention\n","        _src, _ = self.self_attention(src, src, src, src_mask)\n","        \n","        #dropout, residual connection and layer norm\n","        src = self.self_att_layer_norm(src + self.dropout(_src))\n","        \n","        #src = [batch size, src len, hid dim]\n","        \n","        #positionwise feedforward\n","        _src = self.positionwise_feedforward(src)\n","        \n","        #dropout, residual and layer norm\n","        src = self.ff_layer_norm(src + self.dropout(_src))\n","        \n","        #src = [batch size, src len, hid dim]\n","        \n","        return src\n","\n","\n","class TransformerWithLMHead(nn.Module):\n","    def __init__(self, \n","                 input_dim, \n","                 emb_dim, \n","                 n_layers, \n","                 n_heads, \n","                 hid_dim,\n","                 dropout, \n","                 device,\n","                 max_length = bptt):\n","        super().__init__()\n","\n","        self.device = device\n","        \n","        self.tok_embedding = nn.Embedding(input_dim, emb_dim)\n","        self.pos_embedding = PositionalEncoding(emb_dim, dropout)\n","\n","        self.layers = nn.ModuleList([Transformer(emb_dim, \n","                                                  n_heads, \n","                                                  hid_dim,\n","                                                  dropout, \n","                                                  device) \n","                                     for _ in range(n_layers)])\n","        \n","        self.dropout = nn.Dropout(dropout)\n","        \n","        self.scale = torch.sqrt(torch.FloatTensor([emb_dim])).to(device)\n","        self.fc = nn.Linear(emb_dim, input_dim)\n","\n","\n","    def make_src_mask(self, src):\n","        \n","        #src = [batch size, src len]\n","        \n","        src_mask = (src != 0).unsqueeze(1).unsqueeze(2)\n","\n","        #src_mask = [batch size, 1, 1, src len]\n","\n","        return src_mask\n","        \n","    def forward(self, src):\n","        \n","        #src = [batch size, src len]\n","\n","        src_mask = self.make_src_mask(src)\n","\n","        #src_mask = [batch size, 1, 1, src len]\n","        \n","        batch_size = src.shape[0]\n","        src_len = src.shape[1]\n","\n","        src = self.tok_embedding(src) * self.scale\n","\n","        #src = [batch size, src len, hid dim]\n","        \n","        src = self.pos_embedding(src)\n","\n","        #src = [batch size, src len, hid dim]\n","        \n","        for layer in self.layers:\n","            src = layer(src, src_mask)\n","            \n","        #src = [batch size, src len, hid dim]\n","\n","        src = src.transpose(1,0)\n","\n","        #src = [src len, batch size, hid dim]\n","\n","        out = self.fc(src)\n","\n","        # out = [src len, batch size, vocab_size]\n","            \n","        return F.log_softmax(out, dim=-1)"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"vLoNGEHReBIm","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593258183696,"user_tz":-120,"elapsed":47051,"user":{"displayName":"Daniel Cuiñas Vázquez","photoUrl":"","userId":"11552885780691803973"}}},"source":["vocab_size = len(TEXT.vocab)\n","emb_dim = 128\n","hid_dim = 128\n","n_layers = 4\n","n_heads = 4\n","dropout = 0.1\n","\n","lr = 4\n","log_interval = 20\n","\n","model = TransformerWithLMHead(vocab_size, emb_dim, n_layers, n_heads, hid_dim, dropout, device)"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"6sCJ6DnIeVdK","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1593258183697,"user_tz":-120,"elapsed":47041,"user":{"displayName":"Daniel Cuiñas Vázquez","photoUrl":"","userId":"11552885780691803973"}},"outputId":"65e625a4-e79c-4582-ffae-aba26ed89b02"},"source":["def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","print(f'The model has {count_parameters(model):,} trainable parameters')"],"execution_count":10,"outputs":[{"output_type":"stream","text":["The model has 7,817,926 trainable parameters\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"KsjMRJbBerYD","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593258185753,"user_tz":-120,"elapsed":525,"user":{"displayName":"Daniel Cuiñas Vázquez","photoUrl":"","userId":"11552885780691803973"}}},"source":["import torch.optim as optim\n","\n","criterion = nn.NLLLoss()"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"gINHrmR5ey-F","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593258186628,"user_tz":-120,"elapsed":533,"user":{"displayName":"Daniel Cuiñas Vázquez","photoUrl":"","userId":"11552885780691803973"}}},"source":["model=model.to(device)\n","criterion=criterion.to(device)"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"iAMjyPbye0mI","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593258186864,"user_tz":-120,"elapsed":505,"user":{"displayName":"Daniel Cuiñas Vázquez","photoUrl":"","userId":"11552885780691803973"}}},"source":["def train(model, iterator, criterion):\n","    clip = 0.25\n","    total_loss = 0\n","    \n","    model.train()\n","            \n","    for k, batch in enumerate(iterator):\n","        data = batch.text\n","        data = data.transpose(1,0)\n","        targets = batch.target.view(-1)\n","\n","        data = data.to(device)\n","        targets = targets.to(device)\n","\n","        model.zero_grad()\n","      \n","        output = model(data) \n","\n","        output = output.view(-1, vocab_size)\n","        \n","        loss = criterion(output, targets)\n","                \n","        loss.backward()\n","        \n","        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n","        for p in model.parameters():\n","            p.data.add_(-lr, p.grad.data)\n","        \n","        total_loss += loss.item()\n","        \n","        if k % log_interval == 0 and k > 0:\n","            cur_loss = total_loss / log_interval\n","            print('| epoch {:3d} | {:5d}/{:5d} batches | lr {} | '\n","                    'loss {:5.2f} | ppl {:8.2f}'.format(epoch, k, len(iterator), lr, cur_loss, math.exp(cur_loss)))\n","            total_loss = 0\n"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"ockApLfye-wl","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593258187442,"user_tz":-120,"elapsed":619,"user":{"displayName":"Daniel Cuiñas Vázquez","photoUrl":"","userId":"11552885780691803973"}}},"source":["def evaluate(model, iterator, criterion):\n","    \n","    total_loss = 0\n","    \n","    model.eval()\n","        \n","    with torch.no_grad():\n","    \n","        for batch in iterator:\n","            data = batch.text\n","            data = data.transpose(1,0)\n","            targets = batch.target.view(-1)\n","\n","            len_data = data.shape[1]\n","\n","            data = data.to(device)\n","            targets = targets.to(device)\n","\n","            output = model(data)\n","\n","            output = output.view(-1, vocab_size)\n","            \n","            loss = criterion(output, targets).item()\n","\n","            total_loss += len_data * loss\n","\n","        \n","    return total_loss / (len(iterator)*bptt - 1)"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"ej6eo3HNfB4i","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593258187678,"user_tz":-120,"elapsed":414,"user":{"displayName":"Daniel Cuiñas Vázquez","photoUrl":"","userId":"11552885780691803973"}}},"source":["import time\n","import math\n","\n","def epoch_time(start_time, end_time):\n","    elapsed_time = end_time - start_time\n","    elapsed_mins = int(elapsed_time / 60)\n","    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n","    return elapsed_mins, elapsed_secs"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"id":"K0UrCm32fDya","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"error","timestamp":1593262667688,"user_tz":-120,"elapsed":4479850,"user":{"displayName":"Daniel Cuiñas Vázquez","photoUrl":"","userId":"11552885780691803973"}},"outputId":"ff1463b8-9225-4117-822c-bd19f5c29999"},"source":["N_EPOCHS = 100\n","\n","best_valid_loss = float('inf')\n","counter = 0\n","patience = 5\n","\n","for epoch in range(N_EPOCHS):\n","\n","    start_time = time.time()\n","    \n","    train(model, train_iter, criterion)\n","    valid_loss = evaluate(model, valid_iter, criterion)\n","    \n","    end_time = time.time()\n","\n","    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n","\n","    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n","    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):.2f}')\n","\n","    if valid_loss < best_valid_loss:\n","        best_valid_loss = valid_loss\n","        #torch.save(model.state_dict(), 'tut2-model.pt')\n","        counter = 0 \n","    else:\n","        lr /= 4.0\n","        counter += 1\n","        if counter >= patience:\n","            break\n","\n","    "],"execution_count":16,"outputs":[{"output_type":"stream","text":["/pytorch/torch/csrc/utils/python_arg_parser.cpp:756: UserWarning: This overload of add_ is deprecated:\n","\tadd_(Number alpha, Tensor other)\n","Consider using one of the following signatures instead:\n","\tadd_(Tensor other, *, Number alpha)\n"],"name":"stderr"},{"output_type":"stream","text":["| epoch   0 |    20/  224 batches | lr 4 | loss  8.48 | ppl  4822.47\n","| epoch   0 |    40/  224 batches | lr 4 | loss  7.24 | ppl  1393.59\n","| epoch   0 |    60/  224 batches | lr 4 | loss  7.11 | ppl  1223.15\n","| epoch   0 |    80/  224 batches | lr 4 | loss  6.89 | ppl   983.52\n","| epoch   0 |   100/  224 batches | lr 4 | loss  6.80 | ppl   896.96\n","| epoch   0 |   120/  224 batches | lr 4 | loss  6.75 | ppl   851.21\n","| epoch   0 |   140/  224 batches | lr 4 | loss  6.61 | ppl   745.12\n","| epoch   0 |   160/  224 batches | lr 4 | loss  6.58 | ppl   722.66\n","| epoch   0 |   180/  224 batches | lr 4 | loss  6.55 | ppl   696.23\n","| epoch   0 |   200/  224 batches | lr 4 | loss  6.49 | ppl   657.62\n","| epoch   0 |   220/  224 batches | lr 4 | loss  6.50 | ppl   665.74\n","Epoch: 01 | Epoch Time: 1m 20s\n","\t Val. Loss: 5.661 |  Val. PPL: 287.43\n","| epoch   1 |    20/  224 batches | lr 4 | loss  6.71 | ppl   820.92\n","| epoch   1 |    40/  224 batches | lr 4 | loss  6.33 | ppl   563.66\n","| epoch   1 |    60/  224 batches | lr 4 | loss  6.37 | ppl   586.00\n","| epoch   1 |    80/  224 batches | lr 4 | loss  6.34 | ppl   566.83\n","| epoch   1 |   100/  224 batches | lr 4 | loss  6.30 | ppl   543.51\n","| epoch   1 |   120/  224 batches | lr 4 | loss  6.30 | ppl   544.76\n","| epoch   1 |   140/  224 batches | lr 4 | loss  6.24 | ppl   514.82\n","| epoch   1 |   160/  224 batches | lr 4 | loss  6.25 | ppl   520.14\n","| epoch   1 |   180/  224 batches | lr 4 | loss  6.22 | ppl   503.55\n","| epoch   1 |   200/  224 batches | lr 4 | loss  6.18 | ppl   482.44\n","| epoch   1 |   220/  224 batches | lr 4 | loss  6.21 | ppl   498.22\n","Epoch: 02 | Epoch Time: 1m 20s\n","\t Val. Loss: 5.580 |  Val. PPL: 265.19\n","| epoch   2 |    20/  224 batches | lr 4 | loss  6.45 | ppl   632.85\n","| epoch   2 |    40/  224 batches | lr 4 | loss  6.11 | ppl   450.54\n","| epoch   2 |    60/  224 batches | lr 4 | loss  6.14 | ppl   462.09\n","| epoch   2 |    80/  224 batches | lr 4 | loss  6.10 | ppl   444.06\n","| epoch   2 |   100/  224 batches | lr 4 | loss  6.08 | ppl   436.83\n","| epoch   2 |   120/  224 batches | lr 4 | loss  6.10 | ppl   445.97\n","| epoch   2 |   140/  224 batches | lr 4 | loss  6.05 | ppl   424.79\n","| epoch   2 |   160/  224 batches | lr 4 | loss  6.06 | ppl   429.91\n","| epoch   2 |   180/  224 batches | lr 4 | loss  6.03 | ppl   416.54\n","| epoch   2 |   200/  224 batches | lr 4 | loss  6.01 | ppl   406.12\n","| epoch   2 |   220/  224 batches | lr 4 | loss  6.03 | ppl   415.86\n","Epoch: 03 | Epoch Time: 1m 20s\n","\t Val. Loss: 5.384 |  Val. PPL: 217.79\n","| epoch   3 |    20/  224 batches | lr 4 | loss  6.28 | ppl   531.42\n","| epoch   3 |    40/  224 batches | lr 4 | loss  5.93 | ppl   377.20\n","| epoch   3 |    60/  224 batches | lr 4 | loss  5.98 | ppl   395.05\n","| epoch   3 |    80/  224 batches | lr 4 | loss  5.93 | ppl   377.74\n","| epoch   3 |   100/  224 batches | lr 4 | loss  5.93 | ppl   374.51\n","| epoch   3 |   120/  224 batches | lr 4 | loss  5.94 | ppl   380.49\n","| epoch   3 |   140/  224 batches | lr 4 | loss  5.90 | ppl   363.26\n","| epoch   3 |   160/  224 batches | lr 4 | loss  5.92 | ppl   373.79\n","| epoch   3 |   180/  224 batches | lr 4 | loss  5.90 | ppl   364.87\n","| epoch   3 |   200/  224 batches | lr 4 | loss  5.86 | ppl   349.83\n","| epoch   3 |   220/  224 batches | lr 4 | loss  5.88 | ppl   358.65\n","Epoch: 04 | Epoch Time: 1m 20s\n","\t Val. Loss: 5.321 |  Val. PPL: 204.65\n","| epoch   4 |    20/  224 batches | lr 4 | loss  6.15 | ppl   467.37\n","| epoch   4 |    40/  224 batches | lr 4 | loss  5.80 | ppl   330.35\n","| epoch   4 |    60/  224 batches | lr 4 | loss  5.85 | ppl   347.17\n","| epoch   4 |    80/  224 batches | lr 4 | loss  5.81 | ppl   334.58\n","| epoch   4 |   100/  224 batches | lr 4 | loss  5.80 | ppl   331.76\n","| epoch   4 |   120/  224 batches | lr 4 | loss  5.82 | ppl   338.17\n","| epoch   4 |   140/  224 batches | lr 4 | loss  5.78 | ppl   322.92\n","| epoch   4 |   160/  224 batches | lr 4 | loss  5.81 | ppl   333.43\n","| epoch   4 |   180/  224 batches | lr 4 | loss  5.79 | ppl   325.74\n","| epoch   4 |   200/  224 batches | lr 4 | loss  5.76 | ppl   316.06\n","| epoch   4 |   220/  224 batches | lr 4 | loss  5.77 | ppl   322.03\n","Epoch: 05 | Epoch Time: 1m 20s\n","\t Val. Loss: 5.144 |  Val. PPL: 171.39\n","| epoch   5 |    20/  224 batches | lr 4 | loss  6.03 | ppl   415.67\n","| epoch   5 |    40/  224 batches | lr 4 | loss  5.70 | ppl   298.14\n","| epoch   5 |    60/  224 batches | lr 4 | loss  5.75 | ppl   312.68\n","| epoch   5 |    80/  224 batches | lr 4 | loss  5.71 | ppl   302.20\n","| epoch   5 |   100/  224 batches | lr 4 | loss  5.71 | ppl   300.73\n","| epoch   5 |   120/  224 batches | lr 4 | loss  5.71 | ppl   303.22\n","| epoch   5 |   140/  224 batches | lr 4 | loss  5.70 | ppl   298.14\n","| epoch   5 |   160/  224 batches | lr 4 | loss  5.72 | ppl   303.87\n","| epoch   5 |   180/  224 batches | lr 4 | loss  5.70 | ppl   298.88\n","| epoch   5 |   200/  224 batches | lr 4 | loss  5.66 | ppl   287.20\n","| epoch   5 |   220/  224 batches | lr 4 | loss  5.68 | ppl   292.73\n","Epoch: 06 | Epoch Time: 1m 20s\n","\t Val. Loss: 5.160 |  Val. PPL: 174.24\n","| epoch   6 |    20/  224 batches | lr 1.0 | loss  5.84 | ppl   342.55\n","| epoch   6 |    40/  224 batches | lr 1.0 | loss  5.50 | ppl   244.81\n","| epoch   6 |    60/  224 batches | lr 1.0 | loss  5.54 | ppl   255.76\n","| epoch   6 |    80/  224 batches | lr 1.0 | loss  5.52 | ppl   249.01\n","| epoch   6 |   100/  224 batches | lr 1.0 | loss  5.51 | ppl   246.29\n","| epoch   6 |   120/  224 batches | lr 1.0 | loss  5.52 | ppl   249.69\n","| epoch   6 |   140/  224 batches | lr 1.0 | loss  5.50 | ppl   244.63\n","| epoch   6 |   160/  224 batches | lr 1.0 | loss  5.53 | ppl   251.84\n","| epoch   6 |   180/  224 batches | lr 1.0 | loss  5.51 | ppl   246.25\n","| epoch   6 |   200/  224 batches | lr 1.0 | loss  5.47 | ppl   237.11\n","| epoch   6 |   220/  224 batches | lr 1.0 | loss  5.49 | ppl   242.01\n","Epoch: 07 | Epoch Time: 1m 20s\n","\t Val. Loss: 4.970 |  Val. PPL: 144.03\n","| epoch   7 |    20/  224 batches | lr 1.0 | loss  5.76 | ppl   317.96\n","| epoch   7 |    40/  224 batches | lr 1.0 | loss  5.44 | ppl   231.29\n","| epoch   7 |    60/  224 batches | lr 1.0 | loss  5.49 | ppl   242.47\n","| epoch   7 |    80/  224 batches | lr 1.0 | loss  5.47 | ppl   237.35\n","| epoch   7 |   100/  224 batches | lr 1.0 | loss  5.46 | ppl   234.82\n","| epoch   7 |   120/  224 batches | lr 1.0 | loss  5.47 | ppl   238.24\n","| epoch   7 |   140/  224 batches | lr 1.0 | loss  5.46 | ppl   234.78\n","| epoch   7 |   160/  224 batches | lr 1.0 | loss  5.49 | ppl   241.18\n","| epoch   7 |   180/  224 batches | lr 1.0 | loss  5.47 | ppl   237.53\n","| epoch   7 |   200/  224 batches | lr 1.0 | loss  5.43 | ppl   228.63\n","| epoch   7 |   220/  224 batches | lr 1.0 | loss  5.45 | ppl   233.34\n","Epoch: 08 | Epoch Time: 1m 20s\n","\t Val. Loss: 4.939 |  Val. PPL: 139.63\n","| epoch   8 |    20/  224 batches | lr 1.0 | loss  5.72 | ppl   305.19\n","| epoch   8 |    40/  224 batches | lr 1.0 | loss  5.41 | ppl   222.76\n","| epoch   8 |    60/  224 batches | lr 1.0 | loss  5.45 | ppl   233.54\n","| epoch   8 |    80/  224 batches | lr 1.0 | loss  5.43 | ppl   228.39\n","| epoch   8 |   100/  224 batches | lr 1.0 | loss  5.42 | ppl   226.01\n","| epoch   8 |   120/  224 batches | lr 1.0 | loss  5.44 | ppl   230.12\n","| epoch   8 |   140/  224 batches | lr 1.0 | loss  5.42 | ppl   226.70\n","| epoch   8 |   160/  224 batches | lr 1.0 | loss  5.45 | ppl   233.86\n","| epoch   8 |   180/  224 batches | lr 1.0 | loss  5.44 | ppl   229.39\n","| epoch   8 |   200/  224 batches | lr 1.0 | loss  5.40 | ppl   221.27\n","| epoch   8 |   220/  224 batches | lr 1.0 | loss  5.42 | ppl   225.97\n","Epoch: 09 | Epoch Time: 1m 20s\n","\t Val. Loss: 4.920 |  Val. PPL: 136.96\n","| epoch   9 |    20/  224 batches | lr 1.0 | loss  5.69 | ppl   294.63\n","| epoch   9 |    40/  224 batches | lr 1.0 | loss  5.37 | ppl   214.98\n","| epoch   9 |    60/  224 batches | lr 1.0 | loss  5.42 | ppl   226.02\n","| epoch   9 |    80/  224 batches | lr 1.0 | loss  5.40 | ppl   221.08\n","| epoch   9 |   100/  224 batches | lr 1.0 | loss  5.39 | ppl   218.92\n","| epoch   9 |   120/  224 batches | lr 1.0 | loss  5.41 | ppl   222.70\n","| epoch   9 |   140/  224 batches | lr 1.0 | loss  5.40 | ppl   220.52\n","| epoch   9 |   160/  224 batches | lr 1.0 | loss  5.42 | ppl   227.00\n","| epoch   9 |   180/  224 batches | lr 1.0 | loss  5.41 | ppl   223.39\n","| epoch   9 |   200/  224 batches | lr 1.0 | loss  5.37 | ppl   215.01\n","| epoch   9 |   220/  224 batches | lr 1.0 | loss  5.39 | ppl   218.92\n","Epoch: 10 | Epoch Time: 1m 20s\n","\t Val. Loss: 4.900 |  Val. PPL: 134.34\n","| epoch  10 |    20/  224 batches | lr 1.0 | loss  5.65 | ppl   285.33\n","| epoch  10 |    40/  224 batches | lr 1.0 | loss  5.34 | ppl   208.53\n","| epoch  10 |    60/  224 batches | lr 1.0 | loss  5.39 | ppl   219.35\n","| epoch  10 |    80/  224 batches | lr 1.0 | loss  5.37 | ppl   214.33\n","| epoch  10 |   100/  224 batches | lr 1.0 | loss  5.36 | ppl   212.84\n","| epoch  10 |   120/  224 batches | lr 1.0 | loss  5.38 | ppl   216.83\n","| epoch  10 |   140/  224 batches | lr 1.0 | loss  5.37 | ppl   214.95\n","| epoch  10 |   160/  224 batches | lr 1.0 | loss  5.40 | ppl   221.32\n","| epoch  10 |   180/  224 batches | lr 1.0 | loss  5.38 | ppl   217.64\n","| epoch  10 |   200/  224 batches | lr 1.0 | loss  5.34 | ppl   209.54\n","| epoch  10 |   220/  224 batches | lr 1.0 | loss  5.36 | ppl   213.44\n","Epoch: 11 | Epoch Time: 1m 20s\n","\t Val. Loss: 4.879 |  Val. PPL: 131.49\n","| epoch  11 |    20/  224 batches | lr 1.0 | loss  5.63 | ppl   277.61\n","| epoch  11 |    40/  224 batches | lr 1.0 | loss  5.31 | ppl   203.29\n","| epoch  11 |    60/  224 batches | lr 1.0 | loss  5.37 | ppl   213.93\n","| epoch  11 |    80/  224 batches | lr 1.0 | loss  5.34 | ppl   209.10\n","| epoch  11 |   100/  224 batches | lr 1.0 | loss  5.33 | ppl   207.08\n","| epoch  11 |   120/  224 batches | lr 1.0 | loss  5.35 | ppl   211.19\n","| epoch  11 |   140/  224 batches | lr 1.0 | loss  5.34 | ppl   209.31\n","| epoch  11 |   160/  224 batches | lr 1.0 | loss  5.37 | ppl   215.41\n","| epoch  11 |   180/  224 batches | lr 1.0 | loss  5.36 | ppl   212.34\n","| epoch  11 |   200/  224 batches | lr 1.0 | loss  5.32 | ppl   203.71\n","| epoch  11 |   220/  224 batches | lr 1.0 | loss  5.34 | ppl   208.03\n","Epoch: 12 | Epoch Time: 1m 20s\n","\t Val. Loss: 4.862 |  Val. PPL: 129.23\n","| epoch  12 |    20/  224 batches | lr 1.0 | loss  5.60 | ppl   269.88\n","| epoch  12 |    40/  224 batches | lr 1.0 | loss  5.29 | ppl   198.04\n","| epoch  12 |    60/  224 batches | lr 1.0 | loss  5.34 | ppl   208.21\n","| epoch  12 |    80/  224 batches | lr 1.0 | loss  5.32 | ppl   204.00\n","| epoch  12 |   100/  224 batches | lr 1.0 | loss  5.31 | ppl   202.41\n","| epoch  12 |   120/  224 batches | lr 1.0 | loss  5.33 | ppl   206.08\n","| epoch  12 |   140/  224 batches | lr 1.0 | loss  5.32 | ppl   204.28\n","| epoch  12 |   160/  224 batches | lr 1.0 | loss  5.35 | ppl   210.79\n","| epoch  12 |   180/  224 batches | lr 1.0 | loss  5.34 | ppl   207.87\n","| epoch  12 |   200/  224 batches | lr 1.0 | loss  5.29 | ppl   199.22\n","| epoch  12 |   220/  224 batches | lr 1.0 | loss  5.32 | ppl   203.57\n","Epoch: 13 | Epoch Time: 1m 21s\n","\t Val. Loss: 4.844 |  Val. PPL: 126.96\n","| epoch  13 |    20/  224 batches | lr 1.0 | loss  5.57 | ppl   262.42\n","| epoch  13 |    40/  224 batches | lr 1.0 | loss  5.26 | ppl   193.28\n","| epoch  13 |    60/  224 batches | lr 1.0 | loss  5.32 | ppl   203.67\n","| epoch  13 |    80/  224 batches | lr 1.0 | loss  5.29 | ppl   198.82\n","| epoch  13 |   100/  224 batches | lr 1.0 | loss  5.29 | ppl   197.42\n","| epoch  13 |   120/  224 batches | lr 1.0 | loss  5.31 | ppl   201.53\n","| epoch  13 |   140/  224 batches | lr 1.0 | loss  5.30 | ppl   200.38\n","| epoch  13 |   160/  224 batches | lr 1.0 | loss  5.33 | ppl   206.18\n","| epoch  13 |   180/  224 batches | lr 1.0 | loss  5.31 | ppl   203.13\n","| epoch  13 |   200/  224 batches | lr 1.0 | loss  5.27 | ppl   194.80\n","| epoch  13 |   220/  224 batches | lr 1.0 | loss  5.29 | ppl   199.12\n","Epoch: 14 | Epoch Time: 1m 21s\n","\t Val. Loss: 4.825 |  Val. PPL: 124.53\n","| epoch  14 |    20/  224 batches | lr 1.0 | loss  5.55 | ppl   256.89\n","| epoch  14 |    40/  224 batches | lr 1.0 | loss  5.24 | ppl   189.10\n","| epoch  14 |    60/  224 batches | lr 1.0 | loss  5.30 | ppl   199.41\n","| epoch  14 |    80/  224 batches | lr 1.0 | loss  5.27 | ppl   194.04\n","| epoch  14 |   100/  224 batches | lr 1.0 | loss  5.26 | ppl   192.94\n","| epoch  14 |   120/  224 batches | lr 1.0 | loss  5.28 | ppl   196.90\n","| epoch  14 |   140/  224 batches | lr 1.0 | loss  5.28 | ppl   195.79\n","| epoch  14 |   160/  224 batches | lr 1.0 | loss  5.31 | ppl   201.84\n","| epoch  14 |   180/  224 batches | lr 1.0 | loss  5.29 | ppl   198.77\n","| epoch  14 |   200/  224 batches | lr 1.0 | loss  5.25 | ppl   190.38\n","| epoch  14 |   220/  224 batches | lr 1.0 | loss  5.27 | ppl   194.55\n","Epoch: 15 | Epoch Time: 1m 20s\n","\t Val. Loss: 4.816 |  Val. PPL: 123.44\n","| epoch  15 |    20/  224 batches | lr 1.0 | loss  5.52 | ppl   250.86\n","| epoch  15 |    40/  224 batches | lr 1.0 | loss  5.22 | ppl   184.79\n","| epoch  15 |    60/  224 batches | lr 1.0 | loss  5.27 | ppl   194.39\n","| epoch  15 |    80/  224 batches | lr 1.0 | loss  5.25 | ppl   189.78\n","| epoch  15 |   100/  224 batches | lr 1.0 | loss  5.24 | ppl   188.83\n","| epoch  15 |   120/  224 batches | lr 1.0 | loss  5.26 | ppl   192.10\n","| epoch  15 |   140/  224 batches | lr 1.0 | loss  5.26 | ppl   191.86\n","| epoch  15 |   160/  224 batches | lr 1.0 | loss  5.29 | ppl   197.46\n","| epoch  15 |   180/  224 batches | lr 1.0 | loss  5.27 | ppl   194.57\n","| epoch  15 |   200/  224 batches | lr 1.0 | loss  5.23 | ppl   186.69\n","| epoch  15 |   220/  224 batches | lr 1.0 | loss  5.25 | ppl   190.55\n","Epoch: 16 | Epoch Time: 1m 20s\n","\t Val. Loss: 4.798 |  Val. PPL: 121.23\n","| epoch  16 |    20/  224 batches | lr 1.0 | loss  5.50 | ppl   245.12\n","| epoch  16 |    40/  224 batches | lr 1.0 | loss  5.20 | ppl   180.82\n","| epoch  16 |    60/  224 batches | lr 1.0 | loss  5.25 | ppl   190.90\n","| epoch  16 |    80/  224 batches | lr 1.0 | loss  5.23 | ppl   185.98\n","| epoch  16 |   100/  224 batches | lr 1.0 | loss  5.22 | ppl   184.69\n","| epoch  16 |   120/  224 batches | lr 1.0 | loss  5.24 | ppl   188.02\n","| epoch  16 |   140/  224 batches | lr 1.0 | loss  5.24 | ppl   187.84\n","| epoch  16 |   160/  224 batches | lr 1.0 | loss  5.27 | ppl   193.82\n","| epoch  16 |   180/  224 batches | lr 1.0 | loss  5.25 | ppl   190.89\n","| epoch  16 |   200/  224 batches | lr 1.0 | loss  5.21 | ppl   182.40\n","| epoch  16 |   220/  224 batches | lr 1.0 | loss  5.23 | ppl   186.92\n","Epoch: 17 | Epoch Time: 1m 21s\n","\t Val. Loss: 4.784 |  Val. PPL: 119.56\n","| epoch  17 |    20/  224 batches | lr 1.0 | loss  5.48 | ppl   239.56\n","| epoch  17 |    40/  224 batches | lr 1.0 | loss  5.18 | ppl   177.09\n","| epoch  17 |    60/  224 batches | lr 1.0 | loss  5.23 | ppl   186.93\n","| epoch  17 |    80/  224 batches | lr 1.0 | loss  5.20 | ppl   182.08\n","| epoch  17 |   100/  224 batches | lr 1.0 | loss  5.20 | ppl   180.77\n","| epoch  17 |   120/  224 batches | lr 1.0 | loss  5.22 | ppl   184.48\n","| epoch  17 |   140/  224 batches | lr 1.0 | loss  5.21 | ppl   183.99\n","| epoch  17 |   160/  224 batches | lr 1.0 | loss  5.25 | ppl   190.05\n","| epoch  17 |   180/  224 batches | lr 1.0 | loss  5.23 | ppl   187.22\n","| epoch  17 |   200/  224 batches | lr 1.0 | loss  5.19 | ppl   179.29\n","| epoch  17 |   220/  224 batches | lr 1.0 | loss  5.21 | ppl   183.26\n","Epoch: 18 | Epoch Time: 1m 20s\n","\t Val. Loss: 4.766 |  Val. PPL: 117.50\n","| epoch  18 |    20/  224 batches | lr 1.0 | loss  5.46 | ppl   234.27\n","| epoch  18 |    40/  224 batches | lr 1.0 | loss  5.16 | ppl   173.86\n","| epoch  18 |    60/  224 batches | lr 1.0 | loss  5.21 | ppl   182.86\n","| epoch  18 |    80/  224 batches | lr 1.0 | loss  5.18 | ppl   178.40\n","| epoch  18 |   100/  224 batches | lr 1.0 | loss  5.18 | ppl   177.00\n","| epoch  18 |   120/  224 batches | lr 1.0 | loss  5.20 | ppl   180.52\n","| epoch  18 |   140/  224 batches | lr 1.0 | loss  5.20 | ppl   180.44\n","| epoch  18 |   160/  224 batches | lr 1.0 | loss  5.23 | ppl   186.92\n","| epoch  18 |   180/  224 batches | lr 1.0 | loss  5.21 | ppl   183.43\n","| epoch  18 |   200/  224 batches | lr 1.0 | loss  5.17 | ppl   175.97\n","| epoch  18 |   220/  224 batches | lr 1.0 | loss  5.19 | ppl   179.82\n","Epoch: 19 | Epoch Time: 1m 20s\n","\t Val. Loss: 4.757 |  Val. PPL: 116.41\n","| epoch  19 |    20/  224 batches | lr 1.0 | loss  5.44 | ppl   229.96\n","| epoch  19 |    40/  224 batches | lr 1.0 | loss  5.14 | ppl   170.27\n","| epoch  19 |    60/  224 batches | lr 1.0 | loss  5.19 | ppl   179.74\n","| epoch  19 |    80/  224 batches | lr 1.0 | loss  5.16 | ppl   174.44\n","| epoch  19 |   100/  224 batches | lr 1.0 | loss  5.16 | ppl   174.32\n","| epoch  19 |   120/  224 batches | lr 1.0 | loss  5.18 | ppl   178.07\n","| epoch  19 |   140/  224 batches | lr 1.0 | loss  5.18 | ppl   177.26\n","| epoch  19 |   160/  224 batches | lr 1.0 | loss  5.21 | ppl   183.47\n","| epoch  19 |   180/  224 batches | lr 1.0 | loss  5.19 | ppl   180.30\n","| epoch  19 |   200/  224 batches | lr 1.0 | loss  5.15 | ppl   172.72\n","| epoch  19 |   220/  224 batches | lr 1.0 | loss  5.17 | ppl   176.64\n","Epoch: 20 | Epoch Time: 1m 20s\n","\t Val. Loss: 4.743 |  Val. PPL: 114.78\n","| epoch  20 |    20/  224 batches | lr 1.0 | loss  5.41 | ppl   224.68\n","| epoch  20 |    40/  224 batches | lr 1.0 | loss  5.12 | ppl   167.13\n","| epoch  20 |    60/  224 batches | lr 1.0 | loss  5.17 | ppl   176.37\n","| epoch  20 |    80/  224 batches | lr 1.0 | loss  5.15 | ppl   171.87\n","| epoch  20 |   100/  224 batches | lr 1.0 | loss  5.14 | ppl   171.08\n","| epoch  20 |   120/  224 batches | lr 1.0 | loss  5.16 | ppl   174.01\n","| epoch  20 |   140/  224 batches | lr 1.0 | loss  5.16 | ppl   174.31\n","| epoch  20 |   160/  224 batches | lr 1.0 | loss  5.20 | ppl   180.58\n","| epoch  20 |   180/  224 batches | lr 1.0 | loss  5.18 | ppl   177.70\n","| epoch  20 |   200/  224 batches | lr 1.0 | loss  5.13 | ppl   169.49\n","| epoch  20 |   220/  224 batches | lr 1.0 | loss  5.16 | ppl   174.09\n","Epoch: 21 | Epoch Time: 1m 20s\n","\t Val. Loss: 4.736 |  Val. PPL: 113.92\n","| epoch  21 |    20/  224 batches | lr 1.0 | loss  5.40 | ppl   220.84\n","| epoch  21 |    40/  224 batches | lr 1.0 | loss  5.10 | ppl   163.99\n","| epoch  21 |    60/  224 batches | lr 1.0 | loss  5.15 | ppl   173.12\n","| epoch  21 |    80/  224 batches | lr 1.0 | loss  5.13 | ppl   168.74\n","| epoch  21 |   100/  224 batches | lr 1.0 | loss  5.13 | ppl   168.35\n","| epoch  21 |   120/  224 batches | lr 1.0 | loss  5.14 | ppl   171.47\n","| epoch  21 |   140/  224 batches | lr 1.0 | loss  5.14 | ppl   171.32\n","| epoch  21 |   160/  224 batches | lr 1.0 | loss  5.18 | ppl   177.16\n","| epoch  21 |   180/  224 batches | lr 1.0 | loss  5.16 | ppl   174.24\n","| epoch  21 |   200/  224 batches | lr 1.0 | loss  5.12 | ppl   166.65\n","| epoch  21 |   220/  224 batches | lr 1.0 | loss  5.14 | ppl   170.48\n","Epoch: 22 | Epoch Time: 1m 20s\n","\t Val. Loss: 4.721 |  Val. PPL: 112.27\n","| epoch  22 |    20/  224 batches | lr 1.0 | loss  5.38 | ppl   216.24\n","| epoch  22 |    40/  224 batches | lr 1.0 | loss  5.08 | ppl   161.40\n","| epoch  22 |    60/  224 batches | lr 1.0 | loss  5.14 | ppl   170.42\n","| epoch  22 |    80/  224 batches | lr 1.0 | loss  5.11 | ppl   165.70\n","| epoch  22 |   100/  224 batches | lr 1.0 | loss  5.11 | ppl   165.24\n","| epoch  22 |   120/  224 batches | lr 1.0 | loss  5.13 | ppl   168.57\n","| epoch  22 |   140/  224 batches | lr 1.0 | loss  5.13 | ppl   168.81\n","| epoch  22 |   160/  224 batches | lr 1.0 | loss  5.16 | ppl   174.48\n","| epoch  22 |   180/  224 batches | lr 1.0 | loss  5.14 | ppl   171.37\n","| epoch  22 |   200/  224 batches | lr 1.0 | loss  5.10 | ppl   164.04\n","| epoch  22 |   220/  224 batches | lr 1.0 | loss  5.12 | ppl   168.12\n","Epoch: 23 | Epoch Time: 1m 20s\n","\t Val. Loss: 4.708 |  Val. PPL: 110.81\n","| epoch  23 |    20/  224 batches | lr 1.0 | loss  5.36 | ppl   212.31\n","| epoch  23 |    40/  224 batches | lr 1.0 | loss  5.07 | ppl   158.97\n","| epoch  23 |    60/  224 batches | lr 1.0 | loss  5.12 | ppl   167.92\n","| epoch  23 |    80/  224 batches | lr 1.0 | loss  5.09 | ppl   163.04\n","| epoch  23 |   100/  224 batches | lr 1.0 | loss  5.09 | ppl   162.47\n","| epoch  23 |   120/  224 batches | lr 1.0 | loss  5.11 | ppl   165.25\n","| epoch  23 |   140/  224 batches | lr 1.0 | loss  5.11 | ppl   165.55\n","| epoch  23 |   160/  224 batches | lr 1.0 | loss  5.15 | ppl   171.71\n","| epoch  23 |   180/  224 batches | lr 1.0 | loss  5.13 | ppl   169.06\n","| epoch  23 |   200/  224 batches | lr 1.0 | loss  5.08 | ppl   161.22\n","| epoch  23 |   220/  224 batches | lr 1.0 | loss  5.11 | ppl   165.07\n","Epoch: 24 | Epoch Time: 1m 20s\n","\t Val. Loss: 4.702 |  Val. PPL: 110.22\n","| epoch  24 |    20/  224 batches | lr 1.0 | loss  5.34 | ppl   208.75\n","| epoch  24 |    40/  224 batches | lr 1.0 | loss  5.05 | ppl   156.32\n","| epoch  24 |    60/  224 batches | lr 1.0 | loss  5.10 | ppl   164.63\n","| epoch  24 |    80/  224 batches | lr 1.0 | loss  5.08 | ppl   160.05\n","| epoch  24 |   100/  224 batches | lr 1.0 | loss  5.08 | ppl   160.04\n","| epoch  24 |   120/  224 batches | lr 1.0 | loss  5.09 | ppl   162.91\n","| epoch  24 |   140/  224 batches | lr 1.0 | loss  5.10 | ppl   163.26\n","| epoch  24 |   160/  224 batches | lr 1.0 | loss  5.13 | ppl   169.19\n","| epoch  24 |   180/  224 batches | lr 1.0 | loss  5.11 | ppl   166.03\n","| epoch  24 |   200/  224 batches | lr 1.0 | loss  5.07 | ppl   158.98\n","| epoch  24 |   220/  224 batches | lr 1.0 | loss  5.09 | ppl   162.52\n","Epoch: 25 | Epoch Time: 1m 20s\n","\t Val. Loss: 4.697 |  Val. PPL: 109.62\n","| epoch  25 |    20/  224 batches | lr 1.0 | loss  5.32 | ppl   204.64\n","| epoch  25 |    40/  224 batches | lr 1.0 | loss  5.03 | ppl   153.51\n","| epoch  25 |    60/  224 batches | lr 1.0 | loss  5.09 | ppl   162.25\n","| epoch  25 |    80/  224 batches | lr 1.0 | loss  5.06 | ppl   157.65\n","| epoch  25 |   100/  224 batches | lr 1.0 | loss  5.06 | ppl   157.48\n","| epoch  25 |   120/  224 batches | lr 1.0 | loss  5.08 | ppl   160.27\n","| epoch  25 |   140/  224 batches | lr 1.0 | loss  5.08 | ppl   160.65\n","| epoch  25 |   160/  224 batches | lr 1.0 | loss  5.12 | ppl   166.52\n","| epoch  25 |   180/  224 batches | lr 1.0 | loss  5.10 | ppl   164.02\n","| epoch  25 |   200/  224 batches | lr 1.0 | loss  5.05 | ppl   156.28\n","| epoch  25 |   220/  224 batches | lr 1.0 | loss  5.07 | ppl   159.66\n","Epoch: 26 | Epoch Time: 1m 20s\n","\t Val. Loss: 4.687 |  Val. PPL: 108.56\n","| epoch  26 |    20/  224 batches | lr 1.0 | loss  5.31 | ppl   202.20\n","| epoch  26 |    40/  224 batches | lr 1.0 | loss  5.02 | ppl   150.99\n","| epoch  26 |    60/  224 batches | lr 1.0 | loss  5.07 | ppl   159.77\n","| epoch  26 |    80/  224 batches | lr 1.0 | loss  5.04 | ppl   154.60\n","| epoch  26 |   100/  224 batches | lr 1.0 | loss  5.04 | ppl   155.23\n","| epoch  26 |   120/  224 batches | lr 1.0 | loss  5.06 | ppl   157.75\n","| epoch  26 |   140/  224 batches | lr 1.0 | loss  5.06 | ppl   158.17\n","| epoch  26 |   160/  224 batches | lr 1.0 | loss  5.10 | ppl   164.26\n","| epoch  26 |   180/  224 batches | lr 1.0 | loss  5.08 | ppl   161.39\n","| epoch  26 |   200/  224 batches | lr 1.0 | loss  5.04 | ppl   154.10\n","| epoch  26 |   220/  224 batches | lr 1.0 | loss  5.06 | ppl   157.53\n","Epoch: 27 | Epoch Time: 1m 20s\n","\t Val. Loss: 4.682 |  Val. PPL: 107.97\n","| epoch  27 |    20/  224 batches | lr 1.0 | loss  5.29 | ppl   198.02\n","| epoch  27 |    40/  224 batches | lr 1.0 | loss  5.00 | ppl   148.62\n","| epoch  27 |    60/  224 batches | lr 1.0 | loss  5.06 | ppl   157.61\n","| epoch  27 |    80/  224 batches | lr 1.0 | loss  5.03 | ppl   152.17\n","| epoch  27 |   100/  224 batches | lr 1.0 | loss  5.03 | ppl   152.92\n","| epoch  27 |   120/  224 batches | lr 1.0 | loss  5.05 | ppl   156.03\n","| epoch  27 |   140/  224 batches | lr 1.0 | loss  5.05 | ppl   156.49\n","| epoch  27 |   160/  224 batches | lr 1.0 | loss  5.09 | ppl   161.95\n","| epoch  27 |   180/  224 batches | lr 1.0 | loss  5.07 | ppl   159.24\n","| epoch  27 |   200/  224 batches | lr 1.0 | loss  5.02 | ppl   151.47\n","| epoch  27 |   220/  224 batches | lr 1.0 | loss  5.05 | ppl   155.30\n","Epoch: 28 | Epoch Time: 1m 20s\n","\t Val. Loss: 4.667 |  Val. PPL: 106.40\n","| epoch  28 |    20/  224 batches | lr 1.0 | loss  5.28 | ppl   195.85\n","| epoch  28 |    40/  224 batches | lr 1.0 | loss  4.99 | ppl   146.76\n","| epoch  28 |    60/  224 batches | lr 1.0 | loss  5.04 | ppl   155.24\n","| epoch  28 |    80/  224 batches | lr 1.0 | loss  5.01 | ppl   150.25\n","| epoch  28 |   100/  224 batches | lr 1.0 | loss  5.01 | ppl   150.49\n","| epoch  28 |   120/  224 batches | lr 1.0 | loss  5.03 | ppl   153.14\n","| epoch  28 |   140/  224 batches | lr 1.0 | loss  5.03 | ppl   153.54\n","| epoch  28 |   160/  224 batches | lr 1.0 | loss  5.08 | ppl   160.07\n","| epoch  28 |   180/  224 batches | lr 1.0 | loss  5.05 | ppl   156.73\n","| epoch  28 |   200/  224 batches | lr 1.0 | loss  5.01 | ppl   149.52\n","| epoch  28 |   220/  224 batches | lr 1.0 | loss  5.03 | ppl   152.85\n","Epoch: 29 | Epoch Time: 1m 20s\n","\t Val. Loss: 4.660 |  Val. PPL: 105.68\n","| epoch  29 |    20/  224 batches | lr 1.0 | loss  5.26 | ppl   192.36\n","| epoch  29 |    40/  224 batches | lr 1.0 | loss  4.97 | ppl   144.40\n","| epoch  29 |    60/  224 batches | lr 1.0 | loss  5.03 | ppl   152.62\n","| epoch  29 |    80/  224 batches | lr 1.0 | loss  5.00 | ppl   147.98\n","| epoch  29 |   100/  224 batches | lr 1.0 | loss  5.00 | ppl   148.30\n","| epoch  29 |   120/  224 batches | lr 1.0 | loss  5.02 | ppl   151.21\n","| epoch  29 |   140/  224 batches | lr 1.0 | loss  5.02 | ppl   151.42\n","| epoch  29 |   160/  224 batches | lr 1.0 | loss  5.06 | ppl   157.60\n","| epoch  29 |   180/  224 batches | lr 1.0 | loss  5.04 | ppl   154.63\n","| epoch  29 |   200/  224 batches | lr 1.0 | loss  4.99 | ppl   147.47\n","| epoch  29 |   220/  224 batches | lr 1.0 | loss  5.02 | ppl   150.76\n","Epoch: 30 | Epoch Time: 1m 20s\n","\t Val. Loss: 4.654 |  Val. PPL: 105.00\n","| epoch  30 |    20/  224 batches | lr 1.0 | loss  5.24 | ppl   189.34\n","| epoch  30 |    40/  224 batches | lr 1.0 | loss  4.96 | ppl   142.71\n","| epoch  30 |    60/  224 batches | lr 1.0 | loss  5.01 | ppl   150.44\n","| epoch  30 |    80/  224 batches | lr 1.0 | loss  4.98 | ppl   145.97\n","| epoch  30 |   100/  224 batches | lr 1.0 | loss  4.98 | ppl   146.00\n","| epoch  30 |   120/  224 batches | lr 1.0 | loss  5.00 | ppl   148.97\n","| epoch  30 |   140/  224 batches | lr 1.0 | loss  5.01 | ppl   149.34\n","| epoch  30 |   160/  224 batches | lr 1.0 | loss  5.05 | ppl   155.44\n","| epoch  30 |   180/  224 batches | lr 1.0 | loss  5.03 | ppl   152.79\n","| epoch  30 |   200/  224 batches | lr 1.0 | loss  4.98 | ppl   145.63\n","| epoch  30 |   220/  224 batches | lr 1.0 | loss  5.00 | ppl   148.78\n","Epoch: 31 | Epoch Time: 1m 20s\n","\t Val. Loss: 4.644 |  Val. PPL: 103.99\n","| epoch  31 |    20/  224 batches | lr 1.0 | loss  5.23 | ppl   186.60\n","| epoch  31 |    40/  224 batches | lr 1.0 | loss  4.95 | ppl   140.78\n","| epoch  31 |    60/  224 batches | lr 1.0 | loss  5.00 | ppl   148.33\n","| epoch  31 |    80/  224 batches | lr 1.0 | loss  4.97 | ppl   143.95\n","| epoch  31 |   100/  224 batches | lr 1.0 | loss  4.97 | ppl   144.30\n","| epoch  31 |   120/  224 batches | lr 1.0 | loss  4.99 | ppl   147.04\n","| epoch  31 |   140/  224 batches | lr 1.0 | loss  4.99 | ppl   147.36\n","| epoch  31 |   160/  224 batches | lr 1.0 | loss  5.03 | ppl   153.27\n","| epoch  31 |   180/  224 batches | lr 1.0 | loss  5.02 | ppl   150.67\n","| epoch  31 |   200/  224 batches | lr 1.0 | loss  4.97 | ppl   143.51\n","| epoch  31 |   220/  224 batches | lr 1.0 | loss  4.99 | ppl   146.73\n","Epoch: 32 | Epoch Time: 1m 20s\n","\t Val. Loss: 4.642 |  Val. PPL: 103.73\n","| epoch  32 |    20/  224 batches | lr 1.0 | loss  5.21 | ppl   183.81\n","| epoch  32 |    40/  224 batches | lr 1.0 | loss  4.93 | ppl   138.80\n","| epoch  32 |    60/  224 batches | lr 1.0 | loss  4.99 | ppl   146.45\n","| epoch  32 |    80/  224 batches | lr 1.0 | loss  4.96 | ppl   142.07\n","| epoch  32 |   100/  224 batches | lr 1.0 | loss  4.96 | ppl   142.40\n","| epoch  32 |   120/  224 batches | lr 1.0 | loss  4.98 | ppl   145.21\n","| epoch  32 |   140/  224 batches | lr 1.0 | loss  4.98 | ppl   145.87\n","| epoch  32 |   160/  224 batches | lr 1.0 | loss  5.02 | ppl   151.33\n","| epoch  32 |   180/  224 batches | lr 1.0 | loss  5.00 | ppl   148.82\n","| epoch  32 |   200/  224 batches | lr 1.0 | loss  4.95 | ppl   141.79\n","| epoch  32 |   220/  224 batches | lr 1.0 | loss  4.98 | ppl   144.76\n","Epoch: 33 | Epoch Time: 1m 20s\n","\t Val. Loss: 4.637 |  Val. PPL: 103.21\n","| epoch  33 |    20/  224 batches | lr 1.0 | loss  5.20 | ppl   181.63\n","| epoch  33 |    40/  224 batches | lr 1.0 | loss  4.92 | ppl   136.99\n","| epoch  33 |    60/  224 batches | lr 1.0 | loss  4.97 | ppl   144.74\n","| epoch  33 |    80/  224 batches | lr 1.0 | loss  4.94 | ppl   140.24\n","| epoch  33 |   100/  224 batches | lr 1.0 | loss  4.94 | ppl   140.09\n","| epoch  33 |   120/  224 batches | lr 1.0 | loss  4.96 | ppl   143.25\n","| epoch  33 |   140/  224 batches | lr 1.0 | loss  4.97 | ppl   143.92\n","| epoch  33 |   160/  224 batches | lr 1.0 | loss  5.01 | ppl   149.50\n","| epoch  33 |   180/  224 batches | lr 1.0 | loss  4.99 | ppl   146.99\n","| epoch  33 |   200/  224 batches | lr 1.0 | loss  4.94 | ppl   139.73\n","| epoch  33 |   220/  224 batches | lr 1.0 | loss  4.96 | ppl   142.74\n","Epoch: 34 | Epoch Time: 1m 20s\n","\t Val. Loss: 4.634 |  Val. PPL: 102.97\n","| epoch  34 |    20/  224 batches | lr 1.0 | loss  5.19 | ppl   178.71\n","| epoch  34 |    40/  224 batches | lr 1.0 | loss  4.91 | ppl   135.56\n","| epoch  34 |    60/  224 batches | lr 1.0 | loss  4.96 | ppl   142.94\n","| epoch  34 |    80/  224 batches | lr 1.0 | loss  4.93 | ppl   138.49\n","| epoch  34 |   100/  224 batches | lr 1.0 | loss  4.93 | ppl   138.56\n","| epoch  34 |   120/  224 batches | lr 1.0 | loss  4.95 | ppl   140.98\n","| epoch  34 |   140/  224 batches | lr 1.0 | loss  4.95 | ppl   141.56\n","| epoch  34 |   160/  224 batches | lr 1.0 | loss  4.99 | ppl   147.49\n","| epoch  34 |   180/  224 batches | lr 1.0 | loss  4.98 | ppl   145.36\n","| epoch  34 |   200/  224 batches | lr 1.0 | loss  4.93 | ppl   138.35\n","| epoch  34 |   220/  224 batches | lr 1.0 | loss  4.95 | ppl   141.01\n","Epoch: 35 | Epoch Time: 1m 20s\n","\t Val. Loss: 4.626 |  Val. PPL: 102.12\n","| epoch  35 |    20/  224 batches | lr 1.0 | loss  5.17 | ppl   176.29\n","| epoch  35 |    40/  224 batches | lr 1.0 | loss  4.89 | ppl   133.41\n","| epoch  35 |    60/  224 batches | lr 1.0 | loss  4.95 | ppl   141.27\n","| epoch  35 |    80/  224 batches | lr 1.0 | loss  4.92 | ppl   136.94\n","| epoch  35 |   100/  224 batches | lr 1.0 | loss  4.92 | ppl   137.07\n","| epoch  35 |   120/  224 batches | lr 1.0 | loss  4.94 | ppl   139.45\n","| epoch  35 |   140/  224 batches | lr 1.0 | loss  4.94 | ppl   140.13\n","| epoch  35 |   160/  224 batches | lr 1.0 | loss  4.98 | ppl   145.90\n","| epoch  35 |   180/  224 batches | lr 1.0 | loss  4.97 | ppl   143.93\n","| epoch  35 |   200/  224 batches | lr 1.0 | loss  4.91 | ppl   136.23\n","| epoch  35 |   220/  224 batches | lr 1.0 | loss  4.94 | ppl   139.37\n","Epoch: 36 | Epoch Time: 1m 20s\n","\t Val. Loss: 4.618 |  Val. PPL: 101.32\n","| epoch  36 |    20/  224 batches | lr 1.0 | loss  5.16 | ppl   173.89\n","| epoch  36 |    40/  224 batches | lr 1.0 | loss  4.88 | ppl   131.67\n","| epoch  36 |    60/  224 batches | lr 1.0 | loss  4.94 | ppl   139.39\n","| epoch  36 |    80/  224 batches | lr 1.0 | loss  4.90 | ppl   134.76\n","| epoch  36 |   100/  224 batches | lr 1.0 | loss  4.91 | ppl   135.29\n","| epoch  36 |   120/  224 batches | lr 1.0 | loss  4.93 | ppl   137.86\n","| epoch  36 |   140/  224 batches | lr 1.0 | loss  4.93 | ppl   138.31\n","| epoch  36 |   160/  224 batches | lr 1.0 | loss  4.97 | ppl   143.89\n","| epoch  36 |   180/  224 batches | lr 1.0 | loss  4.96 | ppl   142.25\n","| epoch  36 |   200/  224 batches | lr 1.0 | loss  4.90 | ppl   134.91\n","| epoch  36 |   220/  224 batches | lr 1.0 | loss  4.92 | ppl   137.47\n","Epoch: 37 | Epoch Time: 1m 20s\n","\t Val. Loss: 4.614 |  Val. PPL: 100.88\n","| epoch  37 |    20/  224 batches | lr 1.0 | loss  5.15 | ppl   171.84\n","| epoch  37 |    40/  224 batches | lr 1.0 | loss  4.87 | ppl   130.13\n","| epoch  37 |    60/  224 batches | lr 1.0 | loss  4.92 | ppl   137.43\n","| epoch  37 |    80/  224 batches | lr 1.0 | loss  4.90 | ppl   133.65\n","| epoch  37 |   100/  224 batches | lr 1.0 | loss  4.90 | ppl   133.72\n","| epoch  37 |   120/  224 batches | lr 1.0 | loss  4.91 | ppl   136.31\n","| epoch  37 |   140/  224 batches | lr 1.0 | loss  4.92 | ppl   136.83\n","| epoch  37 |   160/  224 batches | lr 1.0 | loss  4.96 | ppl   142.28\n","| epoch  37 |   180/  224 batches | lr 1.0 | loss  4.95 | ppl   140.88\n","| epoch  37 |   200/  224 batches | lr 1.0 | loss  4.89 | ppl   133.44\n","| epoch  37 |   220/  224 batches | lr 1.0 | loss  4.91 | ppl   136.11\n","Epoch: 38 | Epoch Time: 1m 20s\n","\t Val. Loss: 4.607 |  Val. PPL: 100.21\n","| epoch  38 |    20/  224 batches | lr 1.0 | loss  5.14 | ppl   169.88\n","| epoch  38 |    40/  224 batches | lr 1.0 | loss  4.86 | ppl   128.66\n","| epoch  38 |    60/  224 batches | lr 1.0 | loss  4.91 | ppl   136.21\n","| epoch  38 |    80/  224 batches | lr 1.0 | loss  4.88 | ppl   131.73\n","| epoch  38 |   100/  224 batches | lr 1.0 | loss  4.88 | ppl   132.09\n","| epoch  38 |   120/  224 batches | lr 1.0 | loss  4.91 | ppl   135.07\n","| epoch  38 |   140/  224 batches | lr 1.0 | loss  4.91 | ppl   135.17\n","| epoch  38 |   160/  224 batches | lr 1.0 | loss  4.95 | ppl   140.91\n","| epoch  38 |   180/  224 batches | lr 1.0 | loss  4.93 | ppl   138.99\n","| epoch  38 |   200/  224 batches | lr 1.0 | loss  4.88 | ppl   131.98\n","| epoch  38 |   220/  224 batches | lr 1.0 | loss  4.90 | ppl   134.49\n","Epoch: 39 | Epoch Time: 1m 20s\n","\t Val. Loss: 4.605 |  Val. PPL: 100.01\n","| epoch  39 |    20/  224 batches | lr 1.0 | loss  5.12 | ppl   167.67\n","| epoch  39 |    40/  224 batches | lr 1.0 | loss  4.85 | ppl   127.28\n","| epoch  39 |    60/  224 batches | lr 1.0 | loss  4.90 | ppl   134.73\n","| epoch  39 |    80/  224 batches | lr 1.0 | loss  4.87 | ppl   130.28\n","| epoch  39 |   100/  224 batches | lr 1.0 | loss  4.87 | ppl   130.70\n","| epoch  39 |   120/  224 batches | lr 1.0 | loss  4.89 | ppl   133.44\n","| epoch  39 |   140/  224 batches | lr 1.0 | loss  4.90 | ppl   133.83\n","| epoch  39 |   160/  224 batches | lr 1.0 | loss  4.94 | ppl   139.15\n","| epoch  39 |   180/  224 batches | lr 1.0 | loss  4.92 | ppl   137.57\n","| epoch  39 |   200/  224 batches | lr 1.0 | loss  4.87 | ppl   130.52\n","| epoch  39 |   220/  224 batches | lr 1.0 | loss  4.89 | ppl   132.76\n","Epoch: 40 | Epoch Time: 1m 20s\n","\t Val. Loss: 4.601 |  Val. PPL: 99.56\n","| epoch  40 |    20/  224 batches | lr 1.0 | loss  5.11 | ppl   165.82\n","| epoch  40 |    40/  224 batches | lr 1.0 | loss  4.84 | ppl   126.08\n","| epoch  40 |    60/  224 batches | lr 1.0 | loss  4.89 | ppl   132.85\n","| epoch  40 |    80/  224 batches | lr 1.0 | loss  4.85 | ppl   128.29\n","| epoch  40 |   100/  224 batches | lr 1.0 | loss  4.86 | ppl   129.06\n","| epoch  40 |   120/  224 batches | lr 1.0 | loss  4.88 | ppl   132.23\n","| epoch  40 |   140/  224 batches | lr 1.0 | loss  4.88 | ppl   132.03\n","| epoch  40 |   160/  224 batches | lr 1.0 | loss  4.92 | ppl   137.57\n","| epoch  40 |   180/  224 batches | lr 1.0 | loss  4.91 | ppl   136.13\n","| epoch  40 |   200/  224 batches | lr 1.0 | loss  4.86 | ppl   129.03\n","| epoch  40 |   220/  224 batches | lr 1.0 | loss  4.88 | ppl   131.23\n","Epoch: 41 | Epoch Time: 1m 20s\n","\t Val. Loss: 4.597 |  Val. PPL: 99.19\n","| epoch  41 |    20/  224 batches | lr 1.0 | loss  5.10 | ppl   163.35\n","| epoch  41 |    40/  224 batches | lr 1.0 | loss  4.82 | ppl   124.49\n","| epoch  41 |    60/  224 batches | lr 1.0 | loss  4.88 | ppl   131.38\n","| epoch  41 |    80/  224 batches | lr 1.0 | loss  4.84 | ppl   127.05\n","| epoch  41 |   100/  224 batches | lr 1.0 | loss  4.85 | ppl   127.74\n","| epoch  41 |   120/  224 batches | lr 1.0 | loss  4.87 | ppl   130.69\n","| epoch  41 |   140/  224 batches | lr 1.0 | loss  4.87 | ppl   130.82\n","| epoch  41 |   160/  224 batches | lr 1.0 | loss  4.91 | ppl   136.28\n","| epoch  41 |   180/  224 batches | lr 1.0 | loss  4.90 | ppl   134.64\n","| epoch  41 |   200/  224 batches | lr 1.0 | loss  4.85 | ppl   127.81\n","| epoch  41 |   220/  224 batches | lr 1.0 | loss  4.87 | ppl   130.19\n","Epoch: 42 | Epoch Time: 1m 20s\n","\t Val. Loss: 4.593 |  Val. PPL: 98.81\n","| epoch  42 |    20/  224 batches | lr 1.0 | loss  5.08 | ppl   161.27\n","| epoch  42 |    40/  224 batches | lr 1.0 | loss  4.81 | ppl   123.01\n","| epoch  42 |    60/  224 batches | lr 1.0 | loss  4.87 | ppl   129.93\n","| epoch  42 |    80/  224 batches | lr 1.0 | loss  4.83 | ppl   125.30\n","| epoch  42 |   100/  224 batches | lr 1.0 | loss  4.84 | ppl   126.19\n","| epoch  42 |   120/  224 batches | lr 1.0 | loss  4.86 | ppl   129.01\n","| epoch  42 |   140/  224 batches | lr 1.0 | loss  4.87 | ppl   129.67\n","| epoch  42 |   160/  224 batches | lr 1.0 | loss  4.90 | ppl   134.65\n","| epoch  42 |   180/  224 batches | lr 1.0 | loss  4.89 | ppl   133.37\n","| epoch  42 |   200/  224 batches | lr 1.0 | loss  4.84 | ppl   126.32\n","| epoch  42 |   220/  224 batches | lr 1.0 | loss  4.86 | ppl   128.48\n","Epoch: 43 | Epoch Time: 1m 20s\n","\t Val. Loss: 4.588 |  Val. PPL: 98.26\n","| epoch  43 |    20/  224 batches | lr 1.0 | loss  5.07 | ppl   159.57\n","| epoch  43 |    40/  224 batches | lr 1.0 | loss  4.80 | ppl   121.70\n","| epoch  43 |    60/  224 batches | lr 1.0 | loss  4.86 | ppl   128.80\n","| epoch  43 |    80/  224 batches | lr 1.0 | loss  4.83 | ppl   124.64\n","| epoch  43 |   100/  224 batches | lr 1.0 | loss  4.83 | ppl   124.66\n","| epoch  43 |   120/  224 batches | lr 1.0 | loss  4.85 | ppl   127.74\n","| epoch  43 |   140/  224 batches | lr 1.0 | loss  4.85 | ppl   128.18\n","| epoch  43 |   160/  224 batches | lr 1.0 | loss  4.89 | ppl   133.59\n","| epoch  43 |   180/  224 batches | lr 1.0 | loss  4.88 | ppl   132.05\n","| epoch  43 |   200/  224 batches | lr 1.0 | loss  4.83 | ppl   124.94\n","| epoch  43 |   220/  224 batches | lr 1.0 | loss  4.84 | ppl   127.06\n","Epoch: 44 | Epoch Time: 1m 20s\n","\t Val. Loss: 4.584 |  Val. PPL: 97.92\n","| epoch  44 |    20/  224 batches | lr 1.0 | loss  5.06 | ppl   158.15\n","| epoch  44 |    40/  224 batches | lr 1.0 | loss  4.79 | ppl   120.33\n","| epoch  44 |    60/  224 batches | lr 1.0 | loss  4.85 | ppl   127.57\n","| epoch  44 |    80/  224 batches | lr 1.0 | loss  4.81 | ppl   123.01\n","| epoch  44 |   100/  224 batches | lr 1.0 | loss  4.82 | ppl   123.70\n","| epoch  44 |   120/  224 batches | lr 1.0 | loss  4.84 | ppl   126.59\n","| epoch  44 |   140/  224 batches | lr 1.0 | loss  4.84 | ppl   126.86\n","| epoch  44 |   160/  224 batches | lr 1.0 | loss  4.88 | ppl   132.23\n","| epoch  44 |   180/  224 batches | lr 1.0 | loss  4.87 | ppl   130.58\n","| epoch  44 |   200/  224 batches | lr 1.0 | loss  4.82 | ppl   123.53\n","| epoch  44 |   220/  224 batches | lr 1.0 | loss  4.84 | ppl   126.13\n","Epoch: 45 | Epoch Time: 1m 20s\n","\t Val. Loss: 4.581 |  Val. PPL: 97.62\n","| epoch  45 |    20/  224 batches | lr 1.0 | loss  5.05 | ppl   155.93\n","| epoch  45 |    40/  224 batches | lr 1.0 | loss  4.78 | ppl   119.15\n","| epoch  45 |    60/  224 batches | lr 1.0 | loss  4.84 | ppl   126.12\n","| epoch  45 |    80/  224 batches | lr 1.0 | loss  4.80 | ppl   121.95\n","| epoch  45 |   100/  224 batches | lr 1.0 | loss  4.81 | ppl   122.52\n","| epoch  45 |   120/  224 batches | lr 1.0 | loss  4.83 | ppl   125.42\n","| epoch  45 |   140/  224 batches | lr 1.0 | loss  4.83 | ppl   125.76\n","| epoch  45 |   160/  224 batches | lr 1.0 | loss  4.87 | ppl   130.96\n","| epoch  45 |   180/  224 batches | lr 1.0 | loss  4.86 | ppl   129.52\n","| epoch  45 |   200/  224 batches | lr 1.0 | loss  4.81 | ppl   122.43\n","| epoch  45 |   220/  224 batches | lr 1.0 | loss  4.82 | ppl   124.58\n","Epoch: 46 | Epoch Time: 1m 20s\n","\t Val. Loss: 4.577 |  Val. PPL: 97.18\n","| epoch  46 |    20/  224 batches | lr 1.0 | loss  5.04 | ppl   154.54\n","| epoch  46 |    40/  224 batches | lr 1.0 | loss  4.77 | ppl   118.10\n","| epoch  46 |    60/  224 batches | lr 1.0 | loss  4.83 | ppl   125.14\n","| epoch  46 |    80/  224 batches | lr 1.0 | loss  4.79 | ppl   120.57\n","| epoch  46 |   100/  224 batches | lr 1.0 | loss  4.80 | ppl   121.28\n","| epoch  46 |   120/  224 batches | lr 1.0 | loss  4.82 | ppl   123.93\n","| epoch  46 |   140/  224 batches | lr 1.0 | loss  4.82 | ppl   124.46\n","| epoch  46 |   160/  224 batches | lr 1.0 | loss  4.86 | ppl   129.32\n","| epoch  46 |   180/  224 batches | lr 1.0 | loss  4.85 | ppl   128.15\n","| epoch  46 |   200/  224 batches | lr 1.0 | loss  4.80 | ppl   121.49\n","| epoch  46 |   220/  224 batches | lr 1.0 | loss  4.81 | ppl   123.33\n","Epoch: 47 | Epoch Time: 1m 20s\n","\t Val. Loss: 4.573 |  Val. PPL: 96.83\n","| epoch  47 |    20/  224 batches | lr 1.0 | loss  5.03 | ppl   153.00\n","| epoch  47 |    40/  224 batches | lr 1.0 | loss  4.76 | ppl   116.91\n","| epoch  47 |    60/  224 batches | lr 1.0 | loss  4.82 | ppl   123.74\n","| epoch  47 |    80/  224 batches | lr 1.0 | loss  4.79 | ppl   119.77\n","| epoch  47 |   100/  224 batches | lr 1.0 | loss  4.79 | ppl   119.92\n","| epoch  47 |   120/  224 batches | lr 1.0 | loss  4.81 | ppl   123.03\n","| epoch  47 |   140/  224 batches | lr 1.0 | loss  4.81 | ppl   123.21\n","| epoch  47 |   160/  224 batches | lr 1.0 | loss  4.85 | ppl   128.32\n","| epoch  47 |   180/  224 batches | lr 1.0 | loss  4.84 | ppl   127.01\n","| epoch  47 |   200/  224 batches | lr 1.0 | loss  4.79 | ppl   120.30\n","| epoch  47 |   220/  224 batches | lr 1.0 | loss  4.81 | ppl   122.31\n","Epoch: 48 | Epoch Time: 1m 20s\n","\t Val. Loss: 4.569 |  Val. PPL: 96.41\n","| epoch  48 |    20/  224 batches | lr 1.0 | loss  5.02 | ppl   151.54\n","| epoch  48 |    40/  224 batches | lr 1.0 | loss  4.75 | ppl   115.84\n","| epoch  48 |    60/  224 batches | lr 1.0 | loss  4.81 | ppl   122.63\n","| epoch  48 |    80/  224 batches | lr 1.0 | loss  4.77 | ppl   118.40\n","| epoch  48 |   100/  224 batches | lr 1.0 | loss  4.78 | ppl   119.20\n","| epoch  48 |   120/  224 batches | lr 1.0 | loss  4.80 | ppl   121.63\n","| epoch  48 |   140/  224 batches | lr 1.0 | loss  4.80 | ppl   121.92\n","| epoch  48 |   160/  224 batches | lr 1.0 | loss  4.84 | ppl   126.88\n","| epoch  48 |   180/  224 batches | lr 1.0 | loss  4.83 | ppl   125.62\n","| epoch  48 |   200/  224 batches | lr 1.0 | loss  4.78 | ppl   119.18\n","| epoch  48 |   220/  224 batches | lr 1.0 | loss  4.80 | ppl   121.12\n","Epoch: 49 | Epoch Time: 1m 20s\n","\t Val. Loss: 4.572 |  Val. PPL: 96.70\n","| epoch  49 |    20/  224 batches | lr 0.25 | loss  5.00 | ppl   148.75\n","| epoch  49 |    40/  224 batches | lr 0.25 | loss  4.74 | ppl   113.92\n","| epoch  49 |    60/  224 batches | lr 0.25 | loss  4.79 | ppl   120.27\n","| epoch  49 |    80/  224 batches | lr 0.25 | loss  4.75 | ppl   115.73\n","| epoch  49 |   100/  224 batches | lr 0.25 | loss  4.76 | ppl   116.36\n","| epoch  49 |   120/  224 batches | lr 0.25 | loss  4.78 | ppl   118.77\n","| epoch  49 |   140/  224 batches | lr 0.25 | loss  4.78 | ppl   118.72\n","| epoch  49 |   160/  224 batches | lr 0.25 | loss  4.81 | ppl   123.33\n","| epoch  49 |   180/  224 batches | lr 0.25 | loss  4.80 | ppl   121.64\n","| epoch  49 |   200/  224 batches | lr 0.25 | loss  4.75 | ppl   115.02\n","| epoch  49 |   220/  224 batches | lr 0.25 | loss  4.76 | ppl   116.49\n","Epoch: 50 | Epoch Time: 1m 20s\n","\t Val. Loss: 4.555 |  Val. PPL: 95.11\n","| epoch  50 |    20/  224 batches | lr 0.25 | loss  4.99 | ppl   147.01\n","| epoch  50 |    40/  224 batches | lr 0.25 | loss  4.73 | ppl   112.76\n","| epoch  50 |    60/  224 batches | lr 0.25 | loss  4.78 | ppl   119.40\n","| epoch  50 |    80/  224 batches | lr 0.25 | loss  4.75 | ppl   115.16\n","| epoch  50 |   100/  224 batches | lr 0.25 | loss  4.75 | ppl   115.82\n","| epoch  50 |   120/  224 batches | lr 0.25 | loss  4.77 | ppl   118.15\n","| epoch  50 |   140/  224 batches | lr 0.25 | loss  4.77 | ppl   118.18\n","| epoch  50 |   160/  224 batches | lr 0.25 | loss  4.81 | ppl   122.58\n","| epoch  50 |   180/  224 batches | lr 0.25 | loss  4.80 | ppl   121.30\n","| epoch  50 |   200/  224 batches | lr 0.25 | loss  4.74 | ppl   114.56\n","| epoch  50 |   220/  224 batches | lr 0.25 | loss  4.76 | ppl   116.19\n","Epoch: 51 | Epoch Time: 1m 20s\n","\t Val. Loss: 4.554 |  Val. PPL: 95.05\n","| epoch  51 |    20/  224 batches | lr 0.25 | loss  4.99 | ppl   146.53\n","| epoch  51 |    40/  224 batches | lr 0.25 | loss  4.72 | ppl   112.50\n","| epoch  51 |    60/  224 batches | lr 0.25 | loss  4.78 | ppl   118.65\n","| epoch  51 |    80/  224 batches | lr 0.25 | loss  4.74 | ppl   114.52\n","| epoch  51 |   100/  224 batches | lr 0.25 | loss  4.74 | ppl   114.97\n","| epoch  51 |   120/  224 batches | lr 0.25 | loss  4.77 | ppl   117.74\n","| epoch  51 |   140/  224 batches | lr 0.25 | loss  4.77 | ppl   117.49\n","| epoch  51 |   160/  224 batches | lr 0.25 | loss  4.81 | ppl   122.54\n","| epoch  51 |   180/  224 batches | lr 0.25 | loss  4.79 | ppl   120.63\n","| epoch  51 |   200/  224 batches | lr 0.25 | loss  4.74 | ppl   114.38\n","| epoch  51 |   220/  224 batches | lr 0.25 | loss  4.76 | ppl   116.18\n","Epoch: 52 | Epoch Time: 1m 20s\n","\t Val. Loss: 4.553 |  Val. PPL: 94.94\n","| epoch  52 |    20/  224 batches | lr 0.25 | loss  4.98 | ppl   145.85\n","| epoch  52 |    40/  224 batches | lr 0.25 | loss  4.72 | ppl   111.90\n","| epoch  52 |    60/  224 batches | lr 0.25 | loss  4.77 | ppl   118.16\n","| epoch  52 |    80/  224 batches | lr 0.25 | loss  4.74 | ppl   114.16\n","| epoch  52 |   100/  224 batches | lr 0.25 | loss  4.74 | ppl   114.62\n","| epoch  52 |   120/  224 batches | lr 0.25 | loss  4.77 | ppl   117.53\n","| epoch  52 |   140/  224 batches | lr 0.25 | loss  4.76 | ppl   117.30\n","| epoch  52 |   160/  224 batches | lr 0.25 | loss  4.80 | ppl   121.85\n","| epoch  52 |   180/  224 batches | lr 0.25 | loss  4.79 | ppl   120.57\n","| epoch  52 |   200/  224 batches | lr 0.25 | loss  4.74 | ppl   114.18\n","| epoch  52 |   220/  224 batches | lr 0.25 | loss  4.75 | ppl   115.99\n","Epoch: 53 | Epoch Time: 1m 20s\n","\t Val. Loss: 4.552 |  Val. PPL: 94.80\n","| epoch  53 |    20/  224 batches | lr 0.25 | loss  4.98 | ppl   145.29\n","| epoch  53 |    40/  224 batches | lr 0.25 | loss  4.71 | ppl   111.42\n","| epoch  53 |    60/  224 batches | lr 0.25 | loss  4.77 | ppl   117.70\n","| epoch  53 |    80/  224 batches | lr 0.25 | loss  4.73 | ppl   113.69\n","| epoch  53 |   100/  224 batches | lr 0.25 | loss  4.74 | ppl   114.30\n","| epoch  53 |   120/  224 batches | lr 0.25 | loss  4.76 | ppl   117.04\n","| epoch  53 |   140/  224 batches | lr 0.25 | loss  4.76 | ppl   116.66\n","| epoch  53 |   160/  224 batches | lr 0.25 | loss  4.80 | ppl   121.80\n","| epoch  53 |   180/  224 batches | lr 0.25 | loss  4.79 | ppl   120.29\n","| epoch  53 |   200/  224 batches | lr 0.25 | loss  4.73 | ppl   113.51\n","| epoch  53 |   220/  224 batches | lr 0.25 | loss  4.75 | ppl   115.71\n","Epoch: 54 | Epoch Time: 1m 20s\n","\t Val. Loss: 4.551 |  Val. PPL: 94.71\n","| epoch  54 |    20/  224 batches | lr 0.25 | loss  4.98 | ppl   144.78\n","| epoch  54 |    40/  224 batches | lr 0.25 | loss  4.71 | ppl   111.02\n","| epoch  54 |    60/  224 batches | lr 0.25 | loss  4.76 | ppl   117.19\n","| epoch  54 |    80/  224 batches | lr 0.25 | loss  4.73 | ppl   113.20\n","| epoch  54 |   100/  224 batches | lr 0.25 | loss  4.74 | ppl   113.88\n","| epoch  54 |   120/  224 batches | lr 0.25 | loss  4.76 | ppl   116.27\n","| epoch  54 |   140/  224 batches | lr 0.25 | loss  4.76 | ppl   116.72\n","| epoch  54 |   160/  224 batches | lr 0.25 | loss  4.80 | ppl   121.26\n","| epoch  54 |   180/  224 batches | lr 0.25 | loss  4.78 | ppl   119.69\n","| epoch  54 |   200/  224 batches | lr 0.25 | loss  4.73 | ppl   113.49\n","| epoch  54 |   220/  224 batches | lr 0.25 | loss  4.75 | ppl   115.16\n","Epoch: 55 | Epoch Time: 1m 20s\n","\t Val. Loss: 4.550 |  Val. PPL: 94.64\n","| epoch  55 |    20/  224 batches | lr 0.25 | loss  4.97 | ppl   144.47\n","| epoch  55 |    40/  224 batches | lr 0.25 | loss  4.71 | ppl   110.68\n","| epoch  55 |    60/  224 batches | lr 0.25 | loss  4.76 | ppl   116.90\n","| epoch  55 |    80/  224 batches | lr 0.25 | loss  4.72 | ppl   112.66\n","| epoch  55 |   100/  224 batches | lr 0.25 | loss  4.73 | ppl   113.50\n","| epoch  55 |   120/  224 batches | lr 0.25 | loss  4.75 | ppl   115.98\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-16-a85b8ac5fe51>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mvalid_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-13-158e7f5fa71d>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, iterator, criterion)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    196\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \"\"\"\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     99\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"KxqywU7SfFqw","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"executionInfo":{"status":"ok","timestamp":1593188025217,"user_tz":-120,"elapsed":1441,"user":{"displayName":"Daniel Cuiñas Vázquez","photoUrl":"","userId":"11552885780691803973"}},"outputId":"b0d2eb9c-2220-4c98-a0ec-67932edfe224"},"source":["# Run on test data.\n","test_loss = evaluate(model, test_iter, criterion)\n","print('=' * 89)\n","print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\n","    test_loss, math.exp(test_loss)))\n","print('=' * 89)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["=========================================================================================\n","| End of training | test loss  4.32 | test ppl    75.13\n","=========================================================================================\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"pB4avh9Rjk-g","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2MDJATwDqIGd","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}