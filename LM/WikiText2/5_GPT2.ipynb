{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"GPT2_torchtext_LM.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNrXTRSScRmHhqE1bcdl0YV"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"rKk-B0Tpp9gv","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593546632831,"user_tz":-120,"elapsed":7322,"user":{"displayName":"Daniel Cuiñas Vázquez","photoUrl":"","userId":"11552885780691803973"}}},"source":["import torch\n","from torchtext import data\n","import spacy\n","from spacy.symbols import ORTH\n","from torchtext.datasets import WikiText2\n","\n","my_tok = spacy.load('en')\n"," \n","def spacy_tok(x):\n","    return [tok.text for tok in my_tok.tokenizer(x)]\n"," \n","TEXT = data.Field(lower=True, tokenize=spacy_tok)"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"4XVeD2NoqIYC","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593546644812,"user_tz":-120,"elapsed":824,"user":{"displayName":"Daniel Cuiñas Vázquez","photoUrl":"","userId":"11552885780691803973"}}},"source":["my_tok.tokenizer.add_special_case(\"don't\", [{ORTH: \"do\"}, {ORTH: \"n't\"}])\n"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZxixGykKqJ-s","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"executionInfo":{"status":"ok","timestamp":1593546667344,"user_tz":-120,"elapsed":14569,"user":{"displayName":"Daniel Cuiñas Vázquez","photoUrl":"","userId":"11552885780691803973"}},"outputId":"58284cc7-4d64-46cb-9a73-5ada7fc5d927"},"source":["train, valid, test = WikiText2.splits(TEXT) "],"execution_count":3,"outputs":[{"output_type":"stream","text":["\rwikitext-2-v1.zip:   0%|          | 0.00/4.48M [00:00<?, ?B/s]"],"name":"stderr"},{"output_type":"stream","text":["downloading wikitext-2-v1.zip\n"],"name":"stdout"},{"output_type":"stream","text":["wikitext-2-v1.zip: 100%|██████████| 4.48M/4.48M [00:00<00:00, 38.8MB/s]\n"],"name":"stderr"},{"output_type":"stream","text":["extracting\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"oTAK6AAOqLdo","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593546689768,"user_tz":-120,"elapsed":886,"user":{"displayName":"Daniel Cuiñas Vázquez","photoUrl":"","userId":"11552885780691803973"}}},"source":["TEXT.build_vocab(train)"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"OWgVlKU5qOQp","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593546691176,"user_tz":-120,"elapsed":512,"user":{"displayName":"Daniel Cuiñas Vázquez","photoUrl":"","userId":"11552885780691803973"}}},"source":["batch_size = 50\n","bptt = 200"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"eQbmYqYgqROg","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1593546694192,"user_tz":-120,"elapsed":570,"user":{"displayName":"Daniel Cuiñas Vázquez","photoUrl":"","userId":"11552885780691803973"}},"outputId":"f749a718-3731-406a-c151-9f1d0a5d727a"},"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(device)"],"execution_count":6,"outputs":[{"output_type":"stream","text":["cuda\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"SkMHHca1qSyQ","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593546695183,"user_tz":-120,"elapsed":369,"user":{"displayName":"Daniel Cuiñas Vázquez","photoUrl":"","userId":"11552885780691803973"}}},"source":["train_iter, valid_iter, test_iter = data.BPTTIterator.splits(\n","    (train, valid, test),\n","    batch_size=batch_size,\n","    bptt_len=bptt, \n","    device=device,\n","    repeat=False)"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"KvCXAZheqUch","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593546696077,"user_tz":-120,"elapsed":388,"user":{"displayName":"Daniel Cuiñas Vázquez","photoUrl":"","userId":"11552885780691803973"}}},"source":["import torch\n","import torch.nn as nn\n","import numpy as np\n","import math\n","import torch.nn.functional as F\n","\n","\n","def create_sinusoidal_embeddings(embeds):\n","    position_enc = torch.tensor([\n","        [pos / np.power(10000, 2 * (j // 2) / embeds.embedding_dim) for j in range(embeds.embedding_dim)]\n","                                                                    for pos in range(embeds.num_embeddings)])\n","    embeds.weight[:, 0::2] = torch.sin(position_enc[:, 0::2])\n","    embeds.weight[:, 1::2] = torch.cos(position_enc[:, 1::2])\n","    embeds.weight.detach_()\n","    embeds.weight.requires_grad = False\n","\n","\n","class Transformer(nn.Module):\n","    def __init__(self, embed_dim, hidden_dim, num_embeddings, num_max_positions, num_heads, num_layers, dropout,\n","                 sinusoidal_embeddings, causal=False):\n","        \"\"\" Transformer (GPT-2 architecture) \"\"\"\n","        super().__init__()\n","        self.causal = causal\n","        self.tokens_embeddings = nn.Embedding(num_embeddings, embed_dim)\n","        self.position_embeddings = nn.Embedding(num_max_positions, embed_dim)\n","        if sinusoidal_embeddings:\n","            create_sinusoidal_embeddings(self.position_embeddings)\n","        self.dropout = nn.Dropout(dropout)\n","\n","        self.attentions, self.feed_forwards = nn.ModuleList(), nn.ModuleList()\n","        self.layer_norms_1, self.layer_norms_2 = nn.ModuleList(), nn.ModuleList()\n","        for _ in range(num_layers):\n","            self.attentions.append(nn.MultiheadAttention(embed_dim, num_heads, dropout = dropout))\n","            self.feed_forwards.append(nn.Sequential(nn.Linear(embed_dim, hidden_dim),\n","                                                    nn.ReLU(),\n","                                                    nn.Linear(hidden_dim, embed_dim)))\n","            self.layer_norms_1.append(nn.LayerNorm(embed_dim, eps=1e-12))\n","            self.layer_norms_2.append(nn.LayerNorm(embed_dim, eps=1e-12))\n","\n","    def forward(self, x, padding_mask = None):\n","        \"\"\" Input has shape [seq length, batch] \"\"\"\n","        positions = torch.arange(len(x), device=x.device).unsqueeze(-1)\n","        h = self.tokens_embeddings(x)\n","        h = h + self.position_embeddings(positions).expand_as(h)\n","        h = self.dropout(h)\n","\n","        attn_mask = None\n","        if self.causal:\n","            attn_mask = torch.full((len(x), len(x)), -float('Inf'), device=h.device, dtype=h.dtype)\n","            attn_mask = torch.triu(attn_mask, diagonal = 1)\n","        for layer_norm_1, attention, layer_norm_2, feed_forward in zip(self.layer_norms_1, self.attentions,\n","                                                                       self.layer_norms_2, self.feed_forwards):\n","            h = layer_norm_1(h)\n","            x, _ = attention(h, h, h, attn_mask = attn_mask, need_weights = False, key_padding_mask = padding_mask)\n","            x = self.dropout(x)\n","            h = x + h\n","\n","            h = layer_norm_2(h)\n","            x = feed_forward(h)\n","            x = self.dropout(x)\n","            h = x + h\n","        return h\n","\n","\n","class TransformerWithLMHead(nn.Module):\n","    def __init__(self, embed_dim, \n","                 hidden_dim, \n","                 num_embeddings,\n","                 num_max_positions, \n","                 num_heads, \n","                 num_layers,\n","                 dropout, \n","                 sinusoidal_embeddings, \n","                 mlm,\n","                 initializer_range):\n","      \n","        \"\"\" Transformer with a language modeling head on top (tied weights) \"\"\"\n","\n","        super().__init__()\n","        self.transformer = Transformer(embed_dim, hidden_dim, num_embeddings,\n","                                       num_max_positions, num_heads, num_layers,\n","                                       dropout, sinusoidal_embeddings, causal=not mlm)\n","        self.lm_head = nn.Linear(embed_dim, num_embeddings, bias=False)\n","        self.apply(self.init_weights)\n","        self.tie_weights()\n","\n","    def tie_weights(self):\n","        self.lm_head.weight = self.transformer.tokens_embeddings.weight\n","\n","    def init_weights(self, module):\n","        \"\"\" initialize weights - note that nn.MultiheadAttention is already initalized by PyTorch (xavier_uniform) \"\"\"\n","        if isinstance(module, (nn.Linear, nn.Embedding, nn.LayerNorm)):\n","            module.weight.data.normal_(mean=0.0, std=initializer_range)\n","        if isinstance(module, (nn.Linear, nn.LayerNorm)) and module.bias is not None:\n","            module.bias.data.zero_()\n","\n","    def forward(self, x, padding_mask=None):\n","        \"\"\" Input has shape [seq length, batch] \"\"\"\n","        hidden_states = self.transformer(x, padding_mask)\n","        logits = self.lm_head(hidden_states)\n","\n","        logits = F.log_softmax(logits, dim=-1)\n","\n","        return logits"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"rxj4FkrfrHEw","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593546824556,"user_tz":-120,"elapsed":673,"user":{"displayName":"Daniel Cuiñas Vázquez","photoUrl":"","userId":"11552885780691803973"}}},"source":["embed_dim = 128\n","hidden_dim = 128\n","num_embeddings = len(TEXT.vocab.itos)\n","num_max_positions = 500\n","num_heads = 4\n","num_layers = 4\n","dropout = 0.1\n","sinusoidal_embeddings = True\n","mlm = True\n","causal = not mlm\n","initializer_range = 0.02\n","# lr = 2.5e-4\n","lr = 4\n","# weight_decay = 0.0\n","# gradient_accumulation_steps = 1\n","# max_norm = 0.25\n","log_interval = 20\n","\n","model = TransformerWithLMHead(embed_dim, \n","                 hidden_dim, \n","                 num_embeddings,\n","                 num_max_positions, \n","                 num_heads, \n","                 num_layers,\n","                 dropout, \n","                 sinusoidal_embeddings, \n","                 mlm,\n","                 initializer_range)"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"Eq_IzMTErpjh","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1593546826361,"user_tz":-120,"elapsed":478,"user":{"displayName":"Daniel Cuiñas Vázquez","photoUrl":"","userId":"11552885780691803973"}},"outputId":"abf44d9c-d81e-46ef-e264-0df7186754b0"},"source":["def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","print(f'The model has {count_parameters(model):,} trainable parameters')"],"execution_count":10,"outputs":[{"output_type":"stream","text":["The model has 4,093,696 trainable parameters\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"NAlL0bA_rrv-","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593546860212,"user_tz":-120,"elapsed":9936,"user":{"displayName":"Daniel Cuiñas Vázquez","photoUrl":"","userId":"11552885780691803973"}}},"source":["import torch.optim as optim\n","\n","optimizer = optim.Adam(model.parameters(), lr = lr)\n","\n","criterion = nn.NLLLoss()\n","model=model.to(device)\n","criterion = criterion.to(device)\n"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"KuZI1sWCsuKI","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593546866974,"user_tz":-120,"elapsed":481,"user":{"displayName":"Daniel Cuiñas Vázquez","photoUrl":"","userId":"11552885780691803973"}}},"source":["def train(model, iterator, criterion):\n","    clip = 0.25\n","    total_loss = 0\n","    \n","    model.train()\n","            \n","    for k, batch in enumerate(iterator):\n","        data = batch.text\n","        targets = batch.target.view(-1)\n","\n","        data = data.to(device)\n","        targets = targets.to(device)\n","\n","        model.zero_grad()\n","      \n","        output = model(data) \n","\n","        output = output.view(-1, num_embeddings)\n","        \n","        loss = criterion(output, targets)\n","                \n","        loss.backward()\n","        \n","        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n","\n","        # optimizer.step()\n","        for p in model.parameters():\n","            p.data.add_(-lr, p.grad.data)\n","        \n","        total_loss += loss.item()\n","        \n","        if k % log_interval == 0 and k > 0:\n","            cur_loss = total_loss / log_interval\n","            print('| epoch {:3d} | {:5d}/{:5d} batches | lr {} | '\n","                    'loss {:5.2f} | ppl {:8.2f}'.format(epoch, k, len(iterator), lr, cur_loss, math.exp(cur_loss)))\n","            total_loss = 0\n"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"hmuLax3Ss2PE","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593546868886,"user_tz":-120,"elapsed":477,"user":{"displayName":"Daniel Cuiñas Vázquez","photoUrl":"","userId":"11552885780691803973"}}},"source":["def evaluate(model, iterator, criterion):\n","    \n","    total_loss = 0\n","    \n","    model.eval()\n","        \n","    with torch.no_grad():\n","    \n","        for batch in iterator:\n","            data = batch.text\n","            targets = batch.target.view(-1)\n","\n","            data = data.to(device)\n","            targets = targets.to(device)\n","\n","            output = model(data)\n","\n","            output = output.view(-1, num_embeddings)\n","            \n","            loss = criterion(output, targets).item()\n","\n","            total_loss += len(data) * loss\n","\n","        \n","    return total_loss / (len(iterator)*bptt - 1)"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"7hSGZMjOs2e3","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593546870991,"user_tz":-120,"elapsed":467,"user":{"displayName":"Daniel Cuiñas Vázquez","photoUrl":"","userId":"11552885780691803973"}}},"source":["import time\n","\n","def epoch_time(start_time, end_time):\n","    elapsed_time = end_time - start_time\n","    elapsed_mins = int(elapsed_time / 60)\n","    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n","    return elapsed_mins, elapsed_secs"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"id":"CxJkniAws4QZ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":418},"executionInfo":{"status":"error","timestamp":1593546874057,"user_tz":-120,"elapsed":1372,"user":{"displayName":"Daniel Cuiñas Vázquez","photoUrl":"","userId":"11552885780691803973"}},"outputId":"d76015bb-3cef-47f0-ec86-2a266aafa5c4"},"source":["N_EPOCHS = 100\n","\n","best_valid_loss = float('inf')\n","counter = 0\n","patience = 5\n","\n","for epoch in range(N_EPOCHS):\n","\n","    start_time = time.time()\n","    \n","    train(model, train_iter, criterion)\n","    valid_loss = evaluate(model, valid_iter, criterion)\n","    \n","    end_time = time.time()\n","\n","    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n","\n","    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n","    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):.2f}')\n","\n","    if valid_loss < best_valid_loss:\n","        best_valid_loss = valid_loss\n","        #torch.save(model.state_dict(), 'tut2-model.pt')\n","        counter = 0 \n","    else:\n","        lr /= 4.0\n","        counter += 1\n","        if counter >= patience:\n","            break\n","\n","    "],"execution_count":16,"outputs":[{"output_type":"stream","text":["/pytorch/torch/csrc/utils/python_arg_parser.cpp:756: UserWarning: This overload of add_ is deprecated:\n","\tadd_(Number alpha, Tensor other)\n","Consider using one of the following signatures instead:\n","\tadd_(Tensor other, *, Number alpha)\n"],"name":"stderr"},{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-16-a85b8ac5fe51>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mvalid_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-13-45a879d20811>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, iterator, criterion)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m# optimizer.step()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'data'"]}]},{"cell_type":"code","metadata":{"id":"O247ASe5s7WD","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":231},"executionInfo":{"status":"error","timestamp":1592518150104,"user_tz":-120,"elapsed":1155,"user":{"displayName":"Daniel Cuiñas Vázquez","photoUrl":"","userId":"11552885780691803973"}},"outputId":"2fce69a6-16bc-4e05-f7d4-0a70a701c5a0"},"source":["# Run on test data.\n","test_loss = evaluate(model, test_iter, criterion)\n","print('=' * 89)\n","print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\n","    test_loss, math.exp(test_loss)))\n","print('=' * 89)"],"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-b38c5136faf0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Run on test data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtest_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'='\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m89\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\n\u001b[1;32m      5\u001b[0m     test_loss, math.exp(test_loss)))\n","\u001b[0;31mNameError\u001b[0m: name 'evaluate' is not defined"]}]},{"cell_type":"code","metadata":{"id":"EpU-ZHfTeoLD","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}