{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"GPT2_torchtext_masked_LM.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMMfO8WneIBxEqjBV/LQIOA"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"K4lgWNN9iiN8","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1592504092595,"user_tz":-120,"elapsed":1812,"user":{"displayName":"Daniel Cuiñas Vázquez","photoUrl":"","userId":"11552885780691803973"}}},"source":["import torch\n","from torchtext import data\n","import spacy\n","from spacy.symbols import ORTH\n","from torchtext.datasets import WikiText2\n","\n","my_tok = spacy.load('en')\n"," \n","def spacy_tok(x):\n","    return [tok.text for tok in my_tok.tokenizer(x)]\n"," \n","TEXT = data.Field(lower=True, tokenize=spacy_tok)\n"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"aHGVASu8iogy","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1592504092597,"user_tz":-120,"elapsed":1798,"user":{"displayName":"Daniel Cuiñas Vázquez","photoUrl":"","userId":"11552885780691803973"}}},"source":["my_tok.tokenizer.add_special_case(\"don't\", [{ORTH: \"do\"}, {ORTH: \"n't\"}])\n"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"msH4kWcYiqGO","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1592504112227,"user_tz":-120,"elapsed":21413,"user":{"displayName":"Daniel Cuiñas Vázquez","photoUrl":"","userId":"11552885780691803973"}}},"source":["train, valid, test = WikiText2.splits(TEXT) "],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"sNzZ09cairyv","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1592504112429,"user_tz":-120,"elapsed":21605,"user":{"displayName":"Daniel Cuiñas Vázquez","photoUrl":"","userId":"11552885780691803973"}}},"source":["TEXT.build_vocab(train)"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"QiJ9kTDuixKw","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1592504112671,"user_tz":-120,"elapsed":21829,"user":{"displayName":"Daniel Cuiñas Vázquez","photoUrl":"","userId":"11552885780691803973"}}},"source":["TEXT.vocab.itos = TEXT.vocab.itos + ['[MASK]']\n","TEXT.vocab.stoi['[MASK]']=len(TEXT.vocab.itos)"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"NCGtmfwliyu7","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1592504112672,"user_tz":-120,"elapsed":21805,"user":{"displayName":"Daniel Cuiñas Vázquez","photoUrl":"","userId":"11552885780691803973"}}},"source":["batch_size = 50\n","bptt = 200"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"8rYYhfEOi0JN","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1592504112673,"user_tz":-120,"elapsed":21794,"user":{"displayName":"Daniel Cuiñas Vázquez","photoUrl":"","userId":"11552885780691803973"}},"outputId":"bd9d6084-fa8d-4ae7-d182-0233a17a5619"},"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(device)"],"execution_count":7,"outputs":[{"output_type":"stream","text":["cuda\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"4NoPxsWai1gX","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1592504112674,"user_tz":-120,"elapsed":21784,"user":{"displayName":"Daniel Cuiñas Vázquez","photoUrl":"","userId":"11552885780691803973"}}},"source":["train_iter, valid_iter, test_iter = data.BPTTIterator.splits(\n","    (train, valid, test),\n","    batch_size=batch_size,\n","    bptt_len=bptt, \n","    device=device,\n","    repeat=False)"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"RjTJ9JNui3Zs","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1592504112674,"user_tz":-120,"elapsed":21776,"user":{"displayName":"Daniel Cuiñas Vázquez","photoUrl":"","userId":"11552885780691803973"}}},"source":["mlm_probability = 0.15\n","vocab_size = len(TEXT.vocab)+1\n","\n","def mask_tokens(inputs):\n","    labels = inputs.clone()\n","    masked_indices = torch.bernoulli(torch.full(labels.shape, mlm_probability)).bool()\n","    labels[~masked_indices] = -1                            # We only compute loss on masked tokens\n","    indices_replaced = torch.bernoulli(torch.full(labels.shape, 0.8)).bool() & masked_indices\n","    inputs[indices_replaced] = TEXT.vocab.stoi['[MASK]']    # 80% of the time, replace masked input tokens with [MASK]\n","    indices_random = torch.bernoulli(torch.full(labels.shape, 0.5)).bool() & masked_indices & ~indices_replaced\n","    random_words = torch.randint(vocab_size, labels.shape, dtype=torch.long, device=device)\n","    inputs[indices_random] = random_words[indices_random]   # 10% of the time, replace masked input tokens with random word\n","    return inputs, labels"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"epy9OVM1jJGa","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1592504112873,"user_tz":-120,"elapsed":21964,"user":{"displayName":"Daniel Cuiñas Vázquez","photoUrl":"","userId":"11552885780691803973"}}},"source":["import torch\n","import torch.nn as nn\n","import numpy as np\n","\n","def create_sinusoidal_embeddings(embeds):\n","    position_enc = torch.tensor([\n","        [pos / np.power(10000, 2 * (j // 2) / embeds.embedding_dim) for j in range(embeds.embedding_dim)]\n","                                                                    for pos in range(embeds.num_embeddings)])\n","    embeds.weight[:, 0::2] = torch.sin(position_enc[:, 0::2])\n","    embeds.weight[:, 1::2] = torch.cos(position_enc[:, 1::2])\n","    embeds.weight.detach_()\n","    embeds.weight.requires_grad = False\n","\n","\n","class Transformer(nn.Module):\n","    def __init__(self, embed_dim, hidden_dim, num_embeddings, num_max_positions, num_heads, num_layers, dropout,\n","                 sinusoidal_embeddings, causal=False):\n","        \"\"\" Transformer (GPT-2 architecture) \"\"\"\n","        super().__init__()\n","        self.causal = causal\n","        self.tokens_embeddings = nn.Embedding(num_embeddings, embed_dim)\n","        self.position_embeddings = nn.Embedding(num_max_positions, embed_dim)\n","        if sinusoidal_embeddings:\n","            create_sinusoidal_embeddings(self.position_embeddings)\n","        self.dropout = nn.Dropout(dropout)\n","\n","        self.attentions, self.feed_forwards = nn.ModuleList(), nn.ModuleList()\n","        self.layer_norms_1, self.layer_norms_2 = nn.ModuleList(), nn.ModuleList()\n","        for _ in range(num_layers):\n","            self.attentions.append(nn.MultiheadAttention(embed_dim, num_heads, dropout = dropout))\n","            self.feed_forwards.append(nn.Sequential(nn.Linear(embed_dim, hidden_dim),\n","                                                    nn.ReLU(),\n","                                                    nn.Linear(hidden_dim, embed_dim)))\n","            self.layer_norms_1.append(nn.LayerNorm(embed_dim, eps=1e-12))\n","            self.layer_norms_2.append(nn.LayerNorm(embed_dim, eps=1e-12))\n","\n","    def forward(self, x, padding_mask = None):\n","        \"\"\" Input has shape [seq length, batch] \"\"\"\n","        positions = torch.arange(len(x), device=x.device).unsqueeze(-1)\n","        h = self.tokens_embeddings(x)\n","        h = h + self.position_embeddings(positions).expand_as(h)\n","        h = self.dropout(h)\n","\n","        attn_mask = None\n","        if self.causal:\n","            attn_mask = torch.full((len(x), len(x)), -float('Inf'), device=h.device, dtype=h.dtype)\n","            attn_mask = torch.triu(attn_mask, diagonal = 1)\n","        for layer_norm_1, attention, layer_norm_2, feed_forward in zip(self.layer_norms_1, self.attentions,\n","                                                                       self.layer_norms_2, self.feed_forwards):\n","            h = layer_norm_1(h)\n","            x, _ = attention(h, h, h, attn_mask = attn_mask, need_weights = False, key_padding_mask = padding_mask)\n","            x = self.dropout(x)\n","            h = x + h\n","\n","            h = layer_norm_2(h)\n","            x = feed_forward(h)\n","            x = self.dropout(x)\n","            h = x + h\n","        return h\n","\n","\n","class TransformerWithLMHead(nn.Module):\n","    def __init__(self, embed_dim, \n","                 hidden_dim, \n","                 num_embeddings,\n","                 num_max_positions, \n","                 num_heads, \n","                 num_layers,\n","                 dropout, \n","                 sinusoidal_embeddings, \n","                 mlm,\n","                 initializer_range):\n","      \n","        \"\"\" Transformer with a language modeling head on top (tied weights) \"\"\"\n","\n","        super().__init__()\n","        self.transformer = Transformer(embed_dim, hidden_dim, num_embeddings,\n","                                       num_max_positions, num_heads, num_layers,\n","                                       dropout, sinusoidal_embeddings, causal=not mlm)\n","        self.lm_head = nn.Linear(embed_dim, num_embeddings, bias=False)\n","        self.apply(self.init_weights)\n","        self.tie_weights()\n","\n","    def tie_weights(self):\n","        self.lm_head.weight = self.transformer.tokens_embeddings.weight\n","\n","    def init_weights(self, module):\n","        \"\"\" initialize weights - note that nn.MultiheadAttention is already initalized by PyTorch (xavier_uniform) \"\"\"\n","        if isinstance(module, (nn.Linear, nn.Embedding, nn.LayerNorm)):\n","            module.weight.data.normal_(mean=0.0, std=initializer_range)\n","        if isinstance(module, (nn.Linear, nn.LayerNorm)) and module.bias is not None:\n","            module.bias.data.zero_()\n","\n","    def forward(self, x, labels=None, padding_mask=None):\n","        \"\"\" Input has shape [seq length, batch] \"\"\"\n","        hidden_states = self.transformer(x, padding_mask)\n","        logits = self.lm_head(hidden_states)\n","\n","        if labels is not None:\n","            shift_logits = logits[:-1] if self.transformer.causal else logits\n","            shift_labels = labels[1:] if self.transformer.causal else labels\n","            loss_fct = nn.CrossEntropyLoss(ignore_index=-1)\n","            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n","            return logits, loss\n","\n","        return logits"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"Zijj0jm-jREG","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1592504113076,"user_tz":-120,"elapsed":22153,"user":{"displayName":"Daniel Cuiñas Vázquez","photoUrl":"","userId":"11552885780691803973"}}},"source":["embed_dim = 128\n","hidden_dim = 128\n","num_embeddings = len(TEXT.vocab.itos) + 1\n","num_max_positions = bptt\n","num_heads = 8\n","num_layers = 6\n","dropout = 0.1\n","sinusoidal_embeddings = True\n","mlm = True\n","causal = not mlm\n","initializer_range = 0.02\n","lr = 2.5e-4\n","# lr = 1\n","weight_decay = 0.0\n","gradient_accumulation_steps = 1\n","max_norm = 0.25\n","log_interval = 20\n","\n","model = TransformerWithLMHead(embed_dim, \n","                 hidden_dim, \n","                 num_embeddings,\n","                 num_max_positions, \n","                 num_heads, \n","                 num_layers,\n","                 dropout, \n","                 sinusoidal_embeddings, \n","                 mlm,\n","                 initializer_range)"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"uqL54ucnlYQm","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1592504113078,"user_tz":-120,"elapsed":22144,"user":{"displayName":"Daniel Cuiñas Vázquez","photoUrl":"","userId":"11552885780691803973"}}},"source":[""],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"lEE8Dr5CnUDa","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1592504113079,"user_tz":-120,"elapsed":22131,"user":{"displayName":"Daniel Cuiñas Vázquez","photoUrl":"","userId":"11552885780691803973"}},"outputId":"5aed09b2-d8e8-4bd2-c20a-5e000c93efcc"},"source":["def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","print(f'The model has {count_parameters(model):,} trainable parameters')"],"execution_count":12,"outputs":[{"output_type":"stream","text":["The model has 4,293,120 trainable parameters\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"B0yOkv3akMtf","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1592504115339,"user_tz":-120,"elapsed":24379,"user":{"displayName":"Daniel Cuiñas Vázquez","photoUrl":"","userId":"11552885780691803973"}}},"source":["import torch.optim as optim\n","\n","optimizer = optim.Adam(model.parameters(), lr = lr, weight_decay = weight_decay)\n","\n","model=model.to(device)\n"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"FAgEaMmjn9CR","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1592504115341,"user_tz":-120,"elapsed":24349,"user":{"displayName":"Daniel Cuiñas Vázquez","photoUrl":"","userId":"11552885780691803973"}}},"source":["def train(model, iterator):\n","    clip = 0.25\n","    total_loss = 0\n","    \n","    model.train()\n","            \n","    for k, batch in enumerate(iterator):\n","        inputs = batch.text.contiguous()\n","        inputs, labels = mask_tokens(inputs) if mlm else (inputs, inputs)\n","\n","        inputs = inputs.to(device)\n","\n","        optimizer.zero_grad()\n","\n","        logits, loss = model(inputs, labels=labels)\n","\n","        loss = loss / gradient_accumulation_steps\n","                      \n","        loss.backward()\n","        \n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n","\n","        optimizer.step()\n","        \n","        total_loss += loss.item()\n","        \n","        if k % log_interval == 0 and k > 0:\n","            cur_loss = total_loss / log_interval\n","            print('| epoch {:3d} | {:5d}/{:5d} batches | lr {} | '\n","                    'loss {:5.2f} | ppl {:8.2f}'.format(epoch, k, len(iterator), lr, cur_loss, math.exp(cur_loss)))\n","            total_loss = 0\n","        # return loss.item()\n","\n","loss_fct = nn.CrossEntropyLoss(ignore_index=-1)\n","\n","\n","def evaluate(model, iterator):\n","    total_loss = 0\n","    \n","    model.eval()\n","        \n","    with torch.no_grad():\n","    \n","        for batch in iterator:\n","            inputs = batch.text.contiguous()\n","            inputs, labels = mask_tokens(inputs) if mlm else (inputs, inputs)\n","\n","            inputs = inputs.to(device)\n","\n","            logits = model(inputs)\n","            \n","            shift_logits = logits[:-1] if not mlm else logits\n","\n","            shift_labels = labels[1:] if not mlm else labels\n","\n","            # total_loss += len(data) * loss\n","            \n","            return loss_fct(shift_logits.view(-1, logits.size(-1)), shift_labels.view(-1))\n"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"id":"noRhwDZv8mDA","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1592504115342,"user_tz":-120,"elapsed":24336,"user":{"displayName":"Daniel Cuiñas Vázquez","photoUrl":"","userId":"11552885780691803973"}}},"source":["import time\n","\n","def epoch_time(start_time, end_time):\n","    elapsed_time = end_time - start_time\n","    elapsed_mins = int(elapsed_time / 60)\n","    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n","    return elapsed_mins, elapsed_secs"],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"id":"Hds7Wapk3OTQ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":850},"executionInfo":{"status":"error","timestamp":1592504303897,"user_tz":-120,"elapsed":212880,"user":{"displayName":"Daniel Cuiñas Vázquez","photoUrl":"","userId":"11552885780691803973"}},"outputId":"637d7bfd-401b-4d5b-d2ab-74396469d753"},"source":["import math\n","\n","N_EPOCHS = 100\n","\n","best_valid_loss = float('inf')\n","counter = 0\n","patience = 5\n","\n","for epoch in range(N_EPOCHS):\n","\n","    start_time = time.time()\n","    \n","    train(model, train_iter)\n","    valid_loss = evaluate(model, valid_iter)\n","    \n","    end_time = time.time()\n","\n","    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n","\n","    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n","    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):.2f}')\n","\n","    # if valid_loss < best_valid_loss:\n","    #     best_valid_loss = valid_loss\n","    #     #torch.save(model.state_dict(), 'tut2-model.pt')\n","    #     counter = 0 \n","    # else:\n","    #     lr /= 4.0\n","    #     counter += 1\n","    #     if counter >= patience:\n","    #         break\n"],"execution_count":17,"outputs":[{"output_type":"stream","text":["| epoch   0 |    20/  224 batches | lr 0.00025 | loss 10.77 | ppl 47383.38\n","| epoch   0 |    40/  224 batches | lr 0.00025 | loss 10.16 | ppl 25851.93\n","| epoch   0 |    60/  224 batches | lr 0.00025 | loss  9.82 | ppl 18466.12\n","| epoch   0 |    80/  224 batches | lr 0.00025 | loss  9.01 | ppl  8208.56\n","| epoch   0 |   100/  224 batches | lr 0.00025 | loss  7.76 | ppl  2338.58\n","| epoch   0 |   120/  224 batches | lr 0.00025 | loss  6.96 | ppl  1058.21\n","| epoch   0 |   140/  224 batches | lr 0.00025 | loss  6.84 | ppl   938.31\n","| epoch   0 |   160/  224 batches | lr 0.00025 | loss  6.87 | ppl   962.22\n","| epoch   0 |   180/  224 batches | lr 0.00025 | loss  6.81 | ppl   905.05\n","| epoch   0 |   200/  224 batches | lr 0.00025 | loss  6.80 | ppl   896.70\n","| epoch   0 |   220/  224 batches | lr 0.00025 | loss  6.83 | ppl   925.62\n","Epoch: 01 | Epoch Time: 1m 23s\n","\t Val. Loss: 6.234 |  Val. PPL: 509.86\n","| epoch   1 |    20/  224 batches | lr 0.00025 | loss  7.13 | ppl  1248.80\n","| epoch   1 |    40/  224 batches | lr 0.00025 | loss  6.76 | ppl   865.12\n","| epoch   1 |    60/  224 batches | lr 0.00025 | loss  6.79 | ppl   888.14\n","| epoch   1 |    80/  224 batches | lr 0.00025 | loss  6.79 | ppl   888.63\n","| epoch   1 |   100/  224 batches | lr 0.00025 | loss  6.75 | ppl   850.82\n","| epoch   1 |   120/  224 batches | lr 0.00025 | loss  6.77 | ppl   870.16\n","| epoch   1 |   140/  224 batches | lr 0.00025 | loss  6.72 | ppl   829.30\n","| epoch   1 |   160/  224 batches | lr 0.00025 | loss  6.78 | ppl   877.40\n","| epoch   1 |   180/  224 batches | lr 0.00025 | loss  6.76 | ppl   861.85\n","| epoch   1 |   200/  224 batches | lr 0.00025 | loss  6.76 | ppl   860.05\n","| epoch   1 |   220/  224 batches | lr 0.00025 | loss  6.78 | ppl   877.50\n","Epoch: 02 | Epoch Time: 1m 23s\n","\t Val. Loss: 6.251 |  Val. PPL: 518.39\n","| epoch   2 |    20/  224 batches | lr 0.00025 | loss  7.12 | ppl  1234.44\n","| epoch   2 |    40/  224 batches | lr 0.00025 | loss  6.77 | ppl   867.07\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-17-8229c9411e9e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mvalid_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-15-fa1d8efa45d7>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, iterator)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mgradient_accumulation_steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    196\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \"\"\"\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     99\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"3xHiH3hBja1O","colab_type":"code","colab":{},"executionInfo":{"status":"aborted","timestamp":1592504303895,"user_tz":-120,"elapsed":212865,"user":{"displayName":"Daniel Cuiñas Vázquez","photoUrl":"","userId":"11552885780691803973"}}},"source":[""],"execution_count":null,"outputs":[]}]}