{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Transformer_torchtext_scratch.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPwlYXeRvUnlqjtL+8TEe5z"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"aV4ez212dFrL","colab_type":"code","colab":{}},"source":["import torch\n","from torchtext import data\n","import spacy\n","from spacy.symbols import ORTH\n","from torchtext.datasets import WikiText2\n","\n","my_tok = spacy.load('en')\n"," \n","def spacy_tok(x):\n","    return [tok.text for tok in my_tok.tokenizer(x)]\n"," \n","TEXT = data.Field(lower=True, tokenize=spacy_tok)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"16xGo4uRdOBa","colab_type":"code","colab":{}},"source":["my_tok.tokenizer.add_special_case(\"don't\", [{ORTH: \"do\"}, {ORTH: \"n't\"}])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"adE-xUIIdPjs","colab_type":"code","colab":{}},"source":["train, valid, test = WikiText2.splits(TEXT)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WnslyWK5dQ8i","colab_type":"code","colab":{}},"source":["TEXT.build_vocab(train)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LV13ObSddSoK","colab_type":"code","colab":{}},"source":["batch_size = 50\n","bptt = 200"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5i5oUng3dUIx","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1593173020237,"user_tz":-120,"elapsed":23147,"user":{"displayName":"Daniel Cuiñas Vázquez","photoUrl":"","userId":"11552885780691803973"}},"outputId":"d6a77f0a-d01a-44c5-e954-987c5057dcad"},"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(device)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["cuda\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"sPHoqwA_dVuc","colab_type":"code","colab":{}},"source":["train_iter, valid_iter, test_iter = data.BPTTIterator.splits(\n","    (train, valid, test),\n","    batch_size=batch_size,\n","    bptt_len=bptt, \n","    device=device,\n","    repeat=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MyK0kfyDdXj1","colab_type":"code","colab":{}},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","\n","class PositionwiseFeedforwardLayer(nn.Module):\n","    def __init__(self, hid_dim, pf_dim, dropout):\n","        super().__init__()\n","        \n","        self.fc_1 = nn.Linear(hid_dim, pf_dim)\n","        self.fc_2 = nn.Linear(pf_dim, hid_dim)\n","        \n","        self.dropout = nn.Dropout(dropout)\n","        \n","    def forward(self, x):\n","        \n","        #x = [batch size, seq len, hid dim]\n","        \n","        x = self.dropout(torch.relu(self.fc_1(x)))\n","        \n","        #x = [batch size, seq len, pf dim]\n","        \n","        x = self.fc_2(x)\n","        \n","        #x = [batch size, seq len, hid dim]\n","        \n","        return x\n","\n","class MultiHeadAttentionLayer(nn.Module):\n","    def __init__(self, hid_dim, n_heads, dropout, device):\n","        super().__init__()\n","        \n","        assert hid_dim % n_heads == 0\n","        \n","        self.hid_dim = hid_dim\n","        self.n_heads = n_heads\n","        self.head_dim = hid_dim // n_heads\n","        \n","        self.fc_q = nn.Linear(hid_dim, hid_dim)\n","        self.fc_k = nn.Linear(hid_dim, hid_dim)\n","        self.fc_v = nn.Linear(hid_dim, hid_dim)\n","        \n","        self.fc_o = nn.Linear(hid_dim, hid_dim)\n","        \n","        self.dropout = nn.Dropout(dropout)\n","        \n","        self.scale = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)\n","        \n","    def forward(self, query, key, value, mask = None):\n","        \n","        batch_size = query.shape[0]\n","        \n","        #query = [batch size, query len, hid dim]\n","        #key = [batch size, key len, hid dim]\n","        #value = [batch size, value len, hid dim]\n","                \n","        Q = self.fc_q(query)\n","        K = self.fc_k(key)\n","        V = self.fc_v(value)\n","        \n","        #Q = [batch size, query len, hid dim]\n","        #K = [batch size, key len, hid dim]\n","        #V = [batch size, value len, hid dim]\n","                \n","        Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n","        K = K.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n","        V = V.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n","        \n","        #Q = [batch size, n heads, query len, head dim]\n","        #K = [batch size, n heads, key len, head dim]\n","        #V = [batch size, n heads, value len, head dim]\n","                \n","        energy = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.scale\n","        \n","        #energy = [batch size, n heads, seq len, seq len]\n","        \n","        if mask is not None:\n","            energy = energy.masked_fill(mask == 0, -1e10)\n","        \n","        attention = torch.softmax(energy, dim = -1)\n","                \n","        #attention = [batch size, n heads, query len, key len]\n","        \n","        # x = torch.matmul(self.dropout(attention), V) Vamos a probar a quitar el dropout del attention\n","\n","        x = torch.matmul(attention, V)\n","        \n","        #x = [batch size, n heads, seq len, head dim]\n","        \n","        x = x.permute(0, 2, 1, 3).contiguous()\n","        \n","        #x = [batch size, seq len, n heads, head dim]\n","        \n","        x = x.view(batch_size, -1, self.hid_dim)\n","        \n","        #x = [batch size, seq len, hid dim]\n","        \n","        x = self.fc_o(x)\n","        \n","        #x = [batch size, seq len, hid dim]\n","        \n","        return x, attention\n","\n","\n","\n","class Transformer(nn.Module):\n","    def __init__(self, \n","                 hid_dim, \n","                 n_heads, \n","                 pf_dim,  \n","                 dropout, \n","                 device):\n","        super().__init__()\n","        \n","        self.self_att_layer_norm = nn.LayerNorm(hid_dim)\n","        self.ff_layer_norm = nn.LayerNorm(hid_dim)\n","        self.self_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n","        self.positionwise_feedforward = PositionwiseFeedforwardLayer(hid_dim, \n","                                                                     pf_dim, \n","                                                                     dropout)\n","        self.dropout = nn.Dropout(dropout)\n","        \n","    def forward(self, src, src_mask):\n","        \n","        #src = [batch size, src len, hid dim]\n","        #src_mask = [batch size, src len]\n","                \n","        #self attention\n","        _src, _ = self.self_attention(src, src, src, src_mask)\n","        \n","        #dropout, residual connection and layer norm\n","        src = self.self_att_layer_norm(src + self.dropout(_src))\n","        \n","        #src = [batch size, src len, hid dim]\n","        \n","        #positionwise feedforward\n","        _src = self.positionwise_feedforward(src)\n","        \n","        #dropout, residual and layer norm\n","        src = self.ff_layer_norm(src + self.dropout(_src))\n","        \n","        #src = [batch size, src len, hid dim]\n","        \n","        return src\n","\n","\n","class TransformerWithLMHead(nn.Module):\n","    def __init__(self, \n","                 input_dim, \n","                 emb_dim, \n","                 n_layers, \n","                 n_heads, \n","                 hid_dim,\n","                 dropout, \n","                 device,\n","                 max_length = bptt):\n","        super().__init__()\n","\n","        self.device = device\n","        \n","        self.tok_embedding = nn.Embedding(input_dim, emb_dim)\n","        self.pos_embedding = nn.Embedding(max_length, emb_dim)\n","\n","        \n","        self.layers = nn.ModuleList([Transformer(emb_dim, \n","                                                  n_heads, \n","                                                  hid_dim,\n","                                                  dropout, \n","                                                  device) \n","                                     for _ in range(n_layers)])\n","        \n","        self.dropout = nn.Dropout(dropout)\n","        \n","        self.scale = torch.sqrt(torch.FloatTensor([emb_dim])).to(device)\n","        self.fc = nn.Linear(emb_dim, input_dim)\n","\n","\n","    def make_src_mask(self, src):\n","        \n","        #src = [batch size, src len]\n","        \n","        src_mask = (src != 0).unsqueeze(1).unsqueeze(2)\n","\n","        #src_mask = [batch size, 1, 1, src len]\n","\n","        return src_mask\n","        \n","    def forward(self, src):\n","        \n","        #src = [batch size, src len]\n","\n","        src_mask = self.make_src_mask(src)\n","\n","        #src_mask = [batch size, 1, 1, src len]\n","        \n","        batch_size = src.shape[0]\n","        src_len = src.shape[1]\n","        \n","        pos = torch.arange(0, src_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n","        \n","        #pos = [batch size, src len]\n","        \n","        src = self.dropout((self.tok_embedding(src) * self.scale) + self.pos_embedding(pos))\n","                \n","        #src = [batch size, src len, hid dim]\n","        \n","        for layer in self.layers:\n","            src = layer(src, src_mask)\n","            \n","        #src = [batch size, src len, hid dim]\n","\n","        src = src.transpose(1,0)\n","\n","        #src = [src len, batch size, hid dim]\n","\n","        out = self.fc(src)\n","\n","        # out = [src len, batch size, vocab_size]\n","            \n","        return F.log_softmax(out, dim=-1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vLoNGEHReBIm","colab_type":"code","colab":{}},"source":["vocab_size = len(TEXT.vocab)\n","emb_dim = 128\n","hid_dim = 128\n","n_layers = 4\n","n_heads = 4\n","dropout = 0.1\n","\n","lr = 4\n","log_interval = 20\n","\n","model = TransformerWithLMHead(vocab_size, emb_dim, n_layers, n_heads, hid_dim, dropout, device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6sCJ6DnIeVdK","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1593173022870,"user_tz":-120,"elapsed":25736,"user":{"displayName":"Daniel Cuiñas Vázquez","photoUrl":"","userId":"11552885780691803973"}},"outputId":"cd94b389-15a7-4c36-f2e6-5c90396afd9b"},"source":["def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","print(f'The model has {count_parameters(model):,} trainable parameters')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["The model has 7,843,526 trainable parameters\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"KsjMRJbBerYD","colab_type":"code","colab":{}},"source":["import torch.optim as optim\n","\n","criterion = nn.NLLLoss()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gINHrmR5ey-F","colab_type":"code","colab":{}},"source":["model=model.to(device)\n","criterion=criterion.to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iAMjyPbye0mI","colab_type":"code","colab":{}},"source":["def train(model, iterator, criterion):\n","    clip = 0.25\n","    total_loss = 0\n","    \n","    model.train()\n","            \n","    for k, batch in enumerate(iterator):\n","        data = batch.text\n","        data = data.transpose(1,0)\n","        targets = batch.target.view(-1)\n","\n","        data = data.to(device)\n","        targets = targets.to(device)\n","\n","        model.zero_grad()\n","      \n","        output = model(data) \n","\n","        output = output.view(-1, vocab_size)\n","        \n","        loss = criterion(output, targets)\n","                \n","        loss.backward()\n","        \n","        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n","        for p in model.parameters():\n","            p.data.add_(-lr, p.grad.data)\n","        \n","        total_loss += loss.item()\n","        \n","        if k % log_interval == 0 and k > 0:\n","            cur_loss = total_loss / log_interval\n","            print('| epoch {:3d} | {:5d}/{:5d} batches | lr {} | '\n","                    'loss {:5.2f} | ppl {:8.2f}'.format(epoch, k, len(iterator), lr, cur_loss, math.exp(cur_loss)))\n","            total_loss = 0\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ockApLfye-wl","colab_type":"code","colab":{}},"source":["def evaluate(model, iterator, criterion):\n","    \n","    total_loss = 0\n","    \n","    model.eval()\n","        \n","    with torch.no_grad():\n","    \n","        for batch in iterator:\n","            data = batch.text\n","            data = data.transpose(1,0)\n","            targets = batch.target.view(-1)\n","\n","            len_data = data.shape[1]\n","\n","            data = data.to(device)\n","            targets = targets.to(device)\n","\n","            output = model(data)\n","\n","            output = output.view(-1, vocab_size)\n","            \n","            loss = criterion(output, targets).item()\n","\n","            total_loss += len_data * loss\n","\n","        \n","    return total_loss / (len(iterator)*bptt - 1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ej6eo3HNfB4i","colab_type":"code","colab":{}},"source":["import time\n","import math\n","\n","def epoch_time(start_time, end_time):\n","    elapsed_time = end_time - start_time\n","    elapsed_mins = int(elapsed_time / 60)\n","    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n","    return elapsed_mins, elapsed_secs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"K0UrCm32fDya","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"error","timestamp":1593173491056,"user_tz":-120,"elapsed":405520,"user":{"displayName":"Daniel Cuiñas Vázquez","photoUrl":"","userId":"11552885780691803973"}},"outputId":"823f786c-32ce-4f9e-f22d-3199452cd287"},"source":["N_EPOCHS = 100\n","\n","best_valid_loss = float('inf')\n","counter = 0\n","patience = 5\n","\n","for epoch in range(N_EPOCHS):\n","\n","    start_time = time.time()\n","    \n","    train(model, train_iter, criterion)\n","    valid_loss = evaluate(model, valid_iter, criterion)\n","    \n","    end_time = time.time()\n","\n","    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n","\n","    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n","    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):.2f}')\n","\n","    if valid_loss < best_valid_loss:\n","        best_valid_loss = valid_loss\n","        #torch.save(model.state_dict(), 'tut2-model.pt')\n","        counter = 0 \n","    else:\n","        lr /= 4.0\n","        counter += 1\n","        if counter >= patience:\n","            break\n","\n","    "],"execution_count":null,"outputs":[{"output_type":"stream","text":["/pytorch/torch/csrc/utils/python_arg_parser.cpp:756: UserWarning: This overload of add_ is deprecated:\n","\tadd_(Number alpha, Tensor other)\n","Consider using one of the following signatures instead:\n","\tadd_(Tensor other, *, Number alpha)\n"],"name":"stderr"},{"output_type":"stream","text":["| epoch   0 |    20/  224 batches | lr 4 | loss  8.47 | ppl  4792.34\n","| epoch   0 |    40/  224 batches | lr 4 | loss  7.28 | ppl  1445.63\n","| epoch   0 |    60/  224 batches | lr 4 | loss  7.09 | ppl  1201.14\n","| epoch   0 |    80/  224 batches | lr 4 | loss  6.91 | ppl   998.39\n","| epoch   0 |   100/  224 batches | lr 4 | loss  6.80 | ppl   898.79\n","| epoch   0 |   120/  224 batches | lr 4 | loss  6.74 | ppl   845.47\n","| epoch   0 |   140/  224 batches | lr 4 | loss  6.64 | ppl   762.49\n","| epoch   0 |   160/  224 batches | lr 4 | loss  6.60 | ppl   733.70\n","| epoch   0 |   180/  224 batches | lr 4 | loss  6.54 | ppl   693.71\n","| epoch   0 |   200/  224 batches | lr 4 | loss  6.46 | ppl   640.95\n","| epoch   0 |   220/  224 batches | lr 4 | loss  6.47 | ppl   644.66\n","Epoch: 01 | Epoch Time: 1m 16s\n","\t Val. Loss: 5.669 |  Val. PPL: 289.86\n","| epoch   1 |    20/  224 batches | lr 4 | loss  6.73 | ppl   838.76\n","| epoch   1 |    40/  224 batches | lr 4 | loss  6.35 | ppl   571.33\n","| epoch   1 |    60/  224 batches | lr 4 | loss  6.37 | ppl   585.36\n","| epoch   1 |    80/  224 batches | lr 4 | loss  6.31 | ppl   552.32\n","| epoch   1 |   100/  224 batches | lr 4 | loss  6.31 | ppl   548.82\n","| epoch   1 |   120/  224 batches | lr 4 | loss  6.30 | ppl   546.42\n","| epoch   1 |   140/  224 batches | lr 4 | loss  6.24 | ppl   513.50\n","| epoch   1 |   160/  224 batches | lr 4 | loss  6.25 | ppl   520.18\n","| epoch   1 |   180/  224 batches | lr 4 | loss  6.22 | ppl   503.41\n","| epoch   1 |   200/  224 batches | lr 4 | loss  6.18 | ppl   480.88\n","| epoch   1 |   220/  224 batches | lr 4 | loss  6.21 | ppl   497.03\n","Epoch: 02 | Epoch Time: 1m 16s\n","\t Val. Loss: 5.502 |  Val. PPL: 245.26\n","| epoch   2 |    20/  224 batches | lr 4 | loss  6.46 | ppl   639.58\n","| epoch   2 |    40/  224 batches | lr 4 | loss  6.09 | ppl   441.62\n","| epoch   2 |    60/  224 batches | lr 4 | loss  6.14 | ppl   464.91\n","| epoch   2 |    80/  224 batches | lr 4 | loss  6.11 | ppl   449.36\n","| epoch   2 |   100/  224 batches | lr 4 | loss  6.09 | ppl   439.23\n","| epoch   2 |   120/  224 batches | lr 4 | loss  6.11 | ppl   451.73\n","| epoch   2 |   140/  224 batches | lr 4 | loss  6.06 | ppl   429.18\n","| epoch   2 |   160/  224 batches | lr 4 | loss  6.06 | ppl   429.31\n","| epoch   2 |   180/  224 batches | lr 4 | loss  6.04 | ppl   420.15\n","| epoch   2 |   200/  224 batches | lr 4 | loss  6.01 | ppl   405.64\n","| epoch   2 |   220/  224 batches | lr 4 | loss  6.04 | ppl   419.15\n","Epoch: 03 | Epoch Time: 1m 16s\n","\t Val. Loss: 5.403 |  Val. PPL: 221.96\n","| epoch   3 |    20/  224 batches | lr 4 | loss  6.30 | ppl   542.54\n","| epoch   3 |    40/  224 batches | lr 4 | loss  5.94 | ppl   378.20\n","| epoch   3 |    60/  224 batches | lr 4 | loss  5.98 | ppl   396.16\n","| epoch   3 |    80/  224 batches | lr 4 | loss  5.96 | ppl   388.14\n","| epoch   3 |   100/  224 batches | lr 4 | loss  5.93 | ppl   375.04\n","| epoch   3 |   120/  224 batches | lr 4 | loss  5.96 | ppl   388.67\n","| epoch   3 |   140/  224 batches | lr 4 | loss  5.91 | ppl   369.08\n","| epoch   3 |   160/  224 batches | lr 4 | loss  5.93 | ppl   375.20\n","| epoch   3 |   180/  224 batches | lr 4 | loss  5.91 | ppl   368.44\n","| epoch   3 |   200/  224 batches | lr 4 | loss  5.87 | ppl   354.12\n","| epoch   3 |   220/  224 batches | lr 4 | loss  5.91 | ppl   368.47\n","Epoch: 04 | Epoch Time: 1m 16s\n","\t Val. Loss: 5.256 |  Val. PPL: 191.74\n","| epoch   4 |    20/  224 batches | lr 4 | loss  6.16 | ppl   471.29\n","| epoch   4 |    40/  224 batches | lr 4 | loss  5.82 | ppl   335.71\n","| epoch   4 |    60/  224 batches | lr 4 | loss  5.86 | ppl   350.32\n","| epoch   4 |    80/  224 batches | lr 4 | loss  5.84 | ppl   342.16\n","| epoch   4 |   100/  224 batches | lr 4 | loss  5.82 | ppl   336.58\n","| epoch   4 |   120/  224 batches | lr 4 | loss  5.84 | ppl   343.74\n","| epoch   4 |   140/  224 batches | lr 4 | loss  5.79 | ppl   328.46\n","| epoch   4 |   160/  224 batches | lr 4 | loss  5.82 | ppl   336.61\n","| epoch   4 |   180/  224 batches | lr 4 | loss  5.80 | ppl   329.83\n","| epoch   4 |   200/  224 batches | lr 4 | loss  5.76 | ppl   317.75\n","| epoch   4 |   220/  224 batches | lr 4 | loss  5.80 | ppl   329.59\n","Epoch: 05 | Epoch Time: 1m 16s\n","\t Val. Loss: 5.186 |  Val. PPL: 178.74\n","| epoch   5 |    20/  224 batches | lr 4 | loss  6.06 | ppl   426.41\n","| epoch   5 |    40/  224 batches | lr 4 | loss  5.71 | ppl   303.21\n","| epoch   5 |    60/  224 batches | lr 4 | loss  5.76 | ppl   317.35\n","| epoch   5 |    80/  224 batches | lr 4 | loss  5.74 | ppl   310.38\n","| epoch   5 |   100/  224 batches | lr 4 | loss  5.71 | ppl   303.13\n","| epoch   5 |   120/  224 batches | lr 4 | loss  5.74 | ppl   310.64\n","| epoch   5 |   140/  224 batches | lr 4 | loss  5.70 | ppl   299.69\n","| epoch   5 |   160/  224 batches | lr 4 | loss  5.73 | ppl   308.15\n","| epoch   5 |   180/  224 batches | lr 4 | loss  5.71 | ppl   302.10\n","| epoch   5 |   200/  224 batches | lr 4 | loss  5.68 | ppl   291.79\n","| epoch   5 |   220/  224 batches | lr 4 | loss  5.70 | ppl   298.56\n","Epoch: 06 | Epoch Time: 1m 16s\n","\t Val. Loss: 5.190 |  Val. PPL: 179.51\n","| epoch   6 |    20/  224 batches | lr 1.0 | loss  5.86 | ppl   349.48\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-16-a85b8ac5fe51>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mvalid_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-13-158e7f5fa71d>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, iterator, criterion)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    196\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \"\"\"\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     99\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"KxqywU7SfFqw","colab_type":"code","colab":{}},"source":["# Run on test data.\n","test_loss = evaluate(model, test_iter, criterion)\n","print('=' * 89)\n","print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\n","    test_loss, math.exp(test_loss)))\n","print('=' * 89)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pB4avh9Rjk-g","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2MDJATwDqIGd","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gWTmgPZdqIJS","colab_type":"code","colab":{}},"source":["batch = next(iter(train_iter))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NZvNDs5MqIM6","colab_type":"code","colab":{}},"source":["data = batch.text\n","targets = batch.target\n","targets = targets.view(-1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"o9x6drhtqSX6","colab_type":"code","colab":{}},"source":["data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wsoDcrCzqSat","colab_type":"code","colab":{}},"source":["targets"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2jydKvKxqSdQ","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZMrUTsOHqIPJ","colab_type":"code","colab":{}},"source":["data1 = batch.text\n","targets1 = batch.target\n","data1 = data1.transpose(1,0)\n","targets1 = targets1.transpose(1,0)\n","# targets1 = targets1.view(-1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5DhS_6QYqt1K","colab_type":"code","colab":{}},"source":["data1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ocIdwA_aquzL","colab_type":"code","colab":{}},"source":["targets1.flatten()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LYUUQk93qz1s","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}