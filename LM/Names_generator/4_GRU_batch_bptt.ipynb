{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KqPl6AfOaMRq"
   },
   "outputs": [],
   "source": [
    "from io import open\n",
    "import glob\n",
    "import os\n",
    "import unicodedata\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "all_letters = string.ascii_letters + \" .,;'-\"\n",
    "n_letters = len(all_letters) + 1 # Plus EOS marker\n",
    "\n",
    "def findFiles(path):\n",
    "   return glob.glob(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-4tBT78qaTjq"
   },
   "outputs": [],
   "source": [
    "# Turn a Unicode string to plain ASCII, thanks to https://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "        and c in all_letters\n",
    "    )\n",
    "\n",
    "# Read a file and split into lines\n",
    "def readLines(filename):\n",
    "    lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n",
    "    return [unicodeToAscii(line) for line in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yC5CavK5aU_i"
   },
   "outputs": [],
   "source": [
    "# Build the category_lines dictionary, a list of lines per category\n",
    "category_lines = []\n",
    "all_categories = []\n",
    "for filename in findFiles('data/names/*.txt'):\n",
    "    category = os.path.splitext(os.path.basename(filename))[0]\n",
    "    all_categories.append(category)\n",
    "    lines = readLines(filename)\n",
    "    category_line = [category] * len(lines)\n",
    "    category_lines.extend(list(zip(category_line, lines)))\n",
    "\n",
    "n_categories = len(all_categories)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W23SSjBxaWSG"
   },
   "outputs": [],
   "source": [
    "all_categories = {v:k for k,v in enumerate(all_categories)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n_p6jJdUaX0f"
   },
   "outputs": [],
   "source": [
    "# One-hot vector for category\n",
    "def categoryArray(cat):\n",
    "    arr = np.zeros(n_categories)\n",
    "    arr[cat] = 1\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 7168,
     "status": "ok",
     "timestamp": 1591637232885,
     "user": {
      "displayName": "Daniel Cuiñas Vázquez",
      "photoUrl": "",
      "userId": "11552885780691803973"
     },
     "user_tz": -120
    },
    "id": "sBrP2nIJaZOQ",
    "outputId": "b0f5fc1a-ff5d-4a59-9f2d-c2bbd5c4f91d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# categories: 18 {'Czech': 0, 'English': 1, 'Scottish': 2, 'Italian': 3, 'Polish': 4, 'Greek': 5, 'Dutch': 6, 'German': 7, 'Japanese': 8, 'Korean': 9, 'Arabic': 10, 'Portuguese': 11, 'Russian': 12, 'Vietnamese': 13, 'Irish': 14, 'Spanish': 15, 'French': 16, 'Chinese': 17}\n",
      "O'Neal\n"
     ]
    }
   ],
   "source": [
    "print('# categories:', n_categories, all_categories)\n",
    "print(unicodeToAscii(\"O'Néàl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SGg14EX1aavz"
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(category_lines, columns=['category_','name'])\n",
    "df['len'] = df['name'].apply(lambda s: len(s))\n",
    "df['category'] = df['category_'].apply(lambda c: all_categories[c])\n",
    "df['category'] = df['category'].apply(lambda c: categoryArray(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pkFjsgvsab8l"
   },
   "outputs": [],
   "source": [
    "df = df.sample(n=len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 397,
     "status": "ok",
     "timestamp": 1591637235580,
     "user": {
      "displayName": "Daniel Cuiñas Vázquez",
      "photoUrl": "",
      "userId": "11552885780691803973"
     },
     "user_tz": -120
    },
    "id": "faeV_AqDadYD",
    "outputId": "4a3b5935-0f73-43a0-e21c-18fd20ab72bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "max_len = df.len.max()\n",
    "print(max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZlTGo1dNaewx"
   },
   "outputs": [],
   "source": [
    "lista_completa = df[['category','name']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FIYgawHqagjg"
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "from collections import Counter\n",
    "\n",
    "cosa = list(itertools.chain(lista_completa[:,1]))\n",
    "cosa_word = sorted(list(set(list(itertools.chain.from_iterable(cosa)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 384,
     "status": "ok",
     "timestamp": 1591637236687,
     "user": {
      "displayName": "Daniel Cuiñas Vázquez",
      "photoUrl": "",
      "userId": "11552885780691803973"
     },
     "user_tz": -120
    },
    "id": "3e5GquBqah4F",
    "outputId": "0ff840a8-344b-4f17-d582-a4f78b6b7831"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59\n"
     ]
    }
   ],
   "source": [
    "dict_words = {}\n",
    "dict_words['[UNK]'] = 0\n",
    "dict_words['[EOS]'] = 1\n",
    "dict_words['[PAD]'] = 2\n",
    "dict_words.update({v:k for k,v in enumerate(cosa_word, 3)})\n",
    "int_2_vocab = {v:k for k,v in dict_words.items()}\n",
    "n_words = len(dict_words)\n",
    "print(n_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vdIfQEGQai-f"
   },
   "outputs": [],
   "source": [
    "cosa = [[dict_words.get(j,0) for j in d] + [dict_words['[EOS]']] for d in lista_completa[:,1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AadT_ZO2akOE"
   },
   "outputs": [],
   "source": [
    "def pad_features(reviews_ints, seq_length):\n",
    "    ''' Return features of review_ints, where each review is padded with 0's \n",
    "        or truncated to the input seq_length.\n",
    "    '''\n",
    "    \n",
    "    # getting the correct rows x cols shape\n",
    "    features = dict_words['[PAD]']*np.ones((len(reviews_ints), seq_length), dtype=int)\n",
    "\n",
    "    # for each review, I grab that review and \n",
    "    for i, row in enumerate(reviews_ints):\n",
    "        features[i, :len(row)] = np.array(row)[:seq_length]\n",
    "    \n",
    "    return features\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AtekOPrXalsc"
   },
   "outputs": [],
   "source": [
    "features = pad_features(cosa, max_len +1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 671,
     "status": "ok",
     "timestamp": 1591637239030,
     "user": {
      "displayName": "Daniel Cuiñas Vázquez",
      "photoUrl": "",
      "userId": "11552885780691803973"
     },
     "user_tz": -120
    },
    "id": "9dDMJ7fvanMw",
    "outputId": "b4d1c68d-5d24-4365-f6a8-8b0c98801f98"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Romilly[EOS][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]'"
      ]
     },
     "execution_count": 17,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join([int_2_vocab[i] for i in features[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zn9VpNKQapHS"
   },
   "outputs": [],
   "source": [
    "category_features = lista_completa[:,0]\n",
    "category_features = np.expand_dims(category_features, axis=0).transpose(1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "liF9nHw_aq1K"
   },
   "outputs": [],
   "source": [
    "all_features = np.concatenate((category_features, features), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PQQPKgzrasUR"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# One-hot matrix of first to last letters (not including EOS) for input\n",
    "def inputArray(line):\n",
    "    category_index_input = line[:,0]\n",
    "    category_index_input = np.stack(category_index_input, axis=0)\n",
    "    word_indexes_input = line[:,1:]\n",
    "    word_indexes_target = line[:,2:]\n",
    "    to_append = np.array([word_indexes_target.shape[0]*[dict_words['[PAD]']]]).transpose(1,0)\n",
    "    word_indexes_target = np.concatenate((word_indexes_target, to_append), axis=1)\n",
    "    return category_index_input, word_indexes_input, word_indexes_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0BIGrmKUatwg"
   },
   "outputs": [],
   "source": [
    "cat_inputs, inputs, targets = inputArray(all_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IDr_3C9tavCC"
   },
   "outputs": [],
   "source": [
    "inputs = inputs.astype('int64')\n",
    "targets = targets.astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1769,
     "status": "ok",
     "timestamp": 1591637244079,
     "user": {
      "displayName": "Daniel Cuiñas Vázquez",
      "photoUrl": "",
      "userId": "11552885780691803973"
     },
     "user_tz": -120
    },
    "id": "GwGjb6R9aw7w",
    "outputId": "81f9b260-1fde-405f-8ed1-8eff7dc6ec61"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tFeature Shapes:\n",
      "Train set: \t\t(18066, 21) \n",
      "Validation set: \t(1004, 21) \n",
      "Test set: \t\t(1004, 21)\n"
     ]
    }
   ],
   "source": [
    "split_frac = 0.9\n",
    "\n",
    "## split data into training, validation, and test data (features and labels, x and y)\n",
    "\n",
    "split_idx = int(len(inputs)*split_frac)\n",
    "train_x, remaining_x = inputs[:split_idx], inputs[split_idx:]\n",
    "train_y, remaining_y = targets[:split_idx], targets[split_idx:]\n",
    "train_cat, remaining_cat = cat_inputs[:split_idx], cat_inputs[split_idx:]\n",
    "\n",
    "test_idx = int(len(remaining_x)*0.5)\n",
    "val_x, test_x = remaining_x[:test_idx], remaining_x[test_idx:]\n",
    "val_y, test_y = remaining_y[:test_idx], remaining_y[test_idx:]\n",
    "val_cat, test_cat = remaining_cat[:test_idx], remaining_cat[test_idx:]\n",
    "\n",
    "## print out the shapes of your resultant feature data\n",
    "print(\"\\t\\t\\tFeature Shapes:\")\n",
    "print(\"Train set: \\t\\t{}\".format(train_x.shape), \n",
    "      \"\\nValidation set: \\t{}\".format(val_x.shape),\n",
    "      \"\\nTest set: \\t\\t{}\".format(test_x.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "beFZdXD2ayS5"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "train_data = TensorDataset(torch.from_numpy(train_cat), torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
    "valid_data = TensorDataset(torch.from_numpy(val_cat), torch.from_numpy(val_x), torch.from_numpy(val_y))\n",
    "test_data = TensorDataset(torch.from_numpy(test_cat), torch.from_numpy(test_x), torch.from_numpy(test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Jlt8d6-Waz8f"
   },
   "outputs": [],
   "source": [
    "# dataloaders\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# make sure the SHUFFLE your training data\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=BATCH_SIZE)\n",
    "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(test_data, shuffle=True, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 394,
     "status": "ok",
     "timestamp": 1591637245138,
     "user": {
      "displayName": "Daniel Cuiñas Vázquez",
      "photoUrl": "",
      "userId": "11552885780691803973"
     },
     "user_tz": -120
    },
    "id": "DoFkMFjla1Ny",
    "outputId": "bb2510eb-0697-4bd3-ab19-876243551427"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MSv-y9yZa2jK"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    \"\"\"Container module with an encoder, a recurrent module, and a decoder.\"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, embed_dim, hid_dim, n_layers, dropout=0.5):\n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.encoder = nn.Embedding(input_dim, embed_dim, padding_idx = 2)\n",
    "    \n",
    "        self.rnn = nn.GRU(n_categories + embed_dim,\n",
    "                          hid_dim,\n",
    "                          n_layers,\n",
    "                          dropout = 0 if n_layers < 2 else dropout)\n",
    "        \n",
    "        self.decoder = nn.Linear(hid_dim, input_dim)\n",
    "\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.init_weights()\n",
    "\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, cat, inputs, hidden):\n",
    "\n",
    "        seq_len = inputs.shape[0]\n",
    "\n",
    "        #cat = [batch size, n_cats]\n",
    "        #inputs = [seq len, batch size]\n",
    "        #hidden = [num layers * num directions, batch size, hid dim]\n",
    "        \n",
    "        emb = self.drop(self.encoder(inputs))\n",
    "        \n",
    "        #emb = [seq len, batch size, emb dim]\n",
    "\n",
    "        cat = cat.unsqueeze(0).repeat(seq_len, 1, 1)\n",
    "\n",
    "        combined_input = torch.cat((cat, emb), 2)\n",
    "\n",
    "        #combined_input = [seq len, batch size, n_cats + emb dim]\n",
    "        \n",
    "        output, hidden = self.rnn(combined_input, hidden)\n",
    "        output = self.drop(output)\n",
    "        \n",
    "        #output = [seq len, batch size, hid dim * num directions]\n",
    "        #hidden = [num layers * num directions, batch size, hid dim]\n",
    "        \n",
    "        decoded = self.decoder(output)\n",
    "        \n",
    "        #decoded = [seq len, batch size, vocab size]\n",
    "        \n",
    "        decoded = decoded.view(-1, self.input_dim)\n",
    "        \n",
    "        #decoded = [seq len * batch size, vocab size]\n",
    "\n",
    "        return F.log_softmax(decoded, dim=1), hidden\n",
    "\n",
    "    def init_hidden(self, bsz):\n",
    "        weight = next(self.parameters())\n",
    "\n",
    "        return weight.new_zeros(self.n_layers, bsz, self.hid_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gFT4zjVbcs0M"
   },
   "outputs": [],
   "source": [
    "embed_dim = 50\n",
    "hid_dim = 128\n",
    "n_layers = 1\n",
    "dropout = 0.1\n",
    "\n",
    "model = RNN(n_words, embed_dim, hid_dim, n_layers, dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 368,
     "status": "ok",
     "timestamp": 1591637250845,
     "user": {
      "displayName": "Daniel Cuiñas Vázquez",
      "photoUrl": "",
      "userId": "11552885780691803973"
     },
     "user_tz": -120
    },
    "id": "HdKS5jlLcujg",
    "outputId": "1780e98b-fcc1-4870-e81f-d5995e88a0f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 86,593 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8QrVTScac1sg"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9Td4Ofmac3zo"
   },
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "You5EOzpc5QY"
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.0003\n",
    "optimizer = optim.Adam(model.parameters(), learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Yc471KWKfQS9"
   },
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    total_loss = 0\n",
    "    model.train()\n",
    "    \n",
    "    j = 0\n",
    "    \n",
    "    for categories, inputs, targets in iterator:\n",
    "        bsz = inputs.shape[0]\n",
    "        hidden = model.init_hidden(bsz)\n",
    "        hidden = hidden.to(device)\n",
    "        loss = 0\n",
    "        categories, inputs, targets = categories.to(device), inputs.to(device), targets.to(device)\n",
    "        \n",
    "        inputs = inputs.transpose(1,0)\n",
    "        targets = targets.transpose(1,0)\n",
    "        targets = targets.flatten()\n",
    "\n",
    "        categories = categories.float()\n",
    "        inputs = inputs.long()\n",
    "        targets = targets.long()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        hidden = hidden.detach()\n",
    "      \n",
    "        output, hidden = model(categories, inputs, hidden)\n",
    "        loss = criterion(output, targets)\n",
    "            \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        epoch_loss = loss.item()#/seq_len_batched\n",
    "        \n",
    "        total_loss += epoch_loss\n",
    "        \n",
    "        j += 1\n",
    "        \n",
    "        if j % log_interval == 0:\n",
    "            cur_loss = epoch_loss #/ log_interval\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | lr {} | '\n",
    "                    'loss {:5.2f} | ppl {:8.2f}'.format(\n",
    "                epoch, j, len(iterator), learning_rate,\n",
    "                cur_loss, math.exp(cur_loss)))\n",
    "            # total_loss = 0\n",
    "            # example,_,_ = sample('practicante derecho,abogado')\n",
    "            # print(example)\n",
    "        \n",
    "    return total_loss / len(iterator)\n",
    "\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    total_loss = 0\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for categories, inputs, targets in iterator:\n",
    "            bsz = inputs.shape[0]\n",
    "            hidden = model.init_hidden(bsz)\n",
    "            hidden = hidden.to(device)\n",
    "            \n",
    "            loss = 0\n",
    "            \n",
    "            categories, inputs, targets = categories.to(device), inputs.to(device), targets.to(device)\n",
    "\n",
    "            inputs = inputs.transpose(1,0)\n",
    "            targets = targets.transpose(1,0)\n",
    "            targets = targets.flatten()\n",
    "\n",
    "            categories = categories.float()\n",
    "            inputs = inputs.long()\n",
    "            targets = targets.long()\n",
    "            \n",
    "            output, hidden = model(categories, inputs, hidden)\n",
    "            loss = criterion(output, targets)\n",
    "             \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "    return total_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SHi0NzMSgTrH"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 62283,
     "status": "ok",
     "timestamp": 1591637316524,
     "user": {
      "displayName": "Daniel Cuiñas Vázquez",
      "photoUrl": "",
      "userId": "11552885780691803973"
     },
     "user_tz": -120
    },
    "id": "BKvkMoApgWft",
    "outputId": "7296f0af-e74f-4aea-f81e-304a4a3db54d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   0 |    25/  283 batches | lr 0.0003 | loss  3.66 | ppl    38.94\n",
      "| epoch   0 |    50/  283 batches | lr 0.0003 | loss  1.62 | ppl     5.07\n",
      "| epoch   0 |    75/  283 batches | lr 0.0003 | loss  1.35 | ppl     3.87\n",
      "| epoch   0 |   100/  283 batches | lr 0.0003 | loss  1.40 | ppl     4.04\n",
      "| epoch   0 |   125/  283 batches | lr 0.0003 | loss  1.40 | ppl     4.04\n",
      "| epoch   0 |   150/  283 batches | lr 0.0003 | loss  1.35 | ppl     3.86\n",
      "| epoch   0 |   175/  283 batches | lr 0.0003 | loss  1.17 | ppl     3.23\n",
      "| epoch   0 |   200/  283 batches | lr 0.0003 | loss  1.26 | ppl     3.51\n",
      "| epoch   0 |   225/  283 batches | lr 0.0003 | loss  1.22 | ppl     3.40\n",
      "| epoch   0 |   250/  283 batches | lr 0.0003 | loss  1.18 | ppl     3.25\n",
      "| epoch   0 |   275/  283 batches | lr 0.0003 | loss  1.07 | ppl     2.92\n",
      "Epoch: 01 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 1.671 | Val. Loss: 1.173\n",
      "| epoch   1 |    25/  283 batches | lr 0.0003 | loss  1.05 | ppl     2.87\n",
      "| epoch   1 |    50/  283 batches | lr 0.0003 | loss  1.06 | ppl     2.89\n",
      "| epoch   1 |    75/  283 batches | lr 0.0003 | loss  1.11 | ppl     3.03\n",
      "| epoch   1 |   100/  283 batches | lr 0.0003 | loss  0.98 | ppl     2.66\n",
      "| epoch   1 |   125/  283 batches | lr 0.0003 | loss  1.14 | ppl     3.13\n",
      "| epoch   1 |   150/  283 batches | lr 0.0003 | loss  0.97 | ppl     2.63\n",
      "| epoch   1 |   175/  283 batches | lr 0.0003 | loss  1.01 | ppl     2.75\n",
      "| epoch   1 |   200/  283 batches | lr 0.0003 | loss  1.03 | ppl     2.80\n",
      "| epoch   1 |   225/  283 batches | lr 0.0003 | loss  0.97 | ppl     2.65\n",
      "| epoch   1 |   250/  283 batches | lr 0.0003 | loss  0.96 | ppl     2.60\n",
      "| epoch   1 |   275/  283 batches | lr 0.0003 | loss  0.98 | ppl     2.67\n",
      "Epoch: 02 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 1.044 | Val. Loss: 0.983\n",
      "| epoch   2 |    25/  283 batches | lr 0.0003 | loss  0.92 | ppl     2.52\n",
      "| epoch   2 |    50/  283 batches | lr 0.0003 | loss  0.88 | ppl     2.42\n",
      "| epoch   2 |    75/  283 batches | lr 0.0003 | loss  0.97 | ppl     2.64\n",
      "| epoch   2 |   100/  283 batches | lr 0.0003 | loss  0.94 | ppl     2.56\n",
      "| epoch   2 |   125/  283 batches | lr 0.0003 | loss  0.94 | ppl     2.55\n",
      "| epoch   2 |   150/  283 batches | lr 0.0003 | loss  0.94 | ppl     2.57\n",
      "| epoch   2 |   175/  283 batches | lr 0.0003 | loss  0.94 | ppl     2.57\n",
      "| epoch   2 |   200/  283 batches | lr 0.0003 | loss  0.91 | ppl     2.48\n",
      "| epoch   2 |   225/  283 batches | lr 0.0003 | loss  0.98 | ppl     2.67\n",
      "| epoch   2 |   250/  283 batches | lr 0.0003 | loss  0.92 | ppl     2.50\n",
      "| epoch   2 |   275/  283 batches | lr 0.0003 | loss  0.89 | ppl     2.42\n",
      "Epoch: 03 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.934 | Val. Loss: 0.889\n",
      "| epoch   3 |    25/  283 batches | lr 0.0003 | loss  0.85 | ppl     2.35\n",
      "| epoch   3 |    50/  283 batches | lr 0.0003 | loss  0.90 | ppl     2.46\n",
      "| epoch   3 |    75/  283 batches | lr 0.0003 | loss  0.83 | ppl     2.29\n",
      "| epoch   3 |   100/  283 batches | lr 0.0003 | loss  0.88 | ppl     2.41\n",
      "| epoch   3 |   125/  283 batches | lr 0.0003 | loss  0.83 | ppl     2.29\n",
      "| epoch   3 |   150/  283 batches | lr 0.0003 | loss  0.82 | ppl     2.27\n",
      "| epoch   3 |   175/  283 batches | lr 0.0003 | loss  0.86 | ppl     2.37\n",
      "| epoch   3 |   200/  283 batches | lr 0.0003 | loss  0.83 | ppl     2.29\n",
      "| epoch   3 |   225/  283 batches | lr 0.0003 | loss  0.88 | ppl     2.40\n",
      "| epoch   3 |   250/  283 batches | lr 0.0003 | loss  0.79 | ppl     2.20\n",
      "| epoch   3 |   275/  283 batches | lr 0.0003 | loss  0.84 | ppl     2.32\n",
      "Epoch: 04 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.848 | Val. Loss: 0.825\n",
      "| epoch   4 |    25/  283 batches | lr 0.0003 | loss  0.88 | ppl     2.42\n",
      "| epoch   4 |    50/  283 batches | lr 0.0003 | loss  0.79 | ppl     2.20\n",
      "| epoch   4 |    75/  283 batches | lr 0.0003 | loss  0.78 | ppl     2.19\n",
      "| epoch   4 |   100/  283 batches | lr 0.0003 | loss  0.83 | ppl     2.29\n",
      "| epoch   4 |   125/  283 batches | lr 0.0003 | loss  0.80 | ppl     2.23\n",
      "| epoch   4 |   150/  283 batches | lr 0.0003 | loss  0.81 | ppl     2.25\n",
      "| epoch   4 |   175/  283 batches | lr 0.0003 | loss  0.82 | ppl     2.28\n",
      "| epoch   4 |   200/  283 batches | lr 0.0003 | loss  0.74 | ppl     2.11\n",
      "| epoch   4 |   225/  283 batches | lr 0.0003 | loss  0.83 | ppl     2.30\n",
      "| epoch   4 |   250/  283 batches | lr 0.0003 | loss  0.86 | ppl     2.37\n",
      "| epoch   4 |   275/  283 batches | lr 0.0003 | loss  0.77 | ppl     2.17\n",
      "Epoch: 05 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.807 | Val. Loss: 0.791\n",
      "| epoch   5 |    25/  283 batches | lr 0.0003 | loss  0.76 | ppl     2.13\n",
      "| epoch   5 |    50/  283 batches | lr 0.0003 | loss  0.79 | ppl     2.20\n",
      "| epoch   5 |    75/  283 batches | lr 0.0003 | loss  0.73 | ppl     2.07\n",
      "| epoch   5 |   100/  283 batches | lr 0.0003 | loss  0.77 | ppl     2.16\n",
      "| epoch   5 |   125/  283 batches | lr 0.0003 | loss  0.86 | ppl     2.36\n",
      "| epoch   5 |   150/  283 batches | lr 0.0003 | loss  0.76 | ppl     2.13\n",
      "| epoch   5 |   175/  283 batches | lr 0.0003 | loss  0.72 | ppl     2.05\n",
      "| epoch   5 |   200/  283 batches | lr 0.0003 | loss  0.86 | ppl     2.36\n",
      "| epoch   5 |   225/  283 batches | lr 0.0003 | loss  0.73 | ppl     2.08\n",
      "| epoch   5 |   250/  283 batches | lr 0.0003 | loss  0.76 | ppl     2.14\n",
      "| epoch   5 |   275/  283 batches | lr 0.0003 | loss  0.73 | ppl     2.08\n",
      "Epoch: 06 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.783 | Val. Loss: 0.773\n",
      "| epoch   6 |    25/  283 batches | lr 0.0003 | loss  0.75 | ppl     2.12\n",
      "| epoch   6 |    50/  283 batches | lr 0.0003 | loss  0.76 | ppl     2.13\n",
      "| epoch   6 |    75/  283 batches | lr 0.0003 | loss  0.76 | ppl     2.14\n",
      "| epoch   6 |   100/  283 batches | lr 0.0003 | loss  0.77 | ppl     2.17\n",
      "| epoch   6 |   125/  283 batches | lr 0.0003 | loss  0.75 | ppl     2.11\n",
      "| epoch   6 |   150/  283 batches | lr 0.0003 | loss  0.76 | ppl     2.14\n",
      "| epoch   6 |   175/  283 batches | lr 0.0003 | loss  0.76 | ppl     2.14\n",
      "| epoch   6 |   200/  283 batches | lr 0.0003 | loss  0.68 | ppl     1.98\n",
      "| epoch   6 |   225/  283 batches | lr 0.0003 | loss  0.79 | ppl     2.20\n",
      "| epoch   6 |   250/  283 batches | lr 0.0003 | loss  0.73 | ppl     2.08\n",
      "| epoch   6 |   275/  283 batches | lr 0.0003 | loss  0.79 | ppl     2.20\n",
      "Epoch: 07 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.766 | Val. Loss: 0.759\n",
      "| epoch   7 |    25/  283 batches | lr 0.0003 | loss  0.80 | ppl     2.24\n",
      "| epoch   7 |    50/  283 batches | lr 0.0003 | loss  0.70 | ppl     2.02\n",
      "| epoch   7 |    75/  283 batches | lr 0.0003 | loss  0.73 | ppl     2.08\n",
      "| epoch   7 |   100/  283 batches | lr 0.0003 | loss  0.73 | ppl     2.07\n",
      "| epoch   7 |   125/  283 batches | lr 0.0003 | loss  0.76 | ppl     2.14\n",
      "| epoch   7 |   150/  283 batches | lr 0.0003 | loss  0.78 | ppl     2.17\n",
      "| epoch   7 |   175/  283 batches | lr 0.0003 | loss  0.73 | ppl     2.08\n",
      "| epoch   7 |   200/  283 batches | lr 0.0003 | loss  0.77 | ppl     2.16\n",
      "| epoch   7 |   225/  283 batches | lr 0.0003 | loss  0.74 | ppl     2.11\n",
      "| epoch   7 |   250/  283 batches | lr 0.0003 | loss  0.65 | ppl     1.92\n",
      "| epoch   7 |   275/  283 batches | lr 0.0003 | loss  0.75 | ppl     2.11\n",
      "Epoch: 08 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.754 | Val. Loss: 0.747\n",
      "| epoch   8 |    25/  283 batches | lr 0.0003 | loss  0.76 | ppl     2.14\n",
      "| epoch   8 |    50/  283 batches | lr 0.0003 | loss  0.84 | ppl     2.31\n",
      "| epoch   8 |    75/  283 batches | lr 0.0003 | loss  0.73 | ppl     2.07\n",
      "| epoch   8 |   100/  283 batches | lr 0.0003 | loss  0.67 | ppl     1.95\n",
      "| epoch   8 |   125/  283 batches | lr 0.0003 | loss  0.76 | ppl     2.15\n",
      "| epoch   8 |   150/  283 batches | lr 0.0003 | loss  0.76 | ppl     2.13\n",
      "| epoch   8 |   175/  283 batches | lr 0.0003 | loss  0.71 | ppl     2.03\n",
      "| epoch   8 |   200/  283 batches | lr 0.0003 | loss  0.70 | ppl     2.02\n",
      "| epoch   8 |   225/  283 batches | lr 0.0003 | loss  0.78 | ppl     2.19\n",
      "| epoch   8 |   250/  283 batches | lr 0.0003 | loss  0.74 | ppl     2.10\n",
      "| epoch   8 |   275/  283 batches | lr 0.0003 | loss  0.76 | ppl     2.14\n",
      "Epoch: 09 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.743 | Val. Loss: 0.736\n",
      "| epoch   9 |    25/  283 batches | lr 0.0003 | loss  0.74 | ppl     2.09\n",
      "| epoch   9 |    50/  283 batches | lr 0.0003 | loss  0.74 | ppl     2.09\n",
      "| epoch   9 |    75/  283 batches | lr 0.0003 | loss  0.75 | ppl     2.11\n",
      "| epoch   9 |   100/  283 batches | lr 0.0003 | loss  0.73 | ppl     2.08\n",
      "| epoch   9 |   125/  283 batches | lr 0.0003 | loss  0.72 | ppl     2.05\n",
      "| epoch   9 |   150/  283 batches | lr 0.0003 | loss  0.76 | ppl     2.14\n",
      "| epoch   9 |   175/  283 batches | lr 0.0003 | loss  0.70 | ppl     2.02\n",
      "| epoch   9 |   200/  283 batches | lr 0.0003 | loss  0.81 | ppl     2.25\n",
      "| epoch   9 |   225/  283 batches | lr 0.0003 | loss  0.70 | ppl     2.01\n",
      "| epoch   9 |   250/  283 batches | lr 0.0003 | loss  0.71 | ppl     2.04\n",
      "| epoch   9 |   275/  283 batches | lr 0.0003 | loss  0.70 | ppl     2.01\n",
      "Epoch: 10 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.734 | Val. Loss: 0.726\n",
      "| epoch  10 |    25/  283 batches | lr 0.0003 | loss  0.76 | ppl     2.15\n",
      "| epoch  10 |    50/  283 batches | lr 0.0003 | loss  0.74 | ppl     2.09\n",
      "| epoch  10 |    75/  283 batches | lr 0.0003 | loss  0.69 | ppl     2.00\n",
      "| epoch  10 |   100/  283 batches | lr 0.0003 | loss  0.67 | ppl     1.96\n",
      "| epoch  10 |   125/  283 batches | lr 0.0003 | loss  0.72 | ppl     2.05\n",
      "| epoch  10 |   150/  283 batches | lr 0.0003 | loss  0.71 | ppl     2.03\n",
      "| epoch  10 |   175/  283 batches | lr 0.0003 | loss  0.74 | ppl     2.10\n",
      "| epoch  10 |   200/  283 batches | lr 0.0003 | loss  0.78 | ppl     2.19\n",
      "| epoch  10 |   225/  283 batches | lr 0.0003 | loss  0.70 | ppl     2.01\n",
      "| epoch  10 |   250/  283 batches | lr 0.0003 | loss  0.70 | ppl     2.01\n",
      "| epoch  10 |   275/  283 batches | lr 0.0003 | loss  0.71 | ppl     2.03\n",
      "Epoch: 11 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.725 | Val. Loss: 0.719\n",
      "| epoch  11 |    25/  283 batches | lr 0.0003 | loss  0.69 | ppl     2.00\n",
      "| epoch  11 |    50/  283 batches | lr 0.0003 | loss  0.72 | ppl     2.05\n",
      "| epoch  11 |    75/  283 batches | lr 0.0003 | loss  0.71 | ppl     2.03\n",
      "| epoch  11 |   100/  283 batches | lr 0.0003 | loss  0.73 | ppl     2.08\n",
      "| epoch  11 |   125/  283 batches | lr 0.0003 | loss  0.72 | ppl     2.06\n",
      "| epoch  11 |   150/  283 batches | lr 0.0003 | loss  0.74 | ppl     2.10\n",
      "| epoch  11 |   175/  283 batches | lr 0.0003 | loss  0.72 | ppl     2.06\n",
      "| epoch  11 |   200/  283 batches | lr 0.0003 | loss  0.70 | ppl     2.01\n",
      "| epoch  11 |   225/  283 batches | lr 0.0003 | loss  0.72 | ppl     2.05\n",
      "| epoch  11 |   250/  283 batches | lr 0.0003 | loss  0.76 | ppl     2.14\n",
      "| epoch  11 |   275/  283 batches | lr 0.0003 | loss  0.68 | ppl     1.98\n",
      "Epoch: 12 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.717 | Val. Loss: 0.711\n",
      "| epoch  12 |    25/  283 batches | lr 0.0003 | loss  0.75 | ppl     2.11\n",
      "| epoch  12 |    50/  283 batches | lr 0.0003 | loss  0.70 | ppl     2.02\n",
      "| epoch  12 |    75/  283 batches | lr 0.0003 | loss  0.70 | ppl     2.01\n",
      "| epoch  12 |   100/  283 batches | lr 0.0003 | loss  0.71 | ppl     2.04\n",
      "| epoch  12 |   125/  283 batches | lr 0.0003 | loss  0.71 | ppl     2.03\n",
      "| epoch  12 |   150/  283 batches | lr 0.0003 | loss  0.71 | ppl     2.04\n",
      "| epoch  12 |   175/  283 batches | lr 0.0003 | loss  0.72 | ppl     2.06\n",
      "| epoch  12 |   200/  283 batches | lr 0.0003 | loss  0.67 | ppl     1.96\n",
      "| epoch  12 |   225/  283 batches | lr 0.0003 | loss  0.70 | ppl     2.01\n",
      "| epoch  12 |   250/  283 batches | lr 0.0003 | loss  0.68 | ppl     1.98\n",
      "| epoch  12 |   275/  283 batches | lr 0.0003 | loss  0.67 | ppl     1.96\n",
      "Epoch: 13 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.708 | Val. Loss: 0.705\n",
      "| epoch  13 |    25/  283 batches | lr 0.0003 | loss  0.71 | ppl     2.04\n",
      "| epoch  13 |    50/  283 batches | lr 0.0003 | loss  0.70 | ppl     2.02\n",
      "| epoch  13 |    75/  283 batches | lr 0.0003 | loss  0.69 | ppl     2.00\n",
      "| epoch  13 |   100/  283 batches | lr 0.0003 | loss  0.67 | ppl     1.95\n",
      "| epoch  13 |   125/  283 batches | lr 0.0003 | loss  0.73 | ppl     2.09\n",
      "| epoch  13 |   150/  283 batches | lr 0.0003 | loss  0.69 | ppl     1.99\n",
      "| epoch  13 |   175/  283 batches | lr 0.0003 | loss  0.72 | ppl     2.06\n",
      "| epoch  13 |   200/  283 batches | lr 0.0003 | loss  0.67 | ppl     1.96\n",
      "| epoch  13 |   225/  283 batches | lr 0.0003 | loss  0.70 | ppl     2.02\n",
      "| epoch  13 |   250/  283 batches | lr 0.0003 | loss  0.71 | ppl     2.03\n",
      "| epoch  13 |   275/  283 batches | lr 0.0003 | loss  0.72 | ppl     2.05\n",
      "Epoch: 14 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.701 | Val. Loss: 0.695\n",
      "| epoch  14 |    25/  283 batches | lr 0.0003 | loss  0.67 | ppl     1.95\n",
      "| epoch  14 |    50/  283 batches | lr 0.0003 | loss  0.66 | ppl     1.93\n",
      "| epoch  14 |    75/  283 batches | lr 0.0003 | loss  0.69 | ppl     2.00\n",
      "| epoch  14 |   100/  283 batches | lr 0.0003 | loss  0.66 | ppl     1.93\n",
      "| epoch  14 |   125/  283 batches | lr 0.0003 | loss  0.69 | ppl     1.99\n",
      "| epoch  14 |   150/  283 batches | lr 0.0003 | loss  0.71 | ppl     2.04\n",
      "| epoch  14 |   175/  283 batches | lr 0.0003 | loss  0.68 | ppl     1.97\n",
      "| epoch  14 |   200/  283 batches | lr 0.0003 | loss  0.64 | ppl     1.90\n",
      "| epoch  14 |   225/  283 batches | lr 0.0003 | loss  0.72 | ppl     2.06\n",
      "| epoch  14 |   250/  283 batches | lr 0.0003 | loss  0.61 | ppl     1.85\n",
      "| epoch  14 |   275/  283 batches | lr 0.0003 | loss  0.63 | ppl     1.89\n",
      "Epoch: 15 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.695 | Val. Loss: 0.689\n",
      "| epoch  15 |    25/  283 batches | lr 0.0003 | loss  0.68 | ppl     1.97\n",
      "| epoch  15 |    50/  283 batches | lr 0.0003 | loss  0.72 | ppl     2.05\n",
      "| epoch  15 |    75/  283 batches | lr 0.0003 | loss  0.69 | ppl     1.99\n",
      "| epoch  15 |   100/  283 batches | lr 0.0003 | loss  0.73 | ppl     2.08\n",
      "| epoch  15 |   125/  283 batches | lr 0.0003 | loss  0.69 | ppl     1.99\n",
      "| epoch  15 |   150/  283 batches | lr 0.0003 | loss  0.71 | ppl     2.03\n",
      "| epoch  15 |   175/  283 batches | lr 0.0003 | loss  0.71 | ppl     2.03\n",
      "| epoch  15 |   200/  283 batches | lr 0.0003 | loss  0.72 | ppl     2.05\n",
      "| epoch  15 |   225/  283 batches | lr 0.0003 | loss  0.67 | ppl     1.95\n",
      "| epoch  15 |   250/  283 batches | lr 0.0003 | loss  0.66 | ppl     1.94\n",
      "| epoch  15 |   275/  283 batches | lr 0.0003 | loss  0.67 | ppl     1.95\n",
      "Epoch: 16 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.688 | Val. Loss: 0.683\n",
      "| epoch  16 |    25/  283 batches | lr 0.0003 | loss  0.71 | ppl     2.03\n",
      "| epoch  16 |    50/  283 batches | lr 0.0003 | loss  0.66 | ppl     1.94\n",
      "| epoch  16 |    75/  283 batches | lr 0.0003 | loss  0.70 | ppl     2.01\n",
      "| epoch  16 |   100/  283 batches | lr 0.0003 | loss  0.63 | ppl     1.88\n",
      "| epoch  16 |   125/  283 batches | lr 0.0003 | loss  0.70 | ppl     2.00\n",
      "| epoch  16 |   150/  283 batches | lr 0.0003 | loss  0.68 | ppl     1.97\n",
      "| epoch  16 |   175/  283 batches | lr 0.0003 | loss  0.65 | ppl     1.92\n",
      "| epoch  16 |   200/  283 batches | lr 0.0003 | loss  0.71 | ppl     2.04\n",
      "| epoch  16 |   225/  283 batches | lr 0.0003 | loss  0.72 | ppl     2.05\n",
      "| epoch  16 |   250/  283 batches | lr 0.0003 | loss  0.68 | ppl     1.98\n",
      "| epoch  16 |   275/  283 batches | lr 0.0003 | loss  0.65 | ppl     1.91\n",
      "Epoch: 17 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.683 | Val. Loss: 0.679\n",
      "| epoch  17 |    25/  283 batches | lr 0.0003 | loss  0.65 | ppl     1.92\n",
      "| epoch  17 |    50/  283 batches | lr 0.0003 | loss  0.74 | ppl     2.09\n",
      "| epoch  17 |    75/  283 batches | lr 0.0003 | loss  0.65 | ppl     1.91\n",
      "| epoch  17 |   100/  283 batches | lr 0.0003 | loss  0.64 | ppl     1.90\n",
      "| epoch  17 |   125/  283 batches | lr 0.0003 | loss  0.70 | ppl     2.02\n",
      "| epoch  17 |   150/  283 batches | lr 0.0003 | loss  0.67 | ppl     1.96\n",
      "| epoch  17 |   175/  283 batches | lr 0.0003 | loss  0.69 | ppl     1.99\n",
      "| epoch  17 |   200/  283 batches | lr 0.0003 | loss  0.66 | ppl     1.94\n",
      "| epoch  17 |   225/  283 batches | lr 0.0003 | loss  0.73 | ppl     2.06\n",
      "| epoch  17 |   250/  283 batches | lr 0.0003 | loss  0.68 | ppl     1.97\n",
      "| epoch  17 |   275/  283 batches | lr 0.0003 | loss  0.66 | ppl     1.93\n",
      "Epoch: 18 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.677 | Val. Loss: 0.673\n",
      "| epoch  18 |    25/  283 batches | lr 0.0003 | loss  0.64 | ppl     1.90\n",
      "| epoch  18 |    50/  283 batches | lr 0.0003 | loss  0.65 | ppl     1.91\n",
      "| epoch  18 |    75/  283 batches | lr 0.0003 | loss  0.66 | ppl     1.93\n",
      "| epoch  18 |   100/  283 batches | lr 0.0003 | loss  0.68 | ppl     1.97\n",
      "| epoch  18 |   125/  283 batches | lr 0.0003 | loss  0.65 | ppl     1.91\n",
      "| epoch  18 |   150/  283 batches | lr 0.0003 | loss  0.67 | ppl     1.96\n",
      "| epoch  18 |   175/  283 batches | lr 0.0003 | loss  0.66 | ppl     1.94\n",
      "| epoch  18 |   200/  283 batches | lr 0.0003 | loss  0.68 | ppl     1.98\n",
      "| epoch  18 |   225/  283 batches | lr 0.0003 | loss  0.62 | ppl     1.85\n",
      "| epoch  18 |   250/  283 batches | lr 0.0003 | loss  0.68 | ppl     1.97\n",
      "| epoch  18 |   275/  283 batches | lr 0.0003 | loss  0.62 | ppl     1.87\n",
      "Epoch: 19 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.673 | Val. Loss: 0.669\n",
      "| epoch  19 |    25/  283 batches | lr 0.0003 | loss  0.70 | ppl     2.01\n",
      "| epoch  19 |    50/  283 batches | lr 0.0003 | loss  0.65 | ppl     1.91\n",
      "| epoch  19 |    75/  283 batches | lr 0.0003 | loss  0.61 | ppl     1.84\n",
      "| epoch  19 |   100/  283 batches | lr 0.0003 | loss  0.66 | ppl     1.93\n",
      "| epoch  19 |   125/  283 batches | lr 0.0003 | loss  0.63 | ppl     1.87\n",
      "| epoch  19 |   150/  283 batches | lr 0.0003 | loss  0.70 | ppl     2.01\n",
      "| epoch  19 |   175/  283 batches | lr 0.0003 | loss  0.69 | ppl     1.99\n",
      "| epoch  19 |   200/  283 batches | lr 0.0003 | loss  0.68 | ppl     1.98\n",
      "| epoch  19 |   225/  283 batches | lr 0.0003 | loss  0.64 | ppl     1.91\n",
      "| epoch  19 |   250/  283 batches | lr 0.0003 | loss  0.61 | ppl     1.84\n",
      "| epoch  19 |   275/  283 batches | lr 0.0003 | loss  0.68 | ppl     1.98\n",
      "Epoch: 20 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.667 | Val. Loss: 0.666\n",
      "| epoch  20 |    25/  283 batches | lr 0.0003 | loss  0.70 | ppl     2.01\n",
      "| epoch  20 |    50/  283 batches | lr 0.0003 | loss  0.68 | ppl     1.98\n",
      "| epoch  20 |    75/  283 batches | lr 0.0003 | loss  0.67 | ppl     1.95\n",
      "| epoch  20 |   100/  283 batches | lr 0.0003 | loss  0.64 | ppl     1.90\n",
      "| epoch  20 |   125/  283 batches | lr 0.0003 | loss  0.64 | ppl     1.90\n",
      "| epoch  20 |   150/  283 batches | lr 0.0003 | loss  0.64 | ppl     1.90\n",
      "| epoch  20 |   175/  283 batches | lr 0.0003 | loss  0.65 | ppl     1.91\n",
      "| epoch  20 |   200/  283 batches | lr 0.0003 | loss  0.72 | ppl     2.06\n",
      "| epoch  20 |   225/  283 batches | lr 0.0003 | loss  0.70 | ppl     2.01\n",
      "| epoch  20 |   250/  283 batches | lr 0.0003 | loss  0.64 | ppl     1.89\n",
      "| epoch  20 |   275/  283 batches | lr 0.0003 | loss  0.66 | ppl     1.93\n",
      "Epoch: 21 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.663 | Val. Loss: 0.663\n",
      "| epoch  21 |    25/  283 batches | lr 0.0003 | loss  0.66 | ppl     1.93\n",
      "| epoch  21 |    50/  283 batches | lr 0.0003 | loss  0.64 | ppl     1.90\n",
      "| epoch  21 |    75/  283 batches | lr 0.0003 | loss  0.61 | ppl     1.85\n",
      "| epoch  21 |   100/  283 batches | lr 0.0003 | loss  0.63 | ppl     1.88\n",
      "| epoch  21 |   125/  283 batches | lr 0.0003 | loss  0.71 | ppl     2.04\n",
      "| epoch  21 |   150/  283 batches | lr 0.0003 | loss  0.66 | ppl     1.93\n",
      "| epoch  21 |   175/  283 batches | lr 0.0003 | loss  0.70 | ppl     2.01\n",
      "| epoch  21 |   200/  283 batches | lr 0.0003 | loss  0.61 | ppl     1.84\n",
      "| epoch  21 |   225/  283 batches | lr 0.0003 | loss  0.66 | ppl     1.94\n",
      "| epoch  21 |   250/  283 batches | lr 0.0003 | loss  0.66 | ppl     1.93\n",
      "| epoch  21 |   275/  283 batches | lr 0.0003 | loss  0.65 | ppl     1.91\n",
      "Epoch: 22 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.658 | Val. Loss: 0.658\n",
      "| epoch  22 |    25/  283 batches | lr 0.0003 | loss  0.69 | ppl     2.00\n",
      "| epoch  22 |    50/  283 batches | lr 0.0003 | loss  0.63 | ppl     1.88\n",
      "| epoch  22 |    75/  283 batches | lr 0.0003 | loss  0.61 | ppl     1.85\n",
      "| epoch  22 |   100/  283 batches | lr 0.0003 | loss  0.63 | ppl     1.88\n",
      "| epoch  22 |   125/  283 batches | lr 0.0003 | loss  0.64 | ppl     1.90\n",
      "| epoch  22 |   150/  283 batches | lr 0.0003 | loss  0.61 | ppl     1.84\n",
      "| epoch  22 |   175/  283 batches | lr 0.0003 | loss  0.70 | ppl     2.02\n",
      "| epoch  22 |   200/  283 batches | lr 0.0003 | loss  0.67 | ppl     1.96\n",
      "| epoch  22 |   225/  283 batches | lr 0.0003 | loss  0.66 | ppl     1.93\n",
      "| epoch  22 |   250/  283 batches | lr 0.0003 | loss  0.68 | ppl     1.98\n",
      "| epoch  22 |   275/  283 batches | lr 0.0003 | loss  0.65 | ppl     1.92\n",
      "Epoch: 23 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.654 | Val. Loss: 0.655\n",
      "| epoch  23 |    25/  283 batches | lr 0.0003 | loss  0.64 | ppl     1.90\n",
      "| epoch  23 |    50/  283 batches | lr 0.0003 | loss  0.63 | ppl     1.87\n",
      "| epoch  23 |    75/  283 batches | lr 0.0003 | loss  0.64 | ppl     1.89\n",
      "| epoch  23 |   100/  283 batches | lr 0.0003 | loss  0.62 | ppl     1.86\n",
      "| epoch  23 |   125/  283 batches | lr 0.0003 | loss  0.68 | ppl     1.97\n",
      "| epoch  23 |   150/  283 batches | lr 0.0003 | loss  0.69 | ppl     2.00\n",
      "| epoch  23 |   175/  283 batches | lr 0.0003 | loss  0.63 | ppl     1.88\n",
      "| epoch  23 |   200/  283 batches | lr 0.0003 | loss  0.70 | ppl     2.01\n",
      "| epoch  23 |   225/  283 batches | lr 0.0003 | loss  0.63 | ppl     1.87\n",
      "| epoch  23 |   250/  283 batches | lr 0.0003 | loss  0.61 | ppl     1.84\n",
      "| epoch  23 |   275/  283 batches | lr 0.0003 | loss  0.61 | ppl     1.84\n",
      "Epoch: 24 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.650 | Val. Loss: 0.650\n",
      "| epoch  24 |    25/  283 batches | lr 0.0003 | loss  0.68 | ppl     1.97\n",
      "| epoch  24 |    50/  283 batches | lr 0.0003 | loss  0.61 | ppl     1.84\n",
      "| epoch  24 |    75/  283 batches | lr 0.0003 | loss  0.68 | ppl     1.97\n",
      "| epoch  24 |   100/  283 batches | lr 0.0003 | loss  0.60 | ppl     1.82\n",
      "| epoch  24 |   125/  283 batches | lr 0.0003 | loss  0.65 | ppl     1.92\n",
      "| epoch  24 |   150/  283 batches | lr 0.0003 | loss  0.64 | ppl     1.90\n",
      "| epoch  24 |   175/  283 batches | lr 0.0003 | loss  0.66 | ppl     1.93\n",
      "| epoch  24 |   200/  283 batches | lr 0.0003 | loss  0.65 | ppl     1.91\n",
      "| epoch  24 |   225/  283 batches | lr 0.0003 | loss  0.68 | ppl     1.97\n",
      "| epoch  24 |   250/  283 batches | lr 0.0003 | loss  0.62 | ppl     1.86\n",
      "| epoch  24 |   275/  283 batches | lr 0.0003 | loss  0.57 | ppl     1.78\n",
      "Epoch: 25 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.646 | Val. Loss: 0.646\n",
      "| epoch  25 |    25/  283 batches | lr 0.0003 | loss  0.62 | ppl     1.86\n",
      "| epoch  25 |    50/  283 batches | lr 0.0003 | loss  0.66 | ppl     1.93\n",
      "| epoch  25 |    75/  283 batches | lr 0.0003 | loss  0.70 | ppl     2.00\n",
      "| epoch  25 |   100/  283 batches | lr 0.0003 | loss  0.65 | ppl     1.92\n",
      "| epoch  25 |   125/  283 batches | lr 0.0003 | loss  0.67 | ppl     1.95\n",
      "| epoch  25 |   150/  283 batches | lr 0.0003 | loss  0.62 | ppl     1.87\n",
      "| epoch  25 |   175/  283 batches | lr 0.0003 | loss  0.63 | ppl     1.89\n",
      "| epoch  25 |   200/  283 batches | lr 0.0003 | loss  0.63 | ppl     1.89\n",
      "| epoch  25 |   225/  283 batches | lr 0.0003 | loss  0.68 | ppl     1.98\n",
      "| epoch  25 |   250/  283 batches | lr 0.0003 | loss  0.67 | ppl     1.95\n",
      "| epoch  25 |   275/  283 batches | lr 0.0003 | loss  0.62 | ppl     1.86\n",
      "Epoch: 26 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.643 | Val. Loss: 0.643\n",
      "| epoch  26 |    25/  283 batches | lr 0.0003 | loss  0.60 | ppl     1.82\n",
      "| epoch  26 |    50/  283 batches | lr 0.0003 | loss  0.66 | ppl     1.94\n",
      "| epoch  26 |    75/  283 batches | lr 0.0003 | loss  0.62 | ppl     1.85\n",
      "| epoch  26 |   100/  283 batches | lr 0.0003 | loss  0.64 | ppl     1.90\n",
      "| epoch  26 |   125/  283 batches | lr 0.0003 | loss  0.67 | ppl     1.95\n",
      "| epoch  26 |   150/  283 batches | lr 0.0003 | loss  0.60 | ppl     1.83\n",
      "| epoch  26 |   175/  283 batches | lr 0.0003 | loss  0.65 | ppl     1.92\n",
      "| epoch  26 |   200/  283 batches | lr 0.0003 | loss  0.66 | ppl     1.94\n",
      "| epoch  26 |   225/  283 batches | lr 0.0003 | loss  0.64 | ppl     1.91\n",
      "| epoch  26 |   250/  283 batches | lr 0.0003 | loss  0.63 | ppl     1.87\n",
      "| epoch  26 |   275/  283 batches | lr 0.0003 | loss  0.61 | ppl     1.84\n",
      "Epoch: 27 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.639 | Val. Loss: 0.640\n",
      "| epoch  27 |    25/  283 batches | lr 0.0003 | loss  0.68 | ppl     1.97\n",
      "| epoch  27 |    50/  283 batches | lr 0.0003 | loss  0.62 | ppl     1.85\n",
      "| epoch  27 |    75/  283 batches | lr 0.0003 | loss  0.64 | ppl     1.91\n",
      "| epoch  27 |   100/  283 batches | lr 0.0003 | loss  0.65 | ppl     1.92\n",
      "| epoch  27 |   125/  283 batches | lr 0.0003 | loss  0.65 | ppl     1.91\n",
      "| epoch  27 |   150/  283 batches | lr 0.0003 | loss  0.59 | ppl     1.81\n",
      "| epoch  27 |   175/  283 batches | lr 0.0003 | loss  0.66 | ppl     1.94\n",
      "| epoch  27 |   200/  283 batches | lr 0.0003 | loss  0.65 | ppl     1.91\n",
      "| epoch  27 |   225/  283 batches | lr 0.0003 | loss  0.63 | ppl     1.87\n",
      "| epoch  27 |   250/  283 batches | lr 0.0003 | loss  0.60 | ppl     1.83\n",
      "| epoch  27 |   275/  283 batches | lr 0.0003 | loss  0.61 | ppl     1.83\n",
      "Epoch: 28 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.635 | Val. Loss: 0.636\n",
      "| epoch  28 |    25/  283 batches | lr 0.0003 | loss  0.65 | ppl     1.91\n",
      "| epoch  28 |    50/  283 batches | lr 0.0003 | loss  0.63 | ppl     1.87\n",
      "| epoch  28 |    75/  283 batches | lr 0.0003 | loss  0.62 | ppl     1.87\n",
      "| epoch  28 |   100/  283 batches | lr 0.0003 | loss  0.60 | ppl     1.83\n",
      "| epoch  28 |   125/  283 batches | lr 0.0003 | loss  0.61 | ppl     1.85\n",
      "| epoch  28 |   150/  283 batches | lr 0.0003 | loss  0.56 | ppl     1.75\n",
      "| epoch  28 |   175/  283 batches | lr 0.0003 | loss  0.63 | ppl     1.87\n",
      "| epoch  28 |   200/  283 batches | lr 0.0003 | loss  0.58 | ppl     1.79\n",
      "| epoch  28 |   225/  283 batches | lr 0.0003 | loss  0.62 | ppl     1.86\n",
      "| epoch  28 |   250/  283 batches | lr 0.0003 | loss  0.67 | ppl     1.96\n",
      "| epoch  28 |   275/  283 batches | lr 0.0003 | loss  0.57 | ppl     1.77\n",
      "Epoch: 29 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.633 | Val. Loss: 0.633\n",
      "| epoch  29 |    25/  283 batches | lr 0.0003 | loss  0.62 | ppl     1.87\n",
      "| epoch  29 |    50/  283 batches | lr 0.0003 | loss  0.64 | ppl     1.89\n",
      "| epoch  29 |    75/  283 batches | lr 0.0003 | loss  0.61 | ppl     1.84\n",
      "| epoch  29 |   100/  283 batches | lr 0.0003 | loss  0.63 | ppl     1.87\n",
      "| epoch  29 |   125/  283 batches | lr 0.0003 | loss  0.61 | ppl     1.84\n",
      "| epoch  29 |   150/  283 batches | lr 0.0003 | loss  0.64 | ppl     1.90\n",
      "| epoch  29 |   175/  283 batches | lr 0.0003 | loss  0.60 | ppl     1.83\n",
      "| epoch  29 |   200/  283 batches | lr 0.0003 | loss  0.65 | ppl     1.92\n",
      "| epoch  29 |   225/  283 batches | lr 0.0003 | loss  0.63 | ppl     1.87\n",
      "| epoch  29 |   250/  283 batches | lr 0.0003 | loss  0.67 | ppl     1.95\n",
      "| epoch  29 |   275/  283 batches | lr 0.0003 | loss  0.58 | ppl     1.79\n",
      "Epoch: 30 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.629 | Val. Loss: 0.630\n",
      "| epoch  30 |    25/  283 batches | lr 0.0003 | loss  0.59 | ppl     1.81\n",
      "| epoch  30 |    50/  283 batches | lr 0.0003 | loss  0.67 | ppl     1.95\n",
      "| epoch  30 |    75/  283 batches | lr 0.0003 | loss  0.61 | ppl     1.85\n",
      "| epoch  30 |   100/  283 batches | lr 0.0003 | loss  0.67 | ppl     1.95\n",
      "| epoch  30 |   125/  283 batches | lr 0.0003 | loss  0.59 | ppl     1.80\n",
      "| epoch  30 |   150/  283 batches | lr 0.0003 | loss  0.68 | ppl     1.98\n",
      "| epoch  30 |   175/  283 batches | lr 0.0003 | loss  0.62 | ppl     1.86\n",
      "| epoch  30 |   200/  283 batches | lr 0.0003 | loss  0.61 | ppl     1.84\n",
      "| epoch  30 |   225/  283 batches | lr 0.0003 | loss  0.60 | ppl     1.82\n",
      "| epoch  30 |   250/  283 batches | lr 0.0003 | loss  0.63 | ppl     1.88\n",
      "| epoch  30 |   275/  283 batches | lr 0.0003 | loss  0.65 | ppl     1.91\n",
      "Epoch: 31 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.625 | Val. Loss: 0.628\n",
      "| epoch  31 |    25/  283 batches | lr 0.0003 | loss  0.62 | ppl     1.86\n",
      "| epoch  31 |    50/  283 batches | lr 0.0003 | loss  0.60 | ppl     1.82\n",
      "| epoch  31 |    75/  283 batches | lr 0.0003 | loss  0.61 | ppl     1.85\n",
      "| epoch  31 |   100/  283 batches | lr 0.0003 | loss  0.59 | ppl     1.80\n",
      "| epoch  31 |   125/  283 batches | lr 0.0003 | loss  0.61 | ppl     1.83\n",
      "| epoch  31 |   150/  283 batches | lr 0.0003 | loss  0.64 | ppl     1.89\n",
      "| epoch  31 |   175/  283 batches | lr 0.0003 | loss  0.55 | ppl     1.74\n",
      "| epoch  31 |   200/  283 batches | lr 0.0003 | loss  0.62 | ppl     1.86\n",
      "| epoch  31 |   225/  283 batches | lr 0.0003 | loss  0.58 | ppl     1.79\n",
      "| epoch  31 |   250/  283 batches | lr 0.0003 | loss  0.65 | ppl     1.92\n",
      "| epoch  31 |   275/  283 batches | lr 0.0003 | loss  0.62 | ppl     1.86\n",
      "Epoch: 32 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.623 | Val. Loss: 0.624\n",
      "| epoch  32 |    25/  283 batches | lr 0.0003 | loss  0.61 | ppl     1.84\n",
      "| epoch  32 |    50/  283 batches | lr 0.0003 | loss  0.62 | ppl     1.85\n",
      "| epoch  32 |    75/  283 batches | lr 0.0003 | loss  0.60 | ppl     1.81\n",
      "| epoch  32 |   100/  283 batches | lr 0.0003 | loss  0.62 | ppl     1.86\n",
      "| epoch  32 |   125/  283 batches | lr 0.0003 | loss  0.63 | ppl     1.88\n",
      "| epoch  32 |   150/  283 batches | lr 0.0003 | loss  0.61 | ppl     1.84\n",
      "| epoch  32 |   175/  283 batches | lr 0.0003 | loss  0.62 | ppl     1.86\n",
      "| epoch  32 |   200/  283 batches | lr 0.0003 | loss  0.63 | ppl     1.87\n",
      "| epoch  32 |   225/  283 batches | lr 0.0003 | loss  0.64 | ppl     1.89\n",
      "| epoch  32 |   250/  283 batches | lr 0.0003 | loss  0.58 | ppl     1.78\n",
      "| epoch  32 |   275/  283 batches | lr 0.0003 | loss  0.63 | ppl     1.87\n",
      "Epoch: 33 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.619 | Val. Loss: 0.621\n",
      "| epoch  33 |    25/  283 batches | lr 0.0003 | loss  0.61 | ppl     1.85\n",
      "| epoch  33 |    50/  283 batches | lr 0.0003 | loss  0.62 | ppl     1.86\n",
      "| epoch  33 |    75/  283 batches | lr 0.0003 | loss  0.71 | ppl     2.03\n",
      "| epoch  33 |   100/  283 batches | lr 0.0003 | loss  0.63 | ppl     1.88\n",
      "| epoch  33 |   125/  283 batches | lr 0.0003 | loss  0.61 | ppl     1.84\n",
      "| epoch  33 |   150/  283 batches | lr 0.0003 | loss  0.63 | ppl     1.89\n",
      "| epoch  33 |   175/  283 batches | lr 0.0003 | loss  0.63 | ppl     1.88\n",
      "| epoch  33 |   200/  283 batches | lr 0.0003 | loss  0.64 | ppl     1.89\n",
      "| epoch  33 |   225/  283 batches | lr 0.0003 | loss  0.59 | ppl     1.80\n",
      "| epoch  33 |   250/  283 batches | lr 0.0003 | loss  0.62 | ppl     1.85\n",
      "| epoch  33 |   275/  283 batches | lr 0.0003 | loss  0.60 | ppl     1.83\n",
      "Epoch: 34 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.616 | Val. Loss: 0.621\n",
      "| epoch  34 |    25/  283 batches | lr 0.0003 | loss  0.60 | ppl     1.83\n",
      "| epoch  34 |    50/  283 batches | lr 0.0003 | loss  0.57 | ppl     1.76\n",
      "| epoch  34 |    75/  283 batches | lr 0.0003 | loss  0.66 | ppl     1.93\n",
      "| epoch  34 |   100/  283 batches | lr 0.0003 | loss  0.62 | ppl     1.85\n",
      "| epoch  34 |   125/  283 batches | lr 0.0003 | loss  0.65 | ppl     1.92\n",
      "| epoch  34 |   150/  283 batches | lr 0.0003 | loss  0.63 | ppl     1.87\n",
      "| epoch  34 |   175/  283 batches | lr 0.0003 | loss  0.58 | ppl     1.78\n",
      "| epoch  34 |   200/  283 batches | lr 0.0003 | loss  0.59 | ppl     1.81\n",
      "| epoch  34 |   225/  283 batches | lr 0.0003 | loss  0.59 | ppl     1.81\n",
      "| epoch  34 |   250/  283 batches | lr 0.0003 | loss  0.61 | ppl     1.84\n",
      "| epoch  34 |   275/  283 batches | lr 0.0003 | loss  0.66 | ppl     1.93\n",
      "Epoch: 35 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.614 | Val. Loss: 0.616\n",
      "| epoch  35 |    25/  283 batches | lr 0.0003 | loss  0.60 | ppl     1.82\n",
      "| epoch  35 |    50/  283 batches | lr 0.0003 | loss  0.64 | ppl     1.90\n",
      "| epoch  35 |    75/  283 batches | lr 0.0003 | loss  0.71 | ppl     2.03\n",
      "| epoch  35 |   100/  283 batches | lr 0.0003 | loss  0.61 | ppl     1.83\n",
      "| epoch  35 |   125/  283 batches | lr 0.0003 | loss  0.66 | ppl     1.94\n",
      "| epoch  35 |   150/  283 batches | lr 0.0003 | loss  0.59 | ppl     1.80\n",
      "| epoch  35 |   175/  283 batches | lr 0.0003 | loss  0.62 | ppl     1.86\n",
      "| epoch  35 |   200/  283 batches | lr 0.0003 | loss  0.55 | ppl     1.74\n",
      "| epoch  35 |   225/  283 batches | lr 0.0003 | loss  0.62 | ppl     1.87\n",
      "| epoch  35 |   250/  283 batches | lr 0.0003 | loss  0.60 | ppl     1.83\n",
      "| epoch  35 |   275/  283 batches | lr 0.0003 | loss  0.67 | ppl     1.96\n",
      "Epoch: 36 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.610 | Val. Loss: 0.614\n",
      "| epoch  36 |    25/  283 batches | lr 0.0003 | loss  0.60 | ppl     1.83\n",
      "| epoch  36 |    50/  283 batches | lr 0.0003 | loss  0.63 | ppl     1.87\n",
      "| epoch  36 |    75/  283 batches | lr 0.0003 | loss  0.61 | ppl     1.84\n",
      "| epoch  36 |   100/  283 batches | lr 0.0003 | loss  0.58 | ppl     1.78\n",
      "| epoch  36 |   125/  283 batches | lr 0.0003 | loss  0.57 | ppl     1.77\n",
      "| epoch  36 |   150/  283 batches | lr 0.0003 | loss  0.60 | ppl     1.82\n",
      "| epoch  36 |   175/  283 batches | lr 0.0003 | loss  0.63 | ppl     1.88\n",
      "| epoch  36 |   200/  283 batches | lr 0.0003 | loss  0.65 | ppl     1.92\n",
      "| epoch  36 |   225/  283 batches | lr 0.0003 | loss  0.61 | ppl     1.84\n",
      "| epoch  36 |   250/  283 batches | lr 0.0003 | loss  0.62 | ppl     1.86\n",
      "| epoch  36 |   275/  283 batches | lr 0.0003 | loss  0.62 | ppl     1.86\n",
      "Epoch: 37 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.609 | Val. Loss: 0.613\n",
      "| epoch  37 |    25/  283 batches | lr 0.0003 | loss  0.60 | ppl     1.83\n",
      "| epoch  37 |    50/  283 batches | lr 0.0003 | loss  0.61 | ppl     1.83\n",
      "| epoch  37 |    75/  283 batches | lr 0.0003 | loss  0.55 | ppl     1.74\n",
      "| epoch  37 |   100/  283 batches | lr 0.0003 | loss  0.58 | ppl     1.78\n",
      "| epoch  37 |   125/  283 batches | lr 0.0003 | loss  0.61 | ppl     1.85\n",
      "| epoch  37 |   150/  283 batches | lr 0.0003 | loss  0.63 | ppl     1.87\n",
      "| epoch  37 |   175/  283 batches | lr 0.0003 | loss  0.62 | ppl     1.87\n",
      "| epoch  37 |   200/  283 batches | lr 0.0003 | loss  0.59 | ppl     1.80\n",
      "| epoch  37 |   225/  283 batches | lr 0.0003 | loss  0.61 | ppl     1.84\n",
      "| epoch  37 |   250/  283 batches | lr 0.0003 | loss  0.61 | ppl     1.83\n",
      "| epoch  37 |   275/  283 batches | lr 0.0003 | loss  0.58 | ppl     1.79\n",
      "Epoch: 38 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.607 | Val. Loss: 0.613\n",
      "| epoch  38 |    25/  283 batches | lr 0.0003 | loss  0.65 | ppl     1.92\n",
      "| epoch  38 |    50/  283 batches | lr 0.0003 | loss  0.59 | ppl     1.80\n",
      "| epoch  38 |    75/  283 batches | lr 0.0003 | loss  0.58 | ppl     1.79\n",
      "| epoch  38 |   100/  283 batches | lr 0.0003 | loss  0.60 | ppl     1.82\n",
      "| epoch  38 |   125/  283 batches | lr 0.0003 | loss  0.65 | ppl     1.92\n",
      "| epoch  38 |   150/  283 batches | lr 0.0003 | loss  0.57 | ppl     1.77\n",
      "| epoch  38 |   175/  283 batches | lr 0.0003 | loss  0.63 | ppl     1.87\n",
      "| epoch  38 |   200/  283 batches | lr 0.0003 | loss  0.61 | ppl     1.84\n",
      "| epoch  38 |   225/  283 batches | lr 0.0003 | loss  0.57 | ppl     1.77\n",
      "| epoch  38 |   250/  283 batches | lr 0.0003 | loss  0.63 | ppl     1.88\n",
      "| epoch  38 |   275/  283 batches | lr 0.0003 | loss  0.59 | ppl     1.80\n",
      "Epoch: 39 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.604 | Val. Loss: 0.608\n",
      "| epoch  39 |    25/  283 batches | lr 0.0003 | loss  0.56 | ppl     1.74\n",
      "| epoch  39 |    50/  283 batches | lr 0.0003 | loss  0.59 | ppl     1.80\n",
      "| epoch  39 |    75/  283 batches | lr 0.0003 | loss  0.58 | ppl     1.79\n",
      "| epoch  39 |   100/  283 batches | lr 0.0003 | loss  0.64 | ppl     1.89\n",
      "| epoch  39 |   125/  283 batches | lr 0.0003 | loss  0.62 | ppl     1.85\n",
      "| epoch  39 |   150/  283 batches | lr 0.0003 | loss  0.61 | ppl     1.84\n",
      "| epoch  39 |   175/  283 batches | lr 0.0003 | loss  0.61 | ppl     1.84\n",
      "| epoch  39 |   200/  283 batches | lr 0.0003 | loss  0.55 | ppl     1.74\n",
      "| epoch  39 |   225/  283 batches | lr 0.0003 | loss  0.61 | ppl     1.85\n",
      "| epoch  39 |   250/  283 batches | lr 0.0003 | loss  0.56 | ppl     1.75\n",
      "| epoch  39 |   275/  283 batches | lr 0.0003 | loss  0.56 | ppl     1.75\n",
      "Epoch: 40 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.601 | Val. Loss: 0.607\n",
      "| epoch  40 |    25/  283 batches | lr 0.0003 | loss  0.62 | ppl     1.85\n",
      "| epoch  40 |    50/  283 batches | lr 0.0003 | loss  0.64 | ppl     1.89\n",
      "| epoch  40 |    75/  283 batches | lr 0.0003 | loss  0.60 | ppl     1.82\n",
      "| epoch  40 |   100/  283 batches | lr 0.0003 | loss  0.57 | ppl     1.77\n",
      "| epoch  40 |   125/  283 batches | lr 0.0003 | loss  0.60 | ppl     1.82\n",
      "| epoch  40 |   150/  283 batches | lr 0.0003 | loss  0.57 | ppl     1.77\n",
      "| epoch  40 |   175/  283 batches | lr 0.0003 | loss  0.60 | ppl     1.82\n",
      "| epoch  40 |   200/  283 batches | lr 0.0003 | loss  0.63 | ppl     1.87\n",
      "| epoch  40 |   225/  283 batches | lr 0.0003 | loss  0.55 | ppl     1.73\n",
      "| epoch  40 |   250/  283 batches | lr 0.0003 | loss  0.64 | ppl     1.89\n",
      "| epoch  40 |   275/  283 batches | lr 0.0003 | loss  0.58 | ppl     1.79\n",
      "Epoch: 41 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.599 | Val. Loss: 0.605\n",
      "| epoch  41 |    25/  283 batches | lr 0.0003 | loss  0.62 | ppl     1.86\n",
      "| epoch  41 |    50/  283 batches | lr 0.0003 | loss  0.62 | ppl     1.86\n",
      "| epoch  41 |    75/  283 batches | lr 0.0003 | loss  0.57 | ppl     1.77\n",
      "| epoch  41 |   100/  283 batches | lr 0.0003 | loss  0.60 | ppl     1.83\n",
      "| epoch  41 |   125/  283 batches | lr 0.0003 | loss  0.58 | ppl     1.79\n",
      "| epoch  41 |   150/  283 batches | lr 0.0003 | loss  0.54 | ppl     1.71\n",
      "| epoch  41 |   175/  283 batches | lr 0.0003 | loss  0.60 | ppl     1.82\n",
      "| epoch  41 |   200/  283 batches | lr 0.0003 | loss  0.55 | ppl     1.73\n",
      "| epoch  41 |   225/  283 batches | lr 0.0003 | loss  0.62 | ppl     1.86\n",
      "| epoch  41 |   250/  283 batches | lr 0.0003 | loss  0.59 | ppl     1.81\n",
      "| epoch  41 |   275/  283 batches | lr 0.0003 | loss  0.62 | ppl     1.86\n",
      "Epoch: 42 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.597 | Val. Loss: 0.603\n",
      "| epoch  42 |    25/  283 batches | lr 0.0003 | loss  0.65 | ppl     1.91\n",
      "| epoch  42 |    50/  283 batches | lr 0.0003 | loss  0.58 | ppl     1.78\n",
      "| epoch  42 |    75/  283 batches | lr 0.0003 | loss  0.60 | ppl     1.83\n",
      "| epoch  42 |   100/  283 batches | lr 0.0003 | loss  0.59 | ppl     1.80\n",
      "| epoch  42 |   125/  283 batches | lr 0.0003 | loss  0.60 | ppl     1.82\n",
      "| epoch  42 |   150/  283 batches | lr 0.0003 | loss  0.58 | ppl     1.78\n",
      "| epoch  42 |   175/  283 batches | lr 0.0003 | loss  0.57 | ppl     1.78\n",
      "| epoch  42 |   200/  283 batches | lr 0.0003 | loss  0.56 | ppl     1.76\n",
      "| epoch  42 |   225/  283 batches | lr 0.0003 | loss  0.61 | ppl     1.84\n",
      "| epoch  42 |   250/  283 batches | lr 0.0003 | loss  0.62 | ppl     1.87\n",
      "| epoch  42 |   275/  283 batches | lr 0.0003 | loss  0.57 | ppl     1.77\n",
      "Epoch: 43 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.595 | Val. Loss: 0.601\n",
      "| epoch  43 |    25/  283 batches | lr 0.0003 | loss  0.57 | ppl     1.78\n",
      "| epoch  43 |    50/  283 batches | lr 0.0003 | loss  0.62 | ppl     1.86\n",
      "| epoch  43 |    75/  283 batches | lr 0.0003 | loss  0.66 | ppl     1.93\n",
      "| epoch  43 |   100/  283 batches | lr 0.0003 | loss  0.61 | ppl     1.84\n",
      "| epoch  43 |   125/  283 batches | lr 0.0003 | loss  0.63 | ppl     1.88\n",
      "| epoch  43 |   150/  283 batches | lr 0.0003 | loss  0.60 | ppl     1.82\n",
      "| epoch  43 |   175/  283 batches | lr 0.0003 | loss  0.57 | ppl     1.77\n",
      "| epoch  43 |   200/  283 batches | lr 0.0003 | loss  0.63 | ppl     1.88\n",
      "| epoch  43 |   225/  283 batches | lr 0.0003 | loss  0.61 | ppl     1.85\n",
      "| epoch  43 |   250/  283 batches | lr 0.0003 | loss  0.62 | ppl     1.86\n",
      "| epoch  43 |   275/  283 batches | lr 0.0003 | loss  0.58 | ppl     1.78\n",
      "Epoch: 44 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.593 | Val. Loss: 0.601\n",
      "| epoch  44 |    25/  283 batches | lr 0.0003 | loss  0.60 | ppl     1.82\n",
      "| epoch  44 |    50/  283 batches | lr 0.0003 | loss  0.63 | ppl     1.87\n",
      "| epoch  44 |    75/  283 batches | lr 0.0003 | loss  0.62 | ppl     1.85\n",
      "| epoch  44 |   100/  283 batches | lr 0.0003 | loss  0.58 | ppl     1.78\n",
      "| epoch  44 |   125/  283 batches | lr 0.0003 | loss  0.59 | ppl     1.81\n",
      "| epoch  44 |   150/  283 batches | lr 0.0003 | loss  0.62 | ppl     1.86\n",
      "| epoch  44 |   175/  283 batches | lr 0.0003 | loss  0.60 | ppl     1.81\n",
      "| epoch  44 |   200/  283 batches | lr 0.0003 | loss  0.61 | ppl     1.85\n",
      "| epoch  44 |   225/  283 batches | lr 0.0003 | loss  0.58 | ppl     1.79\n",
      "| epoch  44 |   250/  283 batches | lr 0.0003 | loss  0.59 | ppl     1.80\n",
      "| epoch  44 |   275/  283 batches | lr 0.0003 | loss  0.60 | ppl     1.83\n",
      "Epoch: 45 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.591 | Val. Loss: 0.598\n",
      "| epoch  45 |    25/  283 batches | lr 0.0003 | loss  0.59 | ppl     1.81\n",
      "| epoch  45 |    50/  283 batches | lr 0.0003 | loss  0.61 | ppl     1.85\n",
      "| epoch  45 |    75/  283 batches | lr 0.0003 | loss  0.63 | ppl     1.87\n",
      "| epoch  45 |   100/  283 batches | lr 0.0003 | loss  0.63 | ppl     1.88\n",
      "| epoch  45 |   125/  283 batches | lr 0.0003 | loss  0.55 | ppl     1.73\n",
      "| epoch  45 |   150/  283 batches | lr 0.0003 | loss  0.61 | ppl     1.84\n",
      "| epoch  45 |   175/  283 batches | lr 0.0003 | loss  0.61 | ppl     1.85\n",
      "| epoch  45 |   200/  283 batches | lr 0.0003 | loss  0.59 | ppl     1.80\n",
      "| epoch  45 |   225/  283 batches | lr 0.0003 | loss  0.59 | ppl     1.81\n",
      "| epoch  45 |   250/  283 batches | lr 0.0003 | loss  0.59 | ppl     1.81\n",
      "| epoch  45 |   275/  283 batches | lr 0.0003 | loss  0.57 | ppl     1.77\n",
      "Epoch: 46 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.589 | Val. Loss: 0.596\n",
      "| epoch  46 |    25/  283 batches | lr 0.0003 | loss  0.55 | ppl     1.73\n",
      "| epoch  46 |    50/  283 batches | lr 0.0003 | loss  0.60 | ppl     1.82\n",
      "| epoch  46 |    75/  283 batches | lr 0.0003 | loss  0.61 | ppl     1.84\n",
      "| epoch  46 |   100/  283 batches | lr 0.0003 | loss  0.61 | ppl     1.84\n",
      "| epoch  46 |   125/  283 batches | lr 0.0003 | loss  0.58 | ppl     1.78\n",
      "| epoch  46 |   150/  283 batches | lr 0.0003 | loss  0.60 | ppl     1.83\n",
      "| epoch  46 |   175/  283 batches | lr 0.0003 | loss  0.56 | ppl     1.74\n",
      "| epoch  46 |   200/  283 batches | lr 0.0003 | loss  0.55 | ppl     1.73\n",
      "| epoch  46 |   225/  283 batches | lr 0.0003 | loss  0.60 | ppl     1.82\n",
      "| epoch  46 |   250/  283 batches | lr 0.0003 | loss  0.57 | ppl     1.77\n",
      "| epoch  46 |   275/  283 batches | lr 0.0003 | loss  0.61 | ppl     1.85\n",
      "Epoch: 47 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.587 | Val. Loss: 0.595\n",
      "| epoch  47 |    25/  283 batches | lr 0.0003 | loss  0.60 | ppl     1.83\n",
      "| epoch  47 |    50/  283 batches | lr 0.0003 | loss  0.59 | ppl     1.80\n",
      "| epoch  47 |    75/  283 batches | lr 0.0003 | loss  0.59 | ppl     1.81\n",
      "| epoch  47 |   100/  283 batches | lr 0.0003 | loss  0.57 | ppl     1.77\n",
      "| epoch  47 |   125/  283 batches | lr 0.0003 | loss  0.58 | ppl     1.78\n",
      "| epoch  47 |   150/  283 batches | lr 0.0003 | loss  0.62 | ppl     1.86\n",
      "| epoch  47 |   175/  283 batches | lr 0.0003 | loss  0.57 | ppl     1.78\n",
      "| epoch  47 |   200/  283 batches | lr 0.0003 | loss  0.57 | ppl     1.76\n",
      "| epoch  47 |   225/  283 batches | lr 0.0003 | loss  0.60 | ppl     1.82\n",
      "| epoch  47 |   250/  283 batches | lr 0.0003 | loss  0.57 | ppl     1.76\n",
      "| epoch  47 |   275/  283 batches | lr 0.0003 | loss  0.49 | ppl     1.64\n",
      "Epoch: 48 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.585 | Val. Loss: 0.593\n",
      "| epoch  48 |    25/  283 batches | lr 0.0003 | loss  0.59 | ppl     1.81\n",
      "| epoch  48 |    50/  283 batches | lr 0.0003 | loss  0.57 | ppl     1.77\n",
      "| epoch  48 |    75/  283 batches | lr 0.0003 | loss  0.52 | ppl     1.68\n",
      "| epoch  48 |   100/  283 batches | lr 0.0003 | loss  0.60 | ppl     1.83\n",
      "| epoch  48 |   125/  283 batches | lr 0.0003 | loss  0.56 | ppl     1.75\n",
      "| epoch  48 |   150/  283 batches | lr 0.0003 | loss  0.59 | ppl     1.81\n",
      "| epoch  48 |   175/  283 batches | lr 0.0003 | loss  0.58 | ppl     1.79\n",
      "| epoch  48 |   200/  283 batches | lr 0.0003 | loss  0.60 | ppl     1.83\n",
      "| epoch  48 |   225/  283 batches | lr 0.0003 | loss  0.63 | ppl     1.87\n",
      "| epoch  48 |   250/  283 batches | lr 0.0003 | loss  0.61 | ppl     1.84\n",
      "| epoch  48 |   275/  283 batches | lr 0.0003 | loss  0.61 | ppl     1.85\n",
      "Epoch: 49 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.584 | Val. Loss: 0.592\n",
      "| epoch  49 |    25/  283 batches | lr 0.0003 | loss  0.57 | ppl     1.76\n",
      "| epoch  49 |    50/  283 batches | lr 0.0003 | loss  0.60 | ppl     1.82\n",
      "| epoch  49 |    75/  283 batches | lr 0.0003 | loss  0.62 | ppl     1.85\n",
      "| epoch  49 |   100/  283 batches | lr 0.0003 | loss  0.58 | ppl     1.79\n",
      "| epoch  49 |   125/  283 batches | lr 0.0003 | loss  0.54 | ppl     1.71\n",
      "| epoch  49 |   150/  283 batches | lr 0.0003 | loss  0.56 | ppl     1.76\n",
      "| epoch  49 |   175/  283 batches | lr 0.0003 | loss  0.58 | ppl     1.78\n",
      "| epoch  49 |   200/  283 batches | lr 0.0003 | loss  0.62 | ppl     1.86\n",
      "| epoch  49 |   225/  283 batches | lr 0.0003 | loss  0.66 | ppl     1.93\n",
      "| epoch  49 |   250/  283 batches | lr 0.0003 | loss  0.58 | ppl     1.78\n",
      "| epoch  49 |   275/  283 batches | lr 0.0003 | loss  0.57 | ppl     1.77\n",
      "Epoch: 50 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.582 | Val. Loss: 0.590\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 50\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "counter = 0\n",
    "patience = 5\n",
    "log_interval = 25\n",
    "plot_every = 10\n",
    "train_loses = []\n",
    "valid_loses = []\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    train_loss = train(model, train_loader, optimizer, criterion)\n",
    "    valid_loss = evaluate(model, valid_loader, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "    print(f'Epoch: {epoch + 1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Val. Loss: {valid_loss:.3f}')\n",
    "    \n",
    "    train_loses.append(train_loss)\n",
    "    valid_loses.append(valid_loss)\n",
    "\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        #torch.save(model.state_dict(), 'tut2-model.pt')\n",
    "        counter = 0 \n",
    "    else:\n",
    "        counter += 1\n",
    "        if counter >= patience:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 544,
     "status": "ok",
     "timestamp": 1591637378129,
     "user": {
      "displayName": "Daniel Cuiñas Vázquez",
      "photoUrl": "",
      "userId": "11552885780691803973"
     },
     "user_tz": -120
    },
    "id": "tJ24Dt00h5Bu",
    "outputId": "56579f6b-913c-4104-fa8b-f9630cf7e96a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fe9d01243c8>]"
      ]
     },
     "execution_count": 36,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "plt.figure()\n",
    "# plt.plot(train_loses)\n",
    "plt.plot(valid_loses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 518,
     "status": "ok",
     "timestamp": 1591637381818,
     "user": {
      "displayName": "Daniel Cuiñas Vázquez",
      "photoUrl": "",
      "userId": "11552885780691803973"
     },
     "user_tz": -120
    },
    "id": "ZhRyEF9R0v93",
    "outputId": "2e261871-fce3-4732-c9c2-36b8b4dcbda5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.588255800306797"
      ]
     },
     "execution_count": 37,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(model, test_loader, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0O_KodF00zC-"
   },
   "outputs": [],
   "source": [
    "def sample(category_, prime_str, predict_len = 10):\n",
    "    cat_input = categoryArray(all_categories[category_])\n",
    "    cat_tensor = torch.tensor(cat_input).unsqueeze(0).float()\n",
    "    word_indexes = [dict_words[li] for li in prime_str]\n",
    "    prime_input = torch.tensor(word_indexes).unsqueeze(1)\n",
    "    prime_input = prime_input.to(device)\n",
    "    cat_tensor = cat_tensor.to(device)\n",
    "    hidden = model.init_hidden(1)\n",
    "    hidden = hidden.to(device)\n",
    "    # Use priming string to \"build up\" hidden state\n",
    "    with torch.no_grad():\n",
    "        for p in range(len(prime_str) - 1):\n",
    "            _, hidden = model(cat_tensor, prime_input[p].unsqueeze(0), hidden)\n",
    "\n",
    "        inp = prime_input[-1].unsqueeze(0)\n",
    "        inp = inp.to(device)\n",
    "        predicted = prime_str\n",
    "        tops = []\n",
    "        for i in range(predict_len):\n",
    "            output, hidden = model(cat_tensor, inp, hidden)\n",
    "            topv, topp = output.topk(1)\n",
    "            _, topps = output.topk(20)\n",
    "            tops.append(topps)\n",
    "            topi = topp[0][0].item()\n",
    "            if topi == 1:\n",
    "                break   \n",
    "            else:\n",
    "                letter = int_2_vocab[topi]\n",
    "                predicted += letter\n",
    "            inp = [dict_words[letter]]\n",
    "            inp = torch.tensor(inp).to(device).unsqueeze(0)\n",
    "\n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 519,
     "status": "ok",
     "timestamp": 1591639630254,
     "user": {
      "displayName": "Daniel Cuiñas Vázquez",
      "photoUrl": "",
      "userId": "11552885780691803973"
     },
     "user_tz": -120
    },
    "id": "_33IV9nGAbo4",
    "outputId": "924799ff-d84b-4864-a93c-860f9268b9fc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Olani'"
      ]
     },
     "execution_count": 77,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample('Italian',\"O\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FgAEtUFeHV9A"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyM4xPAVwFu56koUsFG4NI/E",
   "collapsed_sections": [],
   "name": "GRU_batch_line.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
