{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z4Ijc7SvyOyZ"
   },
   "outputs": [],
   "source": [
    "from io import open\n",
    "import glob\n",
    "import os\n",
    "import unicodedata\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "all_letters = string.ascii_letters + \" .,;'-\"\n",
    "n_letters = len(all_letters) + 1 # Plus EOS marker\n",
    "\n",
    "def findFiles(path):\n",
    "   return glob.glob(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mpEQnG2oyWK0"
   },
   "outputs": [],
   "source": [
    "# Turn a Unicode string to plain ASCII, thanks to https://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "        and c in all_letters\n",
    "    )\n",
    "\n",
    "# Read a file and split into lines\n",
    "def readLines(filename):\n",
    "    lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n",
    "    return [unicodeToAscii(line) for line in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VoloKNCtyX81"
   },
   "outputs": [],
   "source": [
    "# Build the category_lines dictionary, a list of lines per category\n",
    "category_lines = []\n",
    "all_categories = []\n",
    "for filename in findFiles('data/names/*.txt'):\n",
    "    category = os.path.splitext(os.path.basename(filename))[0]\n",
    "    all_categories.append(category)\n",
    "    lines = readLines(filename)\n",
    "    category_line = [category] * len(lines)\n",
    "    category_lines.extend(list(zip(category_line, lines)))\n",
    "\n",
    "n_categories = len(all_categories)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o_MQcFOo3ot0"
   },
   "outputs": [],
   "source": [
    "all_categories = {v:k for k,v in enumerate(all_categories)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UzrYjqm28Ifn"
   },
   "outputs": [],
   "source": [
    "# One-hot vector for category\n",
    "def categoryArray(cat):\n",
    "    arr = np.zeros(n_categories)\n",
    "    arr[cat] = 1\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 9866,
     "status": "ok",
     "timestamp": 1591637147379,
     "user": {
      "displayName": "Daniel Cuiñas Vázquez",
      "photoUrl": "",
      "userId": "11552885780691803973"
     },
     "user_tz": -120
    },
    "id": "Sn5UkT3YyZsf",
    "outputId": "67401914-3dd8-4611-9f88-75758d389daa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# categories: 18 {'Czech': 0, 'English': 1, 'Scottish': 2, 'Italian': 3, 'Polish': 4, 'Greek': 5, 'Dutch': 6, 'German': 7, 'Japanese': 8, 'Korean': 9, 'Arabic': 10, 'Portuguese': 11, 'Russian': 12, 'Vietnamese': 13, 'Irish': 14, 'Spanish': 15, 'French': 16, 'Chinese': 17}\n",
      "O'Neal\n"
     ]
    }
   ],
   "source": [
    "print('# categories:', n_categories, all_categories)\n",
    "print(unicodeToAscii(\"O'Néàl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lT13EXvtydJb"
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(category_lines, columns=['category_','name'])\n",
    "df['len'] = df['name'].apply(lambda s: len(s))\n",
    "df['category'] = df['category_'].apply(lambda c: all_categories[c])\n",
    "df['category'] = df['category'].apply(lambda c: categoryArray(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0eN43FsK8WyW"
   },
   "outputs": [],
   "source": [
    "df = df.sample(n=len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 8387,
     "status": "ok",
     "timestamp": 1591637147383,
     "user": {
      "displayName": "Daniel Cuiñas Vázquez",
      "photoUrl": "",
      "userId": "11552885780691803973"
     },
     "user_tz": -120
    },
    "id": "TeWTaUiW123n",
    "outputId": "010c5560-704b-4e54-e35f-9e13ebe6e779"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "max_len = df.len.max()\n",
    "print(max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nx-CwryOy1FZ"
   },
   "outputs": [],
   "source": [
    "lista_completa = df[['category','name']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "05Z0w0T00I8_"
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "from collections import Counter\n",
    "\n",
    "cosa = list(itertools.chain(lista_completa[:,1]))\n",
    "cosa_word = sorted(list(set(list(itertools.chain.from_iterable(cosa)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 446,
     "status": "ok",
     "timestamp": 1591637157504,
     "user": {
      "displayName": "Daniel Cuiñas Vázquez",
      "photoUrl": "",
      "userId": "11552885780691803973"
     },
     "user_tz": -120
    },
    "id": "YS5ZaKcc0Oqo",
    "outputId": "7129baee-f95f-4b59-f4a0-ef9960b58d59"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59\n"
     ]
    }
   ],
   "source": [
    "dict_words = {}\n",
    "dict_words['[UNK]'] = 0\n",
    "dict_words['[EOS]'] = 1\n",
    "dict_words['[PAD]'] = 2\n",
    "dict_words.update({v:k for k,v in enumerate(cosa_word,3)})\n",
    "int_2_vocab = {v:k for k,v in dict_words.items()}\n",
    "n_words = len(dict_words)\n",
    "print(n_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7PrmN70w0OtC"
   },
   "outputs": [],
   "source": [
    "cosa = [[dict_words.get(j,0) for j in d] + [dict_words['[EOS]']] for d in lista_completa[:,1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KKiu5mPr1Qqq"
   },
   "outputs": [],
   "source": [
    "# ''.join([int_2_vocab[i] for i in cosa[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a3hgV0_C04-G"
   },
   "outputs": [],
   "source": [
    "def pad_features(reviews_ints, seq_length):\n",
    "    ''' Return features of review_ints, where each review is padded with 0's \n",
    "        or truncated to the input seq_length.\n",
    "    '''\n",
    "    \n",
    "    # getting the correct rows x cols shape\n",
    "    features = dict_words['[PAD]']*np.ones((len(reviews_ints), seq_length), dtype=int)\n",
    "\n",
    "    # for each review, I grab that review and \n",
    "    for i, row in enumerate(reviews_ints):\n",
    "        features[i, :len(row)] = np.array(row)[:seq_length]\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qElDHg5L09vY"
   },
   "outputs": [],
   "source": [
    "features = pad_features(cosa, max_len + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 616,
     "status": "ok",
     "timestamp": 1591637162193,
     "user": {
      "displayName": "Daniel Cuiñas Vázquez",
      "photoUrl": "",
      "userId": "11552885780691803973"
     },
     "user_tz": -120
    },
    "id": "k6B2SAQQ1--D",
    "outputId": "01902d06-5b09-4ccb-9fea-32640329489e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Dagher[EOS][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]'"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join([int_2_vocab[i] for i in features[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4-RSCNYa4fEv"
   },
   "outputs": [],
   "source": [
    "category_features = lista_completa[:,0]\n",
    "category_features = np.expand_dims(category_features, axis=0).transpose(1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E08zNsIp4tSd"
   },
   "outputs": [],
   "source": [
    "all_features = np.concatenate((category_features, features), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SKTfIAMs1_vV"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# One-hot matrix of first to last letters (not including EOS) for input\n",
    "def inputArray(line):\n",
    "    category_index_input = line[:,0]\n",
    "    category_index_input = np.stack(category_index_input, axis=0)\n",
    "    word_indexes_input = line[:,1:]\n",
    "    word_indexes_target = line[:,2:]\n",
    "    to_append = np.array([word_indexes_target.shape[0]*[dict_words['[PAD]']]]).transpose(1,0)\n",
    "    word_indexes_target = np.concatenate((word_indexes_target, to_append), axis=1)\n",
    "    return category_index_input, word_indexes_input, word_indexes_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1XnkFVjV9iJ5"
   },
   "outputs": [],
   "source": [
    "cat_inputs, inputs, targets = inputArray(all_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lskodm5S_5ul"
   },
   "outputs": [],
   "source": [
    "inputs = inputs.astype('int64')\n",
    "targets = targets.astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 469,
     "status": "ok",
     "timestamp": 1591637173614,
     "user": {
      "displayName": "Daniel Cuiñas Vázquez",
      "photoUrl": "",
      "userId": "11552885780691803973"
     },
     "user_tz": -120
    },
    "id": "g-GVvaJ29-rJ",
    "outputId": "acb7c073-cc26-45f1-9042-2c426d328c71"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tFeature Shapes:\n",
      "Train set: \t\t(18066, 21) \n",
      "Validation set: \t(1004, 21) \n",
      "Test set: \t\t(1004, 21)\n"
     ]
    }
   ],
   "source": [
    "split_frac = 0.9\n",
    "\n",
    "## split data into training, validation, and test data (features and labels, x and y)\n",
    "\n",
    "split_idx = int(len(inputs)*split_frac)\n",
    "train_x, remaining_x = inputs[:split_idx], inputs[split_idx:]\n",
    "train_y, remaining_y = targets[:split_idx], targets[split_idx:]\n",
    "train_cat, remaining_cat = cat_inputs[:split_idx], cat_inputs[split_idx:]\n",
    "\n",
    "test_idx = int(len(remaining_x)*0.5)\n",
    "val_x, test_x = remaining_x[:test_idx], remaining_x[test_idx:]\n",
    "val_y, test_y = remaining_y[:test_idx], remaining_y[test_idx:]\n",
    "val_cat, test_cat = remaining_cat[:test_idx], remaining_cat[test_idx:]\n",
    "\n",
    "## print out the shapes of your resultant feature data\n",
    "print(\"\\t\\t\\tFeature Shapes:\")\n",
    "print(\"Train set: \\t\\t{}\".format(train_x.shape), \n",
    "      \"\\nValidation set: \\t{}\".format(val_x.shape),\n",
    "      \"\\nTest set: \\t\\t{}\".format(test_x.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oTgIGXUi-3-l"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "train_data = TensorDataset(torch.from_numpy(train_cat), torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
    "valid_data = TensorDataset(torch.from_numpy(val_cat), torch.from_numpy(val_x), torch.from_numpy(val_y))\n",
    "test_data = TensorDataset(torch.from_numpy(test_cat), torch.from_numpy(test_x), torch.from_numpy(test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6mgOd2Du-5rQ"
   },
   "outputs": [],
   "source": [
    "# dataloaders\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# make sure the SHUFFLE your training data\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=BATCH_SIZE)\n",
    "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(test_data, shuffle=True, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 882,
     "status": "ok",
     "timestamp": 1591637176268,
     "user": {
      "displayName": "Daniel Cuiñas Vázquez",
      "photoUrl": "",
      "userId": "11552885780691803973"
     },
     "user_tz": -120
    },
    "id": "OKlyo5Y7AWWU",
    "outputId": "a80feecf-1725-49db-c7de-3eecc19973be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QzifudK0AYAk"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_dim, embed_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.hid_dim = hid_dim\n",
    "\n",
    "        self.embedding = nn.Embedding(input_dim, embed_dim, padding_idx = 2)\n",
    "\n",
    "        self.rnn = nn.GRU(n_categories + embed_dim,\n",
    "                        hid_dim, \n",
    "                        num_layers=n_layers, \n",
    "                        dropout = 0 if n_layers < 2 else dropout)\n",
    "\n",
    "        self.fc = nn.Linear(hid_dim, input_dim)\n",
    "\n",
    "    def forward(self, cat, inputs, hidden):\n",
    "\n",
    "        #cat = [batch size, n_cats]\n",
    "\n",
    "        #inputs = [batch size]\n",
    "\n",
    "        emb = self.embedding(inputs)\n",
    "\n",
    "        #emb = [batch size, emb dim]\n",
    "\n",
    "        combined_input = torch.cat((cat, emb), 1)\n",
    "\n",
    "        #combined_input = [batch size, n_cats + emb dim]\n",
    "\n",
    "        combined_input = combined_input.unsqueeze(0)\n",
    "\n",
    "        # combined_input = [1, batch size, n_cats + emb dim]\n",
    "\n",
    "        output, hidden = self.rnn(combined_input, hidden)\n",
    "\n",
    "        #output = [1, batch size, hid dim * num directions dim]\n",
    "        #hidden = [n layers * num directions, batch size, hid dim]\n",
    "\n",
    "        output = self.fc(output)\n",
    "\n",
    "        #output = [1, batch size, input dim]\n",
    "\n",
    "        output = output.view(-1, self.input_dim)\n",
    "\n",
    "        #output = [batch size*1, input_dim] == [batch size, input dim]\n",
    "\n",
    "        return F.log_softmax(output, dim=1), hidden\n",
    "\n",
    "    def init_hidden(self, bsz):\n",
    "        weight = next(self.parameters())\n",
    "\n",
    "        return weight.new_zeros(self.n_layers, bsz, self.hid_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J_I-hIevAu3s"
   },
   "outputs": [],
   "source": [
    "embed_dim = 50\n",
    "hid_dim = 128\n",
    "n_layers = 1\n",
    "dropout = 0.1\n",
    "\n",
    "model = RNN(n_words, embed_dim, hid_dim, n_layers, dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 650,
     "status": "ok",
     "timestamp": 1591637183974,
     "user": {
      "displayName": "Daniel Cuiñas Vázquez",
      "photoUrl": "",
      "userId": "11552885780691803973"
     },
     "user_tz": -120
    },
    "id": "w5H3TGoXGwvh",
    "outputId": "c2a12b10-46e7-45c8-95ae-3ee5040b1468"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 86,593 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sRS7rfkKN8ly"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "au3gRWgYOAU2"
   },
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gcYjdB9tOCJm"
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.0003\n",
    "optimizer = optim.Adam(model.parameters(), learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TCa5BcuxOGrT"
   },
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    total_loss = 0\n",
    "    model.train()\n",
    "    \n",
    "    j = 0\n",
    "    \n",
    "    for categories, inputs, targets in iterator:\n",
    "        bsz = inputs.shape[0]\n",
    "        hidden = model.init_hidden(bsz)\n",
    "        hidden = hidden.to(device)\n",
    "        loss = 0\n",
    "        categories, inputs, targets = categories.to(device), inputs.to(device), targets.to(device)\n",
    "        \n",
    "        inputs = inputs.transpose(1,0)\n",
    "        targets = targets.transpose(1,0)\n",
    "\n",
    "        categories = categories.float()\n",
    "        inputs = inputs.long()\n",
    "        targets = targets.long()\n",
    "        \n",
    "        seq_len_batched = inputs.shape[0]\n",
    "\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        hidden = hidden.detach()\n",
    "      \n",
    "        for i in range(seq_len_batched):\n",
    "            output, hidden = model(categories, inputs[i], hidden)\n",
    "            loss += criterion(output, targets[i])\n",
    "            \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss = loss.item()/seq_len_batched\n",
    "        \n",
    "        total_loss += epoch_loss\n",
    "        \n",
    "        j += 1\n",
    "        \n",
    "        if j % log_interval == 0:\n",
    "            cur_loss = epoch_loss #/ log_interval\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | lr {} | '\n",
    "                    'loss {:5.2f} | ppl {:8.2f}'.format(\n",
    "                epoch, j, len(iterator), learning_rate,\n",
    "                cur_loss, math.exp(cur_loss)))\n",
    "            total_loss = 0\n",
    "            # example,_,_ = sample('practicante derecho,abogado')\n",
    "            # print(example)\n",
    "        \n",
    "    return total_loss / len(iterator)\n",
    "\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    total_loss = 0\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for categories, inputs, targets in iterator:\n",
    "            bsz = inputs.shape[0]\n",
    "            hidden = model.init_hidden(bsz)\n",
    "            hidden = hidden.to(device)\n",
    "            \n",
    "            loss = 0\n",
    "            \n",
    "            categories, inputs, targets = categories.to(device), inputs.to(device), targets.to(device)\n",
    "\n",
    "            inputs = inputs.transpose(1,0)\n",
    "            targets = targets.transpose(1,0)\n",
    "\n",
    "            categories = categories.float()\n",
    "            inputs = inputs.long()\n",
    "            targets = targets.long()\n",
    "\n",
    "            seq_len_batched = inputs.shape[0]\n",
    "            \n",
    "            for i in range(seq_len_batched):\n",
    "                output, hidden = model(categories, inputs[i], hidden)\n",
    "                loss += criterion(output, targets[i])\n",
    "            \n",
    "            epoch_loss = loss.item()/seq_len_batched \n",
    "            total_loss += epoch_loss\n",
    "        \n",
    "    return total_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9eAlzNlyO22D"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 341146,
     "status": "ok",
     "timestamp": 1591637538876,
     "user": {
      "displayName": "Daniel Cuiñas Vázquez",
      "photoUrl": "",
      "userId": "11552885780691803973"
     },
     "user_tz": -120
    },
    "id": "PUvCNigeO46Y",
    "outputId": "c57441b6-bd2b-4848-aa47-a6f96b12f7b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   0 |    25/  283 batches | lr 0.0003 | loss  3.61 | ppl    36.89\n",
      "| epoch   0 |    50/  283 batches | lr 0.0003 | loss  1.45 | ppl     4.25\n",
      "| epoch   0 |    75/  283 batches | lr 0.0003 | loss  1.31 | ppl     3.70\n",
      "| epoch   0 |   100/  283 batches | lr 0.0003 | loss  1.31 | ppl     3.71\n",
      "| epoch   0 |   125/  283 batches | lr 0.0003 | loss  1.15 | ppl     3.15\n",
      "| epoch   0 |   150/  283 batches | lr 0.0003 | loss  1.15 | ppl     3.15\n",
      "| epoch   0 |   175/  283 batches | lr 0.0003 | loss  1.06 | ppl     2.89\n",
      "| epoch   0 |   200/  283 batches | lr 0.0003 | loss  1.09 | ppl     2.98\n",
      "| epoch   0 |   225/  283 batches | lr 0.0003 | loss  1.00 | ppl     2.72\n",
      "| epoch   0 |   250/  283 batches | lr 0.0003 | loss  0.95 | ppl     2.59\n",
      "| epoch   0 |   275/  283 batches | lr 0.0003 | loss  1.02 | ppl     2.77\n",
      "Epoch: 01 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 0.027 | Val. Loss: 0.948\n",
      "| epoch   1 |    25/  283 batches | lr 0.0003 | loss  0.98 | ppl     2.67\n",
      "| epoch   1 |    50/  283 batches | lr 0.0003 | loss  0.97 | ppl     2.64\n",
      "| epoch   1 |    75/  283 batches | lr 0.0003 | loss  0.90 | ppl     2.46\n",
      "| epoch   1 |   100/  283 batches | lr 0.0003 | loss  0.88 | ppl     2.42\n",
      "| epoch   1 |   125/  283 batches | lr 0.0003 | loss  0.88 | ppl     2.42\n",
      "| epoch   1 |   150/  283 batches | lr 0.0003 | loss  0.82 | ppl     2.28\n",
      "| epoch   1 |   175/  283 batches | lr 0.0003 | loss  0.82 | ppl     2.27\n",
      "| epoch   1 |   200/  283 batches | lr 0.0003 | loss  0.79 | ppl     2.20\n",
      "| epoch   1 |   225/  283 batches | lr 0.0003 | loss  0.78 | ppl     2.19\n",
      "| epoch   1 |   250/  283 batches | lr 0.0003 | loss  0.83 | ppl     2.30\n",
      "| epoch   1 |   275/  283 batches | lr 0.0003 | loss  0.85 | ppl     2.34\n",
      "Epoch: 02 | Epoch Time: 0m 6s\n",
      "\tTrain Loss: 0.023 | Val. Loss: 0.828\n",
      "| epoch   2 |    25/  283 batches | lr 0.0003 | loss  0.84 | ppl     2.32\n",
      "| epoch   2 |    50/  283 batches | lr 0.0003 | loss  0.77 | ppl     2.17\n",
      "| epoch   2 |    75/  283 batches | lr 0.0003 | loss  0.83 | ppl     2.30\n",
      "| epoch   2 |   100/  283 batches | lr 0.0003 | loss  0.78 | ppl     2.17\n",
      "| epoch   2 |   125/  283 batches | lr 0.0003 | loss  0.83 | ppl     2.29\n",
      "| epoch   2 |   150/  283 batches | lr 0.0003 | loss  0.79 | ppl     2.21\n",
      "| epoch   2 |   175/  283 batches | lr 0.0003 | loss  0.77 | ppl     2.16\n",
      "| epoch   2 |   200/  283 batches | lr 0.0003 | loss  0.74 | ppl     2.10\n",
      "| epoch   2 |   225/  283 batches | lr 0.0003 | loss  0.76 | ppl     2.13\n",
      "| epoch   2 |   250/  283 batches | lr 0.0003 | loss  0.72 | ppl     2.06\n",
      "| epoch   2 |   275/  283 batches | lr 0.0003 | loss  0.80 | ppl     2.22\n",
      "Epoch: 03 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 0.021 | Val. Loss: 0.780\n",
      "| epoch   3 |    25/  283 batches | lr 0.0003 | loss  0.82 | ppl     2.27\n",
      "| epoch   3 |    50/  283 batches | lr 0.0003 | loss  0.73 | ppl     2.08\n",
      "| epoch   3 |    75/  283 batches | lr 0.0003 | loss  0.72 | ppl     2.06\n",
      "| epoch   3 |   100/  283 batches | lr 0.0003 | loss  0.76 | ppl     2.15\n",
      "| epoch   3 |   125/  283 batches | lr 0.0003 | loss  0.76 | ppl     2.14\n",
      "| epoch   3 |   150/  283 batches | lr 0.0003 | loss  0.69 | ppl     1.98\n",
      "| epoch   3 |   175/  283 batches | lr 0.0003 | loss  0.74 | ppl     2.10\n",
      "| epoch   3 |   200/  283 batches | lr 0.0003 | loss  0.74 | ppl     2.11\n",
      "| epoch   3 |   225/  283 batches | lr 0.0003 | loss  0.76 | ppl     2.15\n",
      "| epoch   3 |   250/  283 batches | lr 0.0003 | loss  0.73 | ppl     2.08\n",
      "| epoch   3 |   275/  283 batches | lr 0.0003 | loss  0.77 | ppl     2.15\n",
      "Epoch: 04 | Epoch Time: 0m 6s\n",
      "\tTrain Loss: 0.021 | Val. Loss: 0.749\n",
      "| epoch   4 |    25/  283 batches | lr 0.0003 | loss  0.75 | ppl     2.11\n",
      "| epoch   4 |    50/  283 batches | lr 0.0003 | loss  0.68 | ppl     1.98\n",
      "| epoch   4 |    75/  283 batches | lr 0.0003 | loss  0.69 | ppl     2.00\n",
      "| epoch   4 |   100/  283 batches | lr 0.0003 | loss  0.71 | ppl     2.03\n",
      "| epoch   4 |   125/  283 batches | lr 0.0003 | loss  0.74 | ppl     2.10\n",
      "| epoch   4 |   150/  283 batches | lr 0.0003 | loss  0.68 | ppl     1.97\n",
      "| epoch   4 |   175/  283 batches | lr 0.0003 | loss  0.74 | ppl     2.10\n",
      "| epoch   4 |   200/  283 batches | lr 0.0003 | loss  0.67 | ppl     1.95\n",
      "| epoch   4 |   225/  283 batches | lr 0.0003 | loss  0.74 | ppl     2.10\n",
      "| epoch   4 |   250/  283 batches | lr 0.0003 | loss  0.74 | ppl     2.09\n",
      "| epoch   4 |   275/  283 batches | lr 0.0003 | loss  0.76 | ppl     2.15\n",
      "Epoch: 05 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 0.020 | Val. Loss: 0.726\n",
      "| epoch   5 |    25/  283 batches | lr 0.0003 | loss  0.74 | ppl     2.09\n",
      "| epoch   5 |    50/  283 batches | lr 0.0003 | loss  0.71 | ppl     2.03\n",
      "| epoch   5 |    75/  283 batches | lr 0.0003 | loss  0.72 | ppl     2.05\n",
      "| epoch   5 |   100/  283 batches | lr 0.0003 | loss  0.79 | ppl     2.20\n",
      "| epoch   5 |   125/  283 batches | lr 0.0003 | loss  0.74 | ppl     2.09\n",
      "| epoch   5 |   150/  283 batches | lr 0.0003 | loss  0.72 | ppl     2.06\n",
      "| epoch   5 |   175/  283 batches | lr 0.0003 | loss  0.74 | ppl     2.10\n",
      "| epoch   5 |   200/  283 batches | lr 0.0003 | loss  0.73 | ppl     2.07\n",
      "| epoch   5 |   225/  283 batches | lr 0.0003 | loss  0.71 | ppl     2.03\n",
      "| epoch   5 |   250/  283 batches | lr 0.0003 | loss  0.73 | ppl     2.07\n",
      "| epoch   5 |   275/  283 batches | lr 0.0003 | loss  0.78 | ppl     2.18\n",
      "Epoch: 06 | Epoch Time: 0m 6s\n",
      "\tTrain Loss: 0.019 | Val. Loss: 0.709\n",
      "| epoch   6 |    25/  283 batches | lr 0.0003 | loss  0.73 | ppl     2.07\n",
      "| epoch   6 |    50/  283 batches | lr 0.0003 | loss  0.67 | ppl     1.95\n",
      "| epoch   6 |    75/  283 batches | lr 0.0003 | loss  0.70 | ppl     2.01\n",
      "| epoch   6 |   100/  283 batches | lr 0.0003 | loss  0.68 | ppl     1.98\n",
      "| epoch   6 |   125/  283 batches | lr 0.0003 | loss  0.70 | ppl     2.02\n",
      "| epoch   6 |   150/  283 batches | lr 0.0003 | loss  0.71 | ppl     2.03\n",
      "| epoch   6 |   175/  283 batches | lr 0.0003 | loss  0.72 | ppl     2.05\n",
      "| epoch   6 |   200/  283 batches | lr 0.0003 | loss  0.66 | ppl     1.93\n",
      "| epoch   6 |   225/  283 batches | lr 0.0003 | loss  0.64 | ppl     1.91\n",
      "| epoch   6 |   250/  283 batches | lr 0.0003 | loss  0.73 | ppl     2.07\n",
      "| epoch   6 |   275/  283 batches | lr 0.0003 | loss  0.70 | ppl     2.01\n",
      "Epoch: 07 | Epoch Time: 0m 6s\n",
      "\tTrain Loss: 0.019 | Val. Loss: 0.696\n",
      "| epoch   7 |    25/  283 batches | lr 0.0003 | loss  0.67 | ppl     1.96\n",
      "| epoch   7 |    50/  283 batches | lr 0.0003 | loss  0.73 | ppl     2.07\n",
      "| epoch   7 |    75/  283 batches | lr 0.0003 | loss  0.67 | ppl     1.96\n",
      "| epoch   7 |   100/  283 batches | lr 0.0003 | loss  0.70 | ppl     2.02\n",
      "| epoch   7 |   125/  283 batches | lr 0.0003 | loss  0.72 | ppl     2.05\n",
      "| epoch   7 |   150/  283 batches | lr 0.0003 | loss  0.66 | ppl     1.93\n",
      "| epoch   7 |   175/  283 batches | lr 0.0003 | loss  0.71 | ppl     2.03\n",
      "| epoch   7 |   200/  283 batches | lr 0.0003 | loss  0.76 | ppl     2.14\n",
      "| epoch   7 |   225/  283 batches | lr 0.0003 | loss  0.71 | ppl     2.03\n",
      "| epoch   7 |   250/  283 batches | lr 0.0003 | loss  0.69 | ppl     2.00\n",
      "| epoch   7 |   275/  283 batches | lr 0.0003 | loss  0.65 | ppl     1.92\n",
      "Epoch: 08 | Epoch Time: 0m 6s\n",
      "\tTrain Loss: 0.019 | Val. Loss: 0.684\n",
      "| epoch   8 |    25/  283 batches | lr 0.0003 | loss  0.68 | ppl     1.97\n",
      "| epoch   8 |    50/  283 batches | lr 0.0003 | loss  0.74 | ppl     2.09\n",
      "| epoch   8 |    75/  283 batches | lr 0.0003 | loss  0.69 | ppl     2.00\n",
      "| epoch   8 |   100/  283 batches | lr 0.0003 | loss  0.71 | ppl     2.02\n",
      "| epoch   8 |   125/  283 batches | lr 0.0003 | loss  0.73 | ppl     2.07\n",
      "| epoch   8 |   150/  283 batches | lr 0.0003 | loss  0.65 | ppl     1.93\n",
      "| epoch   8 |   175/  283 batches | lr 0.0003 | loss  0.68 | ppl     1.97\n",
      "| epoch   8 |   200/  283 batches | lr 0.0003 | loss  0.67 | ppl     1.96\n",
      "| epoch   8 |   225/  283 batches | lr 0.0003 | loss  0.72 | ppl     2.05\n",
      "| epoch   8 |   250/  283 batches | lr 0.0003 | loss  0.63 | ppl     1.87\n",
      "| epoch   8 |   275/  283 batches | lr 0.0003 | loss  0.65 | ppl     1.91\n",
      "Epoch: 09 | Epoch Time: 0m 6s\n",
      "\tTrain Loss: 0.019 | Val. Loss: 0.675\n",
      "| epoch   9 |    25/  283 batches | lr 0.0003 | loss  0.60 | ppl     1.82\n",
      "| epoch   9 |    50/  283 batches | lr 0.0003 | loss  0.68 | ppl     1.98\n",
      "| epoch   9 |    75/  283 batches | lr 0.0003 | loss  0.69 | ppl     2.00\n",
      "| epoch   9 |   100/  283 batches | lr 0.0003 | loss  0.69 | ppl     1.99\n",
      "| epoch   9 |   125/  283 batches | lr 0.0003 | loss  0.69 | ppl     2.00\n",
      "| epoch   9 |   150/  283 batches | lr 0.0003 | loss  0.63 | ppl     1.88\n",
      "| epoch   9 |   175/  283 batches | lr 0.0003 | loss  0.68 | ppl     1.97\n",
      "| epoch   9 |   200/  283 batches | lr 0.0003 | loss  0.64 | ppl     1.89\n",
      "| epoch   9 |   225/  283 batches | lr 0.0003 | loss  0.62 | ppl     1.86\n",
      "| epoch   9 |   250/  283 batches | lr 0.0003 | loss  0.66 | ppl     1.94\n",
      "| epoch   9 |   275/  283 batches | lr 0.0003 | loss  0.65 | ppl     1.92\n",
      "Epoch: 10 | Epoch Time: 0m 6s\n",
      "\tTrain Loss: 0.018 | Val. Loss: 0.665\n",
      "| epoch  10 |    25/  283 batches | lr 0.0003 | loss  0.64 | ppl     1.89\n",
      "| epoch  10 |    50/  283 batches | lr 0.0003 | loss  0.63 | ppl     1.88\n",
      "| epoch  10 |    75/  283 batches | lr 0.0003 | loss  0.66 | ppl     1.93\n",
      "| epoch  10 |   100/  283 batches | lr 0.0003 | loss  0.67 | ppl     1.96\n",
      "| epoch  10 |   125/  283 batches | lr 0.0003 | loss  0.63 | ppl     1.88\n",
      "| epoch  10 |   150/  283 batches | lr 0.0003 | loss  0.65 | ppl     1.91\n",
      "| epoch  10 |   175/  283 batches | lr 0.0003 | loss  0.70 | ppl     2.02\n",
      "| epoch  10 |   200/  283 batches | lr 0.0003 | loss  0.68 | ppl     1.98\n",
      "| epoch  10 |   225/  283 batches | lr 0.0003 | loss  0.68 | ppl     1.97\n",
      "| epoch  10 |   250/  283 batches | lr 0.0003 | loss  0.68 | ppl     1.98\n",
      "| epoch  10 |   275/  283 batches | lr 0.0003 | loss  0.69 | ppl     2.00\n",
      "Epoch: 11 | Epoch Time: 0m 6s\n",
      "\tTrain Loss: 0.018 | Val. Loss: 0.657\n",
      "| epoch  11 |    25/  283 batches | lr 0.0003 | loss  0.64 | ppl     1.90\n",
      "| epoch  11 |    50/  283 batches | lr 0.0003 | loss  0.59 | ppl     1.80\n",
      "| epoch  11 |    75/  283 batches | lr 0.0003 | loss  0.61 | ppl     1.84\n",
      "| epoch  11 |   100/  283 batches | lr 0.0003 | loss  0.68 | ppl     1.97\n",
      "| epoch  11 |   125/  283 batches | lr 0.0003 | loss  0.66 | ppl     1.94\n",
      "| epoch  11 |   150/  283 batches | lr 0.0003 | loss  0.67 | ppl     1.95\n",
      "| epoch  11 |   175/  283 batches | lr 0.0003 | loss  0.64 | ppl     1.89\n",
      "| epoch  11 |   200/  283 batches | lr 0.0003 | loss  0.66 | ppl     1.93\n",
      "| epoch  11 |   225/  283 batches | lr 0.0003 | loss  0.63 | ppl     1.88\n",
      "| epoch  11 |   250/  283 batches | lr 0.0003 | loss  0.65 | ppl     1.91\n",
      "| epoch  11 |   275/  283 batches | lr 0.0003 | loss  0.62 | ppl     1.87\n",
      "Epoch: 12 | Epoch Time: 0m 6s\n",
      "\tTrain Loss: 0.018 | Val. Loss: 0.650\n",
      "| epoch  12 |    25/  283 batches | lr 0.0003 | loss  0.61 | ppl     1.84\n",
      "| epoch  12 |    50/  283 batches | lr 0.0003 | loss  0.62 | ppl     1.86\n",
      "| epoch  12 |    75/  283 batches | lr 0.0003 | loss  0.66 | ppl     1.94\n",
      "| epoch  12 |   100/  283 batches | lr 0.0003 | loss  0.64 | ppl     1.89\n",
      "| epoch  12 |   125/  283 batches | lr 0.0003 | loss  0.60 | ppl     1.82\n",
      "| epoch  12 |   150/  283 batches | lr 0.0003 | loss  0.59 | ppl     1.81\n",
      "| epoch  12 |   175/  283 batches | lr 0.0003 | loss  0.64 | ppl     1.90\n",
      "| epoch  12 |   200/  283 batches | lr 0.0003 | loss  0.59 | ppl     1.80\n",
      "| epoch  12 |   225/  283 batches | lr 0.0003 | loss  0.57 | ppl     1.77\n",
      "| epoch  12 |   250/  283 batches | lr 0.0003 | loss  0.63 | ppl     1.87\n",
      "| epoch  12 |   275/  283 batches | lr 0.0003 | loss  0.62 | ppl     1.85\n",
      "Epoch: 13 | Epoch Time: 0m 6s\n",
      "\tTrain Loss: 0.017 | Val. Loss: 0.643\n",
      "| epoch  13 |    25/  283 batches | lr 0.0003 | loss  0.65 | ppl     1.91\n",
      "| epoch  13 |    50/  283 batches | lr 0.0003 | loss  0.65 | ppl     1.92\n",
      "| epoch  13 |    75/  283 batches | lr 0.0003 | loss  0.63 | ppl     1.88\n",
      "| epoch  13 |   100/  283 batches | lr 0.0003 | loss  0.65 | ppl     1.92\n",
      "| epoch  13 |   125/  283 batches | lr 0.0003 | loss  0.56 | ppl     1.76\n",
      "| epoch  13 |   150/  283 batches | lr 0.0003 | loss  0.63 | ppl     1.87\n",
      "| epoch  13 |   175/  283 batches | lr 0.0003 | loss  0.58 | ppl     1.78\n",
      "| epoch  13 |   200/  283 batches | lr 0.0003 | loss  0.60 | ppl     1.82\n",
      "| epoch  13 |   225/  283 batches | lr 0.0003 | loss  0.63 | ppl     1.87\n",
      "| epoch  13 |   250/  283 batches | lr 0.0003 | loss  0.65 | ppl     1.91\n",
      "| epoch  13 |   275/  283 batches | lr 0.0003 | loss  0.65 | ppl     1.92\n",
      "Epoch: 14 | Epoch Time: 0m 6s\n",
      "\tTrain Loss: 0.017 | Val. Loss: 0.636\n",
      "| epoch  14 |    25/  283 batches | lr 0.0003 | loss  0.62 | ppl     1.85\n",
      "| epoch  14 |    50/  283 batches | lr 0.0003 | loss  0.61 | ppl     1.83\n",
      "| epoch  14 |    75/  283 batches | lr 0.0003 | loss  0.65 | ppl     1.92\n",
      "| epoch  14 |   100/  283 batches | lr 0.0003 | loss  0.61 | ppl     1.84\n",
      "| epoch  14 |   125/  283 batches | lr 0.0003 | loss  0.61 | ppl     1.83\n",
      "| epoch  14 |   150/  283 batches | lr 0.0003 | loss  0.63 | ppl     1.88\n",
      "| epoch  14 |   175/  283 batches | lr 0.0003 | loss  0.58 | ppl     1.79\n",
      "| epoch  14 |   200/  283 batches | lr 0.0003 | loss  0.64 | ppl     1.90\n",
      "| epoch  14 |   225/  283 batches | lr 0.0003 | loss  0.64 | ppl     1.90\n",
      "| epoch  14 |   250/  283 batches | lr 0.0003 | loss  0.58 | ppl     1.78\n",
      "| epoch  14 |   275/  283 batches | lr 0.0003 | loss  0.64 | ppl     1.89\n",
      "Epoch: 15 | Epoch Time: 0m 6s\n",
      "\tTrain Loss: 0.018 | Val. Loss: 0.631\n",
      "| epoch  15 |    25/  283 batches | lr 0.0003 | loss  0.65 | ppl     1.91\n",
      "| epoch  15 |    50/  283 batches | lr 0.0003 | loss  0.56 | ppl     1.76\n",
      "| epoch  15 |    75/  283 batches | lr 0.0003 | loss  0.66 | ppl     1.93\n",
      "| epoch  15 |   100/  283 batches | lr 0.0003 | loss  0.62 | ppl     1.85\n",
      "| epoch  15 |   125/  283 batches | lr 0.0003 | loss  0.60 | ppl     1.82\n",
      "| epoch  15 |   150/  283 batches | lr 0.0003 | loss  0.57 | ppl     1.76\n",
      "| epoch  15 |   175/  283 batches | lr 0.0003 | loss  0.61 | ppl     1.83\n",
      "| epoch  15 |   200/  283 batches | lr 0.0003 | loss  0.63 | ppl     1.89\n",
      "| epoch  15 |   225/  283 batches | lr 0.0003 | loss  0.61 | ppl     1.83\n",
      "| epoch  15 |   250/  283 batches | lr 0.0003 | loss  0.63 | ppl     1.88\n",
      "| epoch  15 |   275/  283 batches | lr 0.0003 | loss  0.64 | ppl     1.89\n",
      "Epoch: 16 | Epoch Time: 0m 6s\n",
      "\tTrain Loss: 0.018 | Val. Loss: 0.625\n",
      "| epoch  16 |    25/  283 batches | lr 0.0003 | loss  0.61 | ppl     1.84\n",
      "| epoch  16 |    50/  283 batches | lr 0.0003 | loss  0.58 | ppl     1.79\n",
      "| epoch  16 |    75/  283 batches | lr 0.0003 | loss  0.62 | ppl     1.87\n",
      "| epoch  16 |   100/  283 batches | lr 0.0003 | loss  0.65 | ppl     1.92\n",
      "| epoch  16 |   125/  283 batches | lr 0.0003 | loss  0.64 | ppl     1.90\n",
      "| epoch  16 |   150/  283 batches | lr 0.0003 | loss  0.62 | ppl     1.85\n",
      "| epoch  16 |   175/  283 batches | lr 0.0003 | loss  0.62 | ppl     1.86\n",
      "| epoch  16 |   200/  283 batches | lr 0.0003 | loss  0.62 | ppl     1.86\n",
      "| epoch  16 |   225/  283 batches | lr 0.0003 | loss  0.61 | ppl     1.85\n",
      "| epoch  16 |   250/  283 batches | lr 0.0003 | loss  0.60 | ppl     1.81\n",
      "| epoch  16 |   275/  283 batches | lr 0.0003 | loss  0.62 | ppl     1.87\n",
      "Epoch: 17 | Epoch Time: 0m 6s\n",
      "\tTrain Loss: 0.017 | Val. Loss: 0.621\n",
      "| epoch  17 |    25/  283 batches | lr 0.0003 | loss  0.62 | ppl     1.86\n",
      "| epoch  17 |    50/  283 batches | lr 0.0003 | loss  0.61 | ppl     1.85\n",
      "| epoch  17 |    75/  283 batches | lr 0.0003 | loss  0.58 | ppl     1.79\n",
      "| epoch  17 |   100/  283 batches | lr 0.0003 | loss  0.57 | ppl     1.78\n",
      "| epoch  17 |   125/  283 batches | lr 0.0003 | loss  0.59 | ppl     1.80\n",
      "| epoch  17 |   150/  283 batches | lr 0.0003 | loss  0.59 | ppl     1.81\n",
      "| epoch  17 |   175/  283 batches | lr 0.0003 | loss  0.57 | ppl     1.77\n",
      "| epoch  17 |   200/  283 batches | lr 0.0003 | loss  0.51 | ppl     1.67\n",
      "| epoch  17 |   225/  283 batches | lr 0.0003 | loss  0.59 | ppl     1.81\n",
      "| epoch  17 |   250/  283 batches | lr 0.0003 | loss  0.56 | ppl     1.76\n",
      "| epoch  17 |   275/  283 batches | lr 0.0003 | loss  0.59 | ppl     1.80\n",
      "Epoch: 18 | Epoch Time: 0m 6s\n",
      "\tTrain Loss: 0.017 | Val. Loss: 0.616\n",
      "| epoch  18 |    25/  283 batches | lr 0.0003 | loss  0.55 | ppl     1.74\n",
      "| epoch  18 |    50/  283 batches | lr 0.0003 | loss  0.60 | ppl     1.82\n",
      "| epoch  18 |    75/  283 batches | lr 0.0003 | loss  0.57 | ppl     1.76\n",
      "| epoch  18 |   100/  283 batches | lr 0.0003 | loss  0.59 | ppl     1.80\n",
      "| epoch  18 |   125/  283 batches | lr 0.0003 | loss  0.60 | ppl     1.81\n",
      "| epoch  18 |   150/  283 batches | lr 0.0003 | loss  0.59 | ppl     1.80\n",
      "| epoch  18 |   175/  283 batches | lr 0.0003 | loss  0.63 | ppl     1.88\n",
      "| epoch  18 |   200/  283 batches | lr 0.0003 | loss  0.59 | ppl     1.80\n",
      "| epoch  18 |   225/  283 batches | lr 0.0003 | loss  0.58 | ppl     1.79\n",
      "| epoch  18 |   250/  283 batches | lr 0.0003 | loss  0.60 | ppl     1.82\n",
      "| epoch  18 |   275/  283 batches | lr 0.0003 | loss  0.57 | ppl     1.76\n",
      "Epoch: 19 | Epoch Time: 0m 6s\n",
      "\tTrain Loss: 0.017 | Val. Loss: 0.613\n",
      "| epoch  19 |    25/  283 batches | lr 0.0003 | loss  0.60 | ppl     1.82\n",
      "| epoch  19 |    50/  283 batches | lr 0.0003 | loss  0.58 | ppl     1.78\n",
      "| epoch  19 |    75/  283 batches | lr 0.0003 | loss  0.58 | ppl     1.78\n",
      "| epoch  19 |   100/  283 batches | lr 0.0003 | loss  0.60 | ppl     1.82\n",
      "| epoch  19 |   125/  283 batches | lr 0.0003 | loss  0.61 | ppl     1.85\n",
      "| epoch  19 |   150/  283 batches | lr 0.0003 | loss  0.59 | ppl     1.81\n",
      "| epoch  19 |   175/  283 batches | lr 0.0003 | loss  0.56 | ppl     1.75\n",
      "| epoch  19 |   200/  283 batches | lr 0.0003 | loss  0.57 | ppl     1.77\n",
      "| epoch  19 |   225/  283 batches | lr 0.0003 | loss  0.59 | ppl     1.81\n",
      "| epoch  19 |   250/  283 batches | lr 0.0003 | loss  0.57 | ppl     1.77\n",
      "| epoch  19 |   275/  283 batches | lr 0.0003 | loss  0.57 | ppl     1.76\n",
      "Epoch: 20 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 0.016 | Val. Loss: 0.608\n",
      "| epoch  20 |    25/  283 batches | lr 0.0003 | loss  0.57 | ppl     1.76\n",
      "| epoch  20 |    50/  283 batches | lr 0.0003 | loss  0.57 | ppl     1.77\n",
      "| epoch  20 |    75/  283 batches | lr 0.0003 | loss  0.55 | ppl     1.73\n",
      "| epoch  20 |   100/  283 batches | lr 0.0003 | loss  0.58 | ppl     1.78\n",
      "| epoch  20 |   125/  283 batches | lr 0.0003 | loss  0.57 | ppl     1.76\n",
      "| epoch  20 |   150/  283 batches | lr 0.0003 | loss  0.58 | ppl     1.79\n",
      "| epoch  20 |   175/  283 batches | lr 0.0003 | loss  0.60 | ppl     1.82\n",
      "| epoch  20 |   200/  283 batches | lr 0.0003 | loss  0.57 | ppl     1.76\n",
      "| epoch  20 |   225/  283 batches | lr 0.0003 | loss  0.53 | ppl     1.70\n",
      "| epoch  20 |   250/  283 batches | lr 0.0003 | loss  0.60 | ppl     1.83\n",
      "| epoch  20 |   275/  283 batches | lr 0.0003 | loss  0.57 | ppl     1.78\n",
      "Epoch: 21 | Epoch Time: 0m 6s\n",
      "\tTrain Loss: 0.017 | Val. Loss: 0.606\n",
      "| epoch  21 |    25/  283 batches | lr 0.0003 | loss  0.57 | ppl     1.77\n",
      "| epoch  21 |    50/  283 batches | lr 0.0003 | loss  0.58 | ppl     1.79\n",
      "| epoch  21 |    75/  283 batches | lr 0.0003 | loss  0.61 | ppl     1.84\n",
      "| epoch  21 |   100/  283 batches | lr 0.0003 | loss  0.56 | ppl     1.76\n",
      "| epoch  21 |   125/  283 batches | lr 0.0003 | loss  0.59 | ppl     1.81\n",
      "| epoch  21 |   150/  283 batches | lr 0.0003 | loss  0.61 | ppl     1.84\n",
      "| epoch  21 |   175/  283 batches | lr 0.0003 | loss  0.64 | ppl     1.89\n",
      "| epoch  21 |   200/  283 batches | lr 0.0003 | loss  0.57 | ppl     1.76\n",
      "| epoch  21 |   225/  283 batches | lr 0.0003 | loss  0.58 | ppl     1.79\n",
      "| epoch  21 |   250/  283 batches | lr 0.0003 | loss  0.58 | ppl     1.79\n",
      "| epoch  21 |   275/  283 batches | lr 0.0003 | loss  0.52 | ppl     1.69\n",
      "Epoch: 22 | Epoch Time: 0m 6s\n",
      "\tTrain Loss: 0.017 | Val. Loss: 0.602\n",
      "| epoch  22 |    25/  283 batches | lr 0.0003 | loss  0.61 | ppl     1.83\n",
      "| epoch  22 |    50/  283 batches | lr 0.0003 | loss  0.61 | ppl     1.84\n",
      "| epoch  22 |    75/  283 batches | lr 0.0003 | loss  0.53 | ppl     1.69\n",
      "| epoch  22 |   100/  283 batches | lr 0.0003 | loss  0.59 | ppl     1.81\n",
      "| epoch  22 |   125/  283 batches | lr 0.0003 | loss  0.61 | ppl     1.83\n",
      "| epoch  22 |   150/  283 batches | lr 0.0003 | loss  0.57 | ppl     1.78\n",
      "| epoch  22 |   175/  283 batches | lr 0.0003 | loss  0.61 | ppl     1.84\n",
      "| epoch  22 |   200/  283 batches | lr 0.0003 | loss  0.54 | ppl     1.71\n",
      "| epoch  22 |   225/  283 batches | lr 0.0003 | loss  0.56 | ppl     1.75\n",
      "| epoch  22 |   250/  283 batches | lr 0.0003 | loss  0.57 | ppl     1.77\n",
      "| epoch  22 |   275/  283 batches | lr 0.0003 | loss  0.60 | ppl     1.83\n",
      "Epoch: 23 | Epoch Time: 0m 6s\n",
      "\tTrain Loss: 0.016 | Val. Loss: 0.599\n",
      "| epoch  23 |    25/  283 batches | lr 0.0003 | loss  0.60 | ppl     1.82\n",
      "| epoch  23 |    50/  283 batches | lr 0.0003 | loss  0.59 | ppl     1.80\n",
      "| epoch  23 |    75/  283 batches | lr 0.0003 | loss  0.60 | ppl     1.82\n",
      "| epoch  23 |   100/  283 batches | lr 0.0003 | loss  0.57 | ppl     1.77\n",
      "| epoch  23 |   125/  283 batches | lr 0.0003 | loss  0.57 | ppl     1.77\n",
      "| epoch  23 |   150/  283 batches | lr 0.0003 | loss  0.61 | ppl     1.84\n",
      "| epoch  23 |   175/  283 batches | lr 0.0003 | loss  0.54 | ppl     1.72\n",
      "| epoch  23 |   200/  283 batches | lr 0.0003 | loss  0.59 | ppl     1.80\n",
      "| epoch  23 |   225/  283 batches | lr 0.0003 | loss  0.59 | ppl     1.80\n",
      "| epoch  23 |   250/  283 batches | lr 0.0003 | loss  0.54 | ppl     1.71\n",
      "| epoch  23 |   275/  283 batches | lr 0.0003 | loss  0.59 | ppl     1.81\n",
      "Epoch: 24 | Epoch Time: 0m 6s\n",
      "\tTrain Loss: 0.016 | Val. Loss: 0.594\n",
      "| epoch  24 |    25/  283 batches | lr 0.0003 | loss  0.56 | ppl     1.76\n",
      "| epoch  24 |    50/  283 batches | lr 0.0003 | loss  0.55 | ppl     1.74\n",
      "| epoch  24 |    75/  283 batches | lr 0.0003 | loss  0.56 | ppl     1.75\n",
      "| epoch  24 |   100/  283 batches | lr 0.0003 | loss  0.61 | ppl     1.84\n",
      "| epoch  24 |   125/  283 batches | lr 0.0003 | loss  0.53 | ppl     1.70\n",
      "| epoch  24 |   150/  283 batches | lr 0.0003 | loss  0.55 | ppl     1.73\n",
      "| epoch  24 |   175/  283 batches | lr 0.0003 | loss  0.58 | ppl     1.78\n",
      "| epoch  24 |   200/  283 batches | lr 0.0003 | loss  0.55 | ppl     1.73\n",
      "| epoch  24 |   225/  283 batches | lr 0.0003 | loss  0.54 | ppl     1.72\n",
      "| epoch  24 |   250/  283 batches | lr 0.0003 | loss  0.62 | ppl     1.86\n",
      "| epoch  24 |   275/  283 batches | lr 0.0003 | loss  0.50 | ppl     1.65\n",
      "Epoch: 25 | Epoch Time: 0m 6s\n",
      "\tTrain Loss: 0.016 | Val. Loss: 0.592\n",
      "| epoch  25 |    25/  283 batches | lr 0.0003 | loss  0.57 | ppl     1.78\n",
      "| epoch  25 |    50/  283 batches | lr 0.0003 | loss  0.52 | ppl     1.68\n",
      "| epoch  25 |    75/  283 batches | lr 0.0003 | loss  0.57 | ppl     1.77\n",
      "| epoch  25 |   100/  283 batches | lr 0.0003 | loss  0.59 | ppl     1.80\n",
      "| epoch  25 |   125/  283 batches | lr 0.0003 | loss  0.60 | ppl     1.82\n",
      "| epoch  25 |   150/  283 batches | lr 0.0003 | loss  0.55 | ppl     1.73\n",
      "| epoch  25 |   175/  283 batches | lr 0.0003 | loss  0.56 | ppl     1.76\n",
      "| epoch  25 |   200/  283 batches | lr 0.0003 | loss  0.56 | ppl     1.75\n",
      "| epoch  25 |   225/  283 batches | lr 0.0003 | loss  0.51 | ppl     1.67\n",
      "| epoch  25 |   250/  283 batches | lr 0.0003 | loss  0.58 | ppl     1.79\n",
      "| epoch  25 |   275/  283 batches | lr 0.0003 | loss  0.56 | ppl     1.75\n",
      "Epoch: 26 | Epoch Time: 0m 6s\n",
      "\tTrain Loss: 0.016 | Val. Loss: 0.588\n",
      "| epoch  26 |    25/  283 batches | lr 0.0003 | loss  0.52 | ppl     1.68\n",
      "| epoch  26 |    50/  283 batches | lr 0.0003 | loss  0.52 | ppl     1.68\n",
      "| epoch  26 |    75/  283 batches | lr 0.0003 | loss  0.55 | ppl     1.74\n",
      "| epoch  26 |   100/  283 batches | lr 0.0003 | loss  0.62 | ppl     1.86\n",
      "| epoch  26 |   125/  283 batches | lr 0.0003 | loss  0.61 | ppl     1.85\n",
      "| epoch  26 |   150/  283 batches | lr 0.0003 | loss  0.56 | ppl     1.75\n",
      "| epoch  26 |   175/  283 batches | lr 0.0003 | loss  0.57 | ppl     1.76\n",
      "| epoch  26 |   200/  283 batches | lr 0.0003 | loss  0.54 | ppl     1.71\n",
      "| epoch  26 |   225/  283 batches | lr 0.0003 | loss  0.56 | ppl     1.75\n",
      "| epoch  26 |   250/  283 batches | lr 0.0003 | loss  0.56 | ppl     1.76\n",
      "| epoch  26 |   275/  283 batches | lr 0.0003 | loss  0.59 | ppl     1.81\n",
      "Epoch: 27 | Epoch Time: 0m 6s\n",
      "\tTrain Loss: 0.016 | Val. Loss: 0.588\n",
      "| epoch  27 |    25/  283 batches | lr 0.0003 | loss  0.53 | ppl     1.70\n",
      "| epoch  27 |    50/  283 batches | lr 0.0003 | loss  0.52 | ppl     1.69\n",
      "| epoch  27 |    75/  283 batches | lr 0.0003 | loss  0.57 | ppl     1.77\n",
      "| epoch  27 |   100/  283 batches | lr 0.0003 | loss  0.55 | ppl     1.73\n",
      "| epoch  27 |   125/  283 batches | lr 0.0003 | loss  0.49 | ppl     1.64\n",
      "| epoch  27 |   150/  283 batches | lr 0.0003 | loss  0.56 | ppl     1.75\n",
      "| epoch  27 |   175/  283 batches | lr 0.0003 | loss  0.54 | ppl     1.71\n",
      "| epoch  27 |   200/  283 batches | lr 0.0003 | loss  0.52 | ppl     1.69\n",
      "| epoch  27 |   225/  283 batches | lr 0.0003 | loss  0.58 | ppl     1.79\n",
      "| epoch  27 |   250/  283 batches | lr 0.0003 | loss  0.56 | ppl     1.75\n",
      "| epoch  27 |   275/  283 batches | lr 0.0003 | loss  0.56 | ppl     1.75\n",
      "Epoch: 28 | Epoch Time: 0m 6s\n",
      "\tTrain Loss: 0.015 | Val. Loss: 0.585\n",
      "| epoch  28 |    25/  283 batches | lr 0.0003 | loss  0.60 | ppl     1.82\n",
      "| epoch  28 |    50/  283 batches | lr 0.0003 | loss  0.56 | ppl     1.76\n",
      "| epoch  28 |    75/  283 batches | lr 0.0003 | loss  0.56 | ppl     1.75\n",
      "| epoch  28 |   100/  283 batches | lr 0.0003 | loss  0.59 | ppl     1.80\n",
      "| epoch  28 |   125/  283 batches | lr 0.0003 | loss  0.54 | ppl     1.72\n",
      "| epoch  28 |   150/  283 batches | lr 0.0003 | loss  0.53 | ppl     1.70\n",
      "| epoch  28 |   175/  283 batches | lr 0.0003 | loss  0.53 | ppl     1.71\n",
      "| epoch  28 |   200/  283 batches | lr 0.0003 | loss  0.54 | ppl     1.72\n",
      "| epoch  28 |   225/  283 batches | lr 0.0003 | loss  0.52 | ppl     1.68\n",
      "| epoch  28 |   250/  283 batches | lr 0.0003 | loss  0.60 | ppl     1.82\n",
      "| epoch  28 |   275/  283 batches | lr 0.0003 | loss  0.54 | ppl     1.72\n",
      "Epoch: 29 | Epoch Time: 0m 6s\n",
      "\tTrain Loss: 0.015 | Val. Loss: 0.584\n",
      "| epoch  29 |    25/  283 batches | lr 0.0003 | loss  0.48 | ppl     1.62\n",
      "| epoch  29 |    50/  283 batches | lr 0.0003 | loss  0.55 | ppl     1.73\n",
      "| epoch  29 |    75/  283 batches | lr 0.0003 | loss  0.52 | ppl     1.68\n",
      "| epoch  29 |   100/  283 batches | lr 0.0003 | loss  0.52 | ppl     1.69\n",
      "| epoch  29 |   125/  283 batches | lr 0.0003 | loss  0.58 | ppl     1.79\n",
      "| epoch  29 |   150/  283 batches | lr 0.0003 | loss  0.53 | ppl     1.69\n",
      "| epoch  29 |   175/  283 batches | lr 0.0003 | loss  0.57 | ppl     1.77\n",
      "| epoch  29 |   200/  283 batches | lr 0.0003 | loss  0.51 | ppl     1.67\n",
      "| epoch  29 |   225/  283 batches | lr 0.0003 | loss  0.57 | ppl     1.78\n",
      "| epoch  29 |   250/  283 batches | lr 0.0003 | loss  0.51 | ppl     1.67\n",
      "| epoch  29 |   275/  283 batches | lr 0.0003 | loss  0.57 | ppl     1.77\n",
      "Epoch: 30 | Epoch Time: 0m 6s\n",
      "\tTrain Loss: 0.016 | Val. Loss: 0.582\n",
      "| epoch  30 |    25/  283 batches | lr 0.0003 | loss  0.57 | ppl     1.76\n",
      "| epoch  30 |    50/  283 batches | lr 0.0003 | loss  0.56 | ppl     1.75\n",
      "| epoch  30 |    75/  283 batches | lr 0.0003 | loss  0.54 | ppl     1.72\n",
      "| epoch  30 |   100/  283 batches | lr 0.0003 | loss  0.53 | ppl     1.70\n",
      "| epoch  30 |   125/  283 batches | lr 0.0003 | loss  0.52 | ppl     1.69\n",
      "| epoch  30 |   150/  283 batches | lr 0.0003 | loss  0.53 | ppl     1.70\n",
      "| epoch  30 |   175/  283 batches | lr 0.0003 | loss  0.55 | ppl     1.73\n",
      "| epoch  30 |   200/  283 batches | lr 0.0003 | loss  0.53 | ppl     1.70\n",
      "| epoch  30 |   225/  283 batches | lr 0.0003 | loss  0.56 | ppl     1.74\n",
      "| epoch  30 |   250/  283 batches | lr 0.0003 | loss  0.57 | ppl     1.77\n",
      "| epoch  30 |   275/  283 batches | lr 0.0003 | loss  0.57 | ppl     1.76\n",
      "Epoch: 31 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 0.016 | Val. Loss: 0.580\n",
      "| epoch  31 |    25/  283 batches | lr 0.0003 | loss  0.54 | ppl     1.71\n",
      "| epoch  31 |    50/  283 batches | lr 0.0003 | loss  0.55 | ppl     1.73\n",
      "| epoch  31 |    75/  283 batches | lr 0.0003 | loss  0.52 | ppl     1.68\n",
      "| epoch  31 |   100/  283 batches | lr 0.0003 | loss  0.51 | ppl     1.66\n",
      "| epoch  31 |   125/  283 batches | lr 0.0003 | loss  0.54 | ppl     1.72\n",
      "| epoch  31 |   150/  283 batches | lr 0.0003 | loss  0.58 | ppl     1.78\n",
      "| epoch  31 |   175/  283 batches | lr 0.0003 | loss  0.62 | ppl     1.87\n",
      "| epoch  31 |   200/  283 batches | lr 0.0003 | loss  0.53 | ppl     1.70\n",
      "| epoch  31 |   225/  283 batches | lr 0.0003 | loss  0.52 | ppl     1.68\n",
      "| epoch  31 |   250/  283 batches | lr 0.0003 | loss  0.54 | ppl     1.72\n",
      "| epoch  31 |   275/  283 batches | lr 0.0003 | loss  0.55 | ppl     1.73\n",
      "Epoch: 32 | Epoch Time: 0m 6s\n",
      "\tTrain Loss: 0.016 | Val. Loss: 0.578\n",
      "| epoch  32 |    25/  283 batches | lr 0.0003 | loss  0.54 | ppl     1.71\n",
      "| epoch  32 |    50/  283 batches | lr 0.0003 | loss  0.50 | ppl     1.65\n",
      "| epoch  32 |    75/  283 batches | lr 0.0003 | loss  0.54 | ppl     1.72\n",
      "| epoch  32 |   100/  283 batches | lr 0.0003 | loss  0.52 | ppl     1.67\n",
      "| epoch  32 |   125/  283 batches | lr 0.0003 | loss  0.51 | ppl     1.67\n",
      "| epoch  32 |   150/  283 batches | lr 0.0003 | loss  0.53 | ppl     1.70\n",
      "| epoch  32 |   175/  283 batches | lr 0.0003 | loss  0.52 | ppl     1.68\n",
      "| epoch  32 |   200/  283 batches | lr 0.0003 | loss  0.56 | ppl     1.74\n",
      "| epoch  32 |   225/  283 batches | lr 0.0003 | loss  0.59 | ppl     1.80\n",
      "| epoch  32 |   250/  283 batches | lr 0.0003 | loss  0.55 | ppl     1.74\n",
      "| epoch  32 |   275/  283 batches | lr 0.0003 | loss  0.55 | ppl     1.74\n",
      "Epoch: 33 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 0.015 | Val. Loss: 0.576\n",
      "| epoch  33 |    25/  283 batches | lr 0.0003 | loss  0.55 | ppl     1.73\n",
      "| epoch  33 |    50/  283 batches | lr 0.0003 | loss  0.51 | ppl     1.66\n",
      "| epoch  33 |    75/  283 batches | lr 0.0003 | loss  0.52 | ppl     1.68\n",
      "| epoch  33 |   100/  283 batches | lr 0.0003 | loss  0.51 | ppl     1.67\n",
      "| epoch  33 |   125/  283 batches | lr 0.0003 | loss  0.51 | ppl     1.67\n",
      "| epoch  33 |   150/  283 batches | lr 0.0003 | loss  0.53 | ppl     1.70\n",
      "| epoch  33 |   175/  283 batches | lr 0.0003 | loss  0.53 | ppl     1.70\n",
      "| epoch  33 |   200/  283 batches | lr 0.0003 | loss  0.52 | ppl     1.68\n",
      "| epoch  33 |   225/  283 batches | lr 0.0003 | loss  0.52 | ppl     1.69\n",
      "| epoch  33 |   250/  283 batches | lr 0.0003 | loss  0.52 | ppl     1.69\n",
      "| epoch  33 |   275/  283 batches | lr 0.0003 | loss  0.54 | ppl     1.71\n",
      "Epoch: 34 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 0.015 | Val. Loss: 0.575\n",
      "| epoch  34 |    25/  283 batches | lr 0.0003 | loss  0.48 | ppl     1.61\n",
      "| epoch  34 |    50/  283 batches | lr 0.0003 | loss  0.54 | ppl     1.71\n",
      "| epoch  34 |    75/  283 batches | lr 0.0003 | loss  0.54 | ppl     1.72\n",
      "| epoch  34 |   100/  283 batches | lr 0.0003 | loss  0.51 | ppl     1.67\n",
      "| epoch  34 |   125/  283 batches | lr 0.0003 | loss  0.52 | ppl     1.69\n",
      "| epoch  34 |   150/  283 batches | lr 0.0003 | loss  0.50 | ppl     1.65\n",
      "| epoch  34 |   175/  283 batches | lr 0.0003 | loss  0.51 | ppl     1.67\n",
      "| epoch  34 |   200/  283 batches | lr 0.0003 | loss  0.58 | ppl     1.79\n",
      "| epoch  34 |   225/  283 batches | lr 0.0003 | loss  0.52 | ppl     1.68\n",
      "| epoch  34 |   250/  283 batches | lr 0.0003 | loss  0.54 | ppl     1.71\n",
      "| epoch  34 |   275/  283 batches | lr 0.0003 | loss  0.53 | ppl     1.69\n",
      "Epoch: 35 | Epoch Time: 0m 6s\n",
      "\tTrain Loss: 0.016 | Val. Loss: 0.572\n",
      "| epoch  35 |    25/  283 batches | lr 0.0003 | loss  0.54 | ppl     1.71\n",
      "| epoch  35 |    50/  283 batches | lr 0.0003 | loss  0.46 | ppl     1.58\n",
      "| epoch  35 |    75/  283 batches | lr 0.0003 | loss  0.56 | ppl     1.76\n",
      "| epoch  35 |   100/  283 batches | lr 0.0003 | loss  0.55 | ppl     1.73\n",
      "| epoch  35 |   125/  283 batches | lr 0.0003 | loss  0.52 | ppl     1.68\n",
      "| epoch  35 |   150/  283 batches | lr 0.0003 | loss  0.52 | ppl     1.67\n",
      "| epoch  35 |   175/  283 batches | lr 0.0003 | loss  0.49 | ppl     1.63\n",
      "| epoch  35 |   200/  283 batches | lr 0.0003 | loss  0.53 | ppl     1.70\n",
      "| epoch  35 |   225/  283 batches | lr 0.0003 | loss  0.55 | ppl     1.73\n",
      "| epoch  35 |   250/  283 batches | lr 0.0003 | loss  0.51 | ppl     1.67\n",
      "| epoch  35 |   275/  283 batches | lr 0.0003 | loss  0.53 | ppl     1.70\n",
      "Epoch: 36 | Epoch Time: 0m 6s\n",
      "\tTrain Loss: 0.014 | Val. Loss: 0.572\n",
      "| epoch  36 |    25/  283 batches | lr 0.0003 | loss  0.55 | ppl     1.73\n",
      "| epoch  36 |    50/  283 batches | lr 0.0003 | loss  0.53 | ppl     1.71\n",
      "| epoch  36 |    75/  283 batches | lr 0.0003 | loss  0.51 | ppl     1.67\n",
      "| epoch  36 |   100/  283 batches | lr 0.0003 | loss  0.54 | ppl     1.72\n",
      "| epoch  36 |   125/  283 batches | lr 0.0003 | loss  0.57 | ppl     1.77\n",
      "| epoch  36 |   150/  283 batches | lr 0.0003 | loss  0.52 | ppl     1.68\n",
      "| epoch  36 |   175/  283 batches | lr 0.0003 | loss  0.55 | ppl     1.73\n",
      "| epoch  36 |   200/  283 batches | lr 0.0003 | loss  0.54 | ppl     1.71\n",
      "| epoch  36 |   225/  283 batches | lr 0.0003 | loss  0.52 | ppl     1.68\n",
      "| epoch  36 |   250/  283 batches | lr 0.0003 | loss  0.53 | ppl     1.69\n",
      "| epoch  36 |   275/  283 batches | lr 0.0003 | loss  0.52 | ppl     1.68\n",
      "Epoch: 37 | Epoch Time: 0m 6s\n",
      "\tTrain Loss: 0.015 | Val. Loss: 0.571\n",
      "| epoch  37 |    25/  283 batches | lr 0.0003 | loss  0.48 | ppl     1.62\n",
      "| epoch  37 |    50/  283 batches | lr 0.0003 | loss  0.52 | ppl     1.68\n",
      "| epoch  37 |    75/  283 batches | lr 0.0003 | loss  0.48 | ppl     1.62\n",
      "| epoch  37 |   100/  283 batches | lr 0.0003 | loss  0.50 | ppl     1.64\n",
      "| epoch  37 |   125/  283 batches | lr 0.0003 | loss  0.50 | ppl     1.64\n",
      "| epoch  37 |   150/  283 batches | lr 0.0003 | loss  0.54 | ppl     1.72\n",
      "| epoch  37 |   175/  283 batches | lr 0.0003 | loss  0.53 | ppl     1.70\n",
      "| epoch  37 |   200/  283 batches | lr 0.0003 | loss  0.56 | ppl     1.74\n",
      "| epoch  37 |   225/  283 batches | lr 0.0003 | loss  0.52 | ppl     1.67\n",
      "| epoch  37 |   250/  283 batches | lr 0.0003 | loss  0.53 | ppl     1.70\n",
      "| epoch  37 |   275/  283 batches | lr 0.0003 | loss  0.53 | ppl     1.70\n",
      "Epoch: 38 | Epoch Time: 0m 6s\n",
      "\tTrain Loss: 0.015 | Val. Loss: 0.569\n",
      "| epoch  38 |    25/  283 batches | lr 0.0003 | loss  0.52 | ppl     1.69\n",
      "| epoch  38 |    50/  283 batches | lr 0.0003 | loss  0.52 | ppl     1.68\n",
      "| epoch  38 |    75/  283 batches | lr 0.0003 | loss  0.55 | ppl     1.73\n",
      "| epoch  38 |   100/  283 batches | lr 0.0003 | loss  0.53 | ppl     1.70\n",
      "| epoch  38 |   125/  283 batches | lr 0.0003 | loss  0.47 | ppl     1.60\n",
      "| epoch  38 |   150/  283 batches | lr 0.0003 | loss  0.55 | ppl     1.73\n",
      "| epoch  38 |   175/  283 batches | lr 0.0003 | loss  0.53 | ppl     1.69\n",
      "| epoch  38 |   200/  283 batches | lr 0.0003 | loss  0.46 | ppl     1.59\n",
      "| epoch  38 |   225/  283 batches | lr 0.0003 | loss  0.53 | ppl     1.70\n",
      "| epoch  38 |   250/  283 batches | lr 0.0003 | loss  0.53 | ppl     1.69\n",
      "| epoch  38 |   275/  283 batches | lr 0.0003 | loss  0.50 | ppl     1.65\n",
      "Epoch: 39 | Epoch Time: 0m 6s\n",
      "\tTrain Loss: 0.015 | Val. Loss: 0.570\n",
      "| epoch  39 |    25/  283 batches | lr 0.0003 | loss  0.50 | ppl     1.65\n",
      "| epoch  39 |    50/  283 batches | lr 0.0003 | loss  0.55 | ppl     1.73\n",
      "| epoch  39 |    75/  283 batches | lr 0.0003 | loss  0.53 | ppl     1.69\n",
      "| epoch  39 |   100/  283 batches | lr 0.0003 | loss  0.48 | ppl     1.62\n",
      "| epoch  39 |   125/  283 batches | lr 0.0003 | loss  0.52 | ppl     1.68\n",
      "| epoch  39 |   150/  283 batches | lr 0.0003 | loss  0.54 | ppl     1.72\n",
      "| epoch  39 |   175/  283 batches | lr 0.0003 | loss  0.52 | ppl     1.67\n",
      "| epoch  39 |   200/  283 batches | lr 0.0003 | loss  0.52 | ppl     1.69\n",
      "| epoch  39 |   225/  283 batches | lr 0.0003 | loss  0.57 | ppl     1.77\n",
      "| epoch  39 |   250/  283 batches | lr 0.0003 | loss  0.56 | ppl     1.75\n",
      "| epoch  39 |   275/  283 batches | lr 0.0003 | loss  0.49 | ppl     1.63\n",
      "Epoch: 40 | Epoch Time: 0m 6s\n",
      "\tTrain Loss: 0.015 | Val. Loss: 0.567\n",
      "| epoch  40 |    25/  283 batches | lr 0.0003 | loss  0.54 | ppl     1.71\n",
      "| epoch  40 |    50/  283 batches | lr 0.0003 | loss  0.49 | ppl     1.62\n",
      "| epoch  40 |    75/  283 batches | lr 0.0003 | loss  0.51 | ppl     1.67\n",
      "| epoch  40 |   100/  283 batches | lr 0.0003 | loss  0.53 | ppl     1.70\n",
      "| epoch  40 |   125/  283 batches | lr 0.0003 | loss  0.46 | ppl     1.58\n",
      "| epoch  40 |   150/  283 batches | lr 0.0003 | loss  0.54 | ppl     1.72\n",
      "| epoch  40 |   175/  283 batches | lr 0.0003 | loss  0.52 | ppl     1.67\n",
      "| epoch  40 |   200/  283 batches | lr 0.0003 | loss  0.51 | ppl     1.66\n",
      "| epoch  40 |   225/  283 batches | lr 0.0003 | loss  0.53 | ppl     1.70\n",
      "| epoch  40 |   250/  283 batches | lr 0.0003 | loss  0.54 | ppl     1.71\n",
      "| epoch  40 |   275/  283 batches | lr 0.0003 | loss  0.52 | ppl     1.69\n",
      "Epoch: 41 | Epoch Time: 0m 6s\n",
      "\tTrain Loss: 0.015 | Val. Loss: 0.568\n",
      "| epoch  41 |    25/  283 batches | lr 0.0003 | loss  0.52 | ppl     1.67\n",
      "| epoch  41 |    50/  283 batches | lr 0.0003 | loss  0.48 | ppl     1.62\n",
      "| epoch  41 |    75/  283 batches | lr 0.0003 | loss  0.50 | ppl     1.65\n",
      "| epoch  41 |   100/  283 batches | lr 0.0003 | loss  0.53 | ppl     1.70\n",
      "| epoch  41 |   125/  283 batches | lr 0.0003 | loss  0.49 | ppl     1.63\n",
      "| epoch  41 |   150/  283 batches | lr 0.0003 | loss  0.45 | ppl     1.56\n",
      "| epoch  41 |   175/  283 batches | lr 0.0003 | loss  0.55 | ppl     1.74\n",
      "| epoch  41 |   200/  283 batches | lr 0.0003 | loss  0.47 | ppl     1.60\n",
      "| epoch  41 |   225/  283 batches | lr 0.0003 | loss  0.57 | ppl     1.76\n",
      "| epoch  41 |   250/  283 batches | lr 0.0003 | loss  0.52 | ppl     1.68\n",
      "| epoch  41 |   275/  283 batches | lr 0.0003 | loss  0.54 | ppl     1.72\n",
      "Epoch: 42 | Epoch Time: 0m 6s\n",
      "\tTrain Loss: 0.015 | Val. Loss: 0.566\n",
      "| epoch  42 |    25/  283 batches | lr 0.0003 | loss  0.55 | ppl     1.73\n",
      "| epoch  42 |    50/  283 batches | lr 0.0003 | loss  0.48 | ppl     1.61\n",
      "| epoch  42 |    75/  283 batches | lr 0.0003 | loss  0.50 | ppl     1.64\n",
      "| epoch  42 |   100/  283 batches | lr 0.0003 | loss  0.55 | ppl     1.73\n",
      "| epoch  42 |   125/  283 batches | lr 0.0003 | loss  0.51 | ppl     1.66\n",
      "| epoch  42 |   150/  283 batches | lr 0.0003 | loss  0.51 | ppl     1.66\n",
      "| epoch  42 |   175/  283 batches | lr 0.0003 | loss  0.51 | ppl     1.66\n",
      "| epoch  42 |   200/  283 batches | lr 0.0003 | loss  0.52 | ppl     1.69\n",
      "| epoch  42 |   225/  283 batches | lr 0.0003 | loss  0.49 | ppl     1.64\n",
      "| epoch  42 |   250/  283 batches | lr 0.0003 | loss  0.53 | ppl     1.70\n",
      "| epoch  42 |   275/  283 batches | lr 0.0003 | loss  0.51 | ppl     1.67\n",
      "Epoch: 43 | Epoch Time: 0m 6s\n",
      "\tTrain Loss: 0.015 | Val. Loss: 0.566\n",
      "| epoch  43 |    25/  283 batches | lr 0.0003 | loss  0.53 | ppl     1.70\n",
      "| epoch  43 |    50/  283 batches | lr 0.0003 | loss  0.52 | ppl     1.68\n",
      "| epoch  43 |    75/  283 batches | lr 0.0003 | loss  0.51 | ppl     1.66\n",
      "| epoch  43 |   100/  283 batches | lr 0.0003 | loss  0.51 | ppl     1.67\n",
      "| epoch  43 |   125/  283 batches | lr 0.0003 | loss  0.51 | ppl     1.67\n",
      "| epoch  43 |   150/  283 batches | lr 0.0003 | loss  0.50 | ppl     1.64\n",
      "| epoch  43 |   175/  283 batches | lr 0.0003 | loss  0.50 | ppl     1.66\n",
      "| epoch  43 |   200/  283 batches | lr 0.0003 | loss  0.48 | ppl     1.62\n",
      "| epoch  43 |   225/  283 batches | lr 0.0003 | loss  0.49 | ppl     1.64\n",
      "| epoch  43 |   250/  283 batches | lr 0.0003 | loss  0.51 | ppl     1.67\n",
      "| epoch  43 |   275/  283 batches | lr 0.0003 | loss  0.50 | ppl     1.65\n",
      "Epoch: 44 | Epoch Time: 0m 6s\n",
      "\tTrain Loss: 0.014 | Val. Loss: 0.566\n",
      "| epoch  44 |    25/  283 batches | lr 0.0003 | loss  0.50 | ppl     1.65\n",
      "| epoch  44 |    50/  283 batches | lr 0.0003 | loss  0.49 | ppl     1.64\n",
      "| epoch  44 |    75/  283 batches | lr 0.0003 | loss  0.49 | ppl     1.63\n",
      "| epoch  44 |   100/  283 batches | lr 0.0003 | loss  0.51 | ppl     1.66\n",
      "| epoch  44 |   125/  283 batches | lr 0.0003 | loss  0.44 | ppl     1.56\n",
      "| epoch  44 |   150/  283 batches | lr 0.0003 | loss  0.52 | ppl     1.68\n",
      "| epoch  44 |   175/  283 batches | lr 0.0003 | loss  0.51 | ppl     1.66\n",
      "| epoch  44 |   200/  283 batches | lr 0.0003 | loss  0.50 | ppl     1.64\n",
      "| epoch  44 |   225/  283 batches | lr 0.0003 | loss  0.49 | ppl     1.63\n",
      "| epoch  44 |   250/  283 batches | lr 0.0003 | loss  0.54 | ppl     1.72\n",
      "| epoch  44 |   275/  283 batches | lr 0.0003 | loss  0.56 | ppl     1.74\n",
      "Epoch: 45 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 0.014 | Val. Loss: 0.565\n",
      "| epoch  45 |    25/  283 batches | lr 0.0003 | loss  0.52 | ppl     1.69\n",
      "| epoch  45 |    50/  283 batches | lr 0.0003 | loss  0.51 | ppl     1.66\n",
      "| epoch  45 |    75/  283 batches | lr 0.0003 | loss  0.52 | ppl     1.68\n",
      "| epoch  45 |   100/  283 batches | lr 0.0003 | loss  0.56 | ppl     1.75\n",
      "| epoch  45 |   125/  283 batches | lr 0.0003 | loss  0.45 | ppl     1.57\n",
      "| epoch  45 |   150/  283 batches | lr 0.0003 | loss  0.49 | ppl     1.63\n",
      "| epoch  45 |   175/  283 batches | lr 0.0003 | loss  0.51 | ppl     1.66\n",
      "| epoch  45 |   200/  283 batches | lr 0.0003 | loss  0.49 | ppl     1.62\n",
      "| epoch  45 |   225/  283 batches | lr 0.0003 | loss  0.51 | ppl     1.66\n",
      "| epoch  45 |   250/  283 batches | lr 0.0003 | loss  0.50 | ppl     1.65\n",
      "| epoch  45 |   275/  283 batches | lr 0.0003 | loss  0.53 | ppl     1.70\n",
      "Epoch: 46 | Epoch Time: 0m 6s\n",
      "\tTrain Loss: 0.014 | Val. Loss: 0.565\n",
      "| epoch  46 |    25/  283 batches | lr 0.0003 | loss  0.50 | ppl     1.65\n",
      "| epoch  46 |    50/  283 batches | lr 0.0003 | loss  0.51 | ppl     1.66\n",
      "| epoch  46 |    75/  283 batches | lr 0.0003 | loss  0.51 | ppl     1.67\n",
      "| epoch  46 |   100/  283 batches | lr 0.0003 | loss  0.52 | ppl     1.68\n",
      "| epoch  46 |   125/  283 batches | lr 0.0003 | loss  0.55 | ppl     1.73\n",
      "| epoch  46 |   150/  283 batches | lr 0.0003 | loss  0.52 | ppl     1.69\n",
      "| epoch  46 |   175/  283 batches | lr 0.0003 | loss  0.47 | ppl     1.60\n",
      "| epoch  46 |   200/  283 batches | lr 0.0003 | loss  0.50 | ppl     1.65\n",
      "| epoch  46 |   225/  283 batches | lr 0.0003 | loss  0.53 | ppl     1.71\n",
      "| epoch  46 |   250/  283 batches | lr 0.0003 | loss  0.50 | ppl     1.66\n",
      "| epoch  46 |   275/  283 batches | lr 0.0003 | loss  0.50 | ppl     1.65\n",
      "Epoch: 47 | Epoch Time: 0m 6s\n",
      "\tTrain Loss: 0.014 | Val. Loss: 0.563\n",
      "| epoch  47 |    25/  283 batches | lr 0.0003 | loss  0.47 | ppl     1.60\n",
      "| epoch  47 |    50/  283 batches | lr 0.0003 | loss  0.49 | ppl     1.63\n",
      "| epoch  47 |    75/  283 batches | lr 0.0003 | loss  0.50 | ppl     1.66\n",
      "| epoch  47 |   100/  283 batches | lr 0.0003 | loss  0.57 | ppl     1.76\n",
      "| epoch  47 |   125/  283 batches | lr 0.0003 | loss  0.46 | ppl     1.59\n",
      "| epoch  47 |   150/  283 batches | lr 0.0003 | loss  0.51 | ppl     1.67\n",
      "| epoch  47 |   175/  283 batches | lr 0.0003 | loss  0.52 | ppl     1.68\n",
      "| epoch  47 |   200/  283 batches | lr 0.0003 | loss  0.50 | ppl     1.65\n",
      "| epoch  47 |   225/  283 batches | lr 0.0003 | loss  0.53 | ppl     1.70\n",
      "| epoch  47 |   250/  283 batches | lr 0.0003 | loss  0.54 | ppl     1.71\n",
      "| epoch  47 |   275/  283 batches | lr 0.0003 | loss  0.51 | ppl     1.66\n",
      "Epoch: 48 | Epoch Time: 0m 6s\n",
      "\tTrain Loss: 0.014 | Val. Loss: 0.563\n",
      "| epoch  48 |    25/  283 batches | lr 0.0003 | loss  0.48 | ppl     1.62\n",
      "| epoch  48 |    50/  283 batches | lr 0.0003 | loss  0.46 | ppl     1.59\n",
      "| epoch  48 |    75/  283 batches | lr 0.0003 | loss  0.49 | ppl     1.64\n",
      "| epoch  48 |   100/  283 batches | lr 0.0003 | loss  0.44 | ppl     1.56\n",
      "| epoch  48 |   125/  283 batches | lr 0.0003 | loss  0.45 | ppl     1.57\n",
      "| epoch  48 |   150/  283 batches | lr 0.0003 | loss  0.55 | ppl     1.74\n",
      "| epoch  48 |   175/  283 batches | lr 0.0003 | loss  0.51 | ppl     1.67\n",
      "| epoch  48 |   200/  283 batches | lr 0.0003 | loss  0.49 | ppl     1.63\n",
      "| epoch  48 |   225/  283 batches | lr 0.0003 | loss  0.51 | ppl     1.67\n",
      "| epoch  48 |   250/  283 batches | lr 0.0003 | loss  0.53 | ppl     1.69\n",
      "| epoch  48 |   275/  283 batches | lr 0.0003 | loss  0.50 | ppl     1.66\n",
      "Epoch: 49 | Epoch Time: 0m 6s\n",
      "\tTrain Loss: 0.014 | Val. Loss: 0.562\n",
      "| epoch  49 |    25/  283 batches | lr 0.0003 | loss  0.48 | ppl     1.62\n",
      "| epoch  49 |    50/  283 batches | lr 0.0003 | loss  0.46 | ppl     1.58\n",
      "| epoch  49 |    75/  283 batches | lr 0.0003 | loss  0.49 | ppl     1.63\n",
      "| epoch  49 |   100/  283 batches | lr 0.0003 | loss  0.51 | ppl     1.66\n",
      "| epoch  49 |   125/  283 batches | lr 0.0003 | loss  0.45 | ppl     1.56\n",
      "| epoch  49 |   150/  283 batches | lr 0.0003 | loss  0.44 | ppl     1.56\n",
      "| epoch  49 |   175/  283 batches | lr 0.0003 | loss  0.49 | ppl     1.64\n",
      "| epoch  49 |   200/  283 batches | lr 0.0003 | loss  0.55 | ppl     1.74\n",
      "| epoch  49 |   225/  283 batches | lr 0.0003 | loss  0.54 | ppl     1.71\n",
      "| epoch  49 |   250/  283 batches | lr 0.0003 | loss  0.52 | ppl     1.69\n",
      "| epoch  49 |   275/  283 batches | lr 0.0003 | loss  0.48 | ppl     1.61\n",
      "Epoch: 50 | Epoch Time: 0m 6s\n",
      "\tTrain Loss: 0.014 | Val. Loss: 0.562\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 50\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "counter = 0\n",
    "patience = 5\n",
    "log_interval = 25\n",
    "plot_every = 10\n",
    "train_loses = []\n",
    "valid_loses = []\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    train_loss = train(model, train_loader, optimizer, criterion)\n",
    "    valid_loss = evaluate(model, valid_loader, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Val. Loss: {valid_loss:.3f}')\n",
    "    \n",
    "    train_loses.append(train_loss)\n",
    "    valid_loses.append(valid_loss)\n",
    "\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        #torch.save(model.state_dict(), 'tut2-model.pt')\n",
    "        counter = 0 \n",
    "    else:\n",
    "        counter += 1\n",
    "        if counter >= patience:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 951,
     "status": "ok",
     "timestamp": 1591637780100,
     "user": {
      "displayName": "Daniel Cuiñas Vázquez",
      "photoUrl": "",
      "userId": "11552885780691803973"
     },
     "user_tz": -120
    },
    "id": "XCOuGxd3WRnI",
    "outputId": "22ec9662-0782-4fed-8e46-b68c183156f5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fbceae5d2e8>]"
      ]
     },
     "execution_count": 37,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "plt.figure()\n",
    "# plt.plot(train_loses)\n",
    "plt.plot(valid_loses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 889,
     "status": "ok",
     "timestamp": 1591637783204,
     "user": {
      "displayName": "Daniel Cuiñas Vázquez",
      "photoUrl": "",
      "userId": "11552885780691803973"
     },
     "user_tz": -120
    },
    "id": "fydt-eAaZW5l",
    "outputId": "3e7ecb12-9756-480c-8d6b-f51e7437fe2f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5631688719704038"
      ]
     },
     "execution_count": 38,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(model, test_loader, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zhlj8b_-WrWB"
   },
   "outputs": [],
   "source": [
    "def sample(category_, prime_str, predict_len = 10):\n",
    "    cat_input = categoryArray(all_categories[category_])\n",
    "    cat_tensor = torch.tensor(cat_input).unsqueeze(0).float()\n",
    "    word_indexes = [dict_words[li] for li in prime_str]\n",
    "    prime_input = torch.tensor(word_indexes).unsqueeze(1)\n",
    "    prime_input = prime_input.to(device)\n",
    "    cat_tensor = cat_tensor.to(device)\n",
    "    hidden = model.init_hidden(1)\n",
    "    hidden = hidden.to(device)\n",
    "    # Use priming string to \"build up\" hidden state\n",
    "    with torch.no_grad():\n",
    "        for p in range(len(prime_str) - 1):\n",
    "            _, hidden = model(cat_tensor, prime_input[p], hidden)\n",
    "\n",
    "        inp = prime_input[-1]\n",
    "        inp = inp.to(device)\n",
    "        predicted = prime_str\n",
    "        tops = []\n",
    "        for i in range(predict_len):\n",
    "            output, hidden = model(cat_tensor, inp, hidden)\n",
    "            topv, topp = output.topk(1)\n",
    "            _, topps = output.topk(20)\n",
    "            tops.append(topps)\n",
    "            topi = topp[0][0].item()\n",
    "            if topi == 1:\n",
    "                break   \n",
    "            else:\n",
    "                letter = int_2_vocab[topi]\n",
    "                predicted += letter\n",
    "            inp = [dict_words[letter]]\n",
    "            inp = torch.tensor(inp).to(device)\n",
    "\n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 935,
     "status": "ok",
     "timestamp": 1591637802967,
     "user": {
      "displayName": "Daniel Cuiñas Vázquez",
      "photoUrl": "",
      "userId": "11552885780691803973"
     },
     "user_tz": -120
    },
    "id": "X_ACMUgxhM_n",
    "outputId": "59a34fb6-2c89-4999-d388-9c20d9acc8e5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Vitori'"
      ]
     },
     "execution_count": 42,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample('Italian',\"Vit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 886,
     "status": "ok",
     "timestamp": 1591637808719,
     "user": {
      "displayName": "Daniel Cuiñas Vázquez",
      "photoUrl": "",
      "userId": "11552885780691803973"
     },
     "user_tz": -120
    },
    "id": "jM_q4pATYXyG",
    "outputId": "c4c8f1a3-d96a-4d47-ee33-63a21f459758"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Rimino'"
      ]
     },
     "execution_count": 43,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample('Italian','Rimin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 334036,
     "status": "ok",
     "timestamp": 1591210910634,
     "user": {
      "displayName": "Daniel Cuiñas Vázquez",
      "photoUrl": "",
      "userId": "11552885780691803973"
     },
     "user_tz": -120
    },
    "id": "VGNYHiDDwaVv",
    "outputId": "df3f8484-6977-4937-c33c-7df767859553"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category_</th>\n",
       "      <th>name</th>\n",
       "      <th>len</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4447</th>\n",
       "      <td>Italian</td>\n",
       "      <td>Bartalotti</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4292</th>\n",
       "      <td>Italian</td>\n",
       "      <td>Abatescianni</td>\n",
       "      <td>12</td>\n",
       "      <td>[0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4368</th>\n",
       "      <td>Italian</td>\n",
       "      <td>Albrici</td>\n",
       "      <td>7</td>\n",
       "      <td>[0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4886</th>\n",
       "      <td>Italian</td>\n",
       "      <td>Sapienti</td>\n",
       "      <td>8</td>\n",
       "      <td>[0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4899</th>\n",
       "      <td>Italian</td>\n",
       "      <td>Scotti</td>\n",
       "      <td>6</td>\n",
       "      <td>[0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4979</th>\n",
       "      <td>Italian</td>\n",
       "      <td>Vico</td>\n",
       "      <td>4</td>\n",
       "      <td>[0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4611</th>\n",
       "      <td>Italian</td>\n",
       "      <td>Fierro</td>\n",
       "      <td>6</td>\n",
       "      <td>[0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4301</th>\n",
       "      <td>Italian</td>\n",
       "      <td>Abbiati</td>\n",
       "      <td>7</td>\n",
       "      <td>[0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4650</th>\n",
       "      <td>Italian</td>\n",
       "      <td>Guttuso</td>\n",
       "      <td>7</td>\n",
       "      <td>[0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4815</th>\n",
       "      <td>Italian</td>\n",
       "      <td>Pisani</td>\n",
       "      <td>6</td>\n",
       "      <td>[0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>709 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     category_  ...                                           category\n",
       "4447   Italian  ...  [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
       "4292   Italian  ...  [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
       "4368   Italian  ...  [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
       "4886   Italian  ...  [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
       "4899   Italian  ...  [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
       "...        ...  ...                                                ...\n",
       "4979   Italian  ...  [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
       "4611   Italian  ...  [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
       "4301   Italian  ...  [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
       "4650   Italian  ...  [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
       "4815   Italian  ...  [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
       "\n",
       "[709 rows x 4 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['category_']=='Italian']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GSJ-RgVlyEn_"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyM8Gp9bGVG561zjpb4yi7Et",
   "collapsed_sections": [],
   "name": "GRU_batch.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
