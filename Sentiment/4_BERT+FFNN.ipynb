{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1xHs6cKdTfU-"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# read data from text files\n",
    "with open('data/reviews_sample.txt', 'r') as f:\n",
    "    reviews = f.read()\n",
    "with open('data/labels_sample.txt', 'r') as f:\n",
    "    labels = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PKLtoakPTnsl"
   },
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "\n",
    "# get rid of punctuation\n",
    "reviews = reviews.lower() # lowercase, standardize\n",
    "all_text = ''.join([c for c in reviews if c not in punctuation])\n",
    "\n",
    "# split by new lines and spaces\n",
    "reviews_split = all_text.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CA9aUvRUTpKc"
   },
   "outputs": [],
   "source": [
    "reviews_split=reviews_split[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vBcoWjgaTq8u"
   },
   "outputs": [],
   "source": [
    "# 1=positive, 0=negative label conversion\n",
    "labels_split = labels.split('\\n')\n",
    "labels_split=labels_split[:-1]\n",
    "encoded_labels_ = np.array([1 if label == 'positive' else 0 for label in labels_split])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 612
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 35445,
     "status": "ok",
     "timestamp": 1575466350614,
     "user": {
      "displayName": "Daniel Cuiñas Vázquez",
      "photoUrl": "",
      "userId": "11552885780691803973"
     },
     "user_tz": -60
    },
    "id": "WMCctqfTTsVY",
    "outputId": "75d1ff44-8aee-4140-d134-a904a631ed79"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/70/1a/364556102943cacde1ee00fdcae3b1615b39e52649eddbf54953e5b144c9/transformers-2.2.1-py3-none-any.whl (364kB)\n",
      "\r",
      "\u001b[K     |█                               | 10kB 21.7MB/s eta 0:00:01\r",
      "\u001b[K     |█▉                              | 20kB 1.7MB/s eta 0:00:01\r",
      "\u001b[K     |██▊                             | 30kB 2.5MB/s eta 0:00:01\r",
      "\u001b[K     |███▋                            | 40kB 1.7MB/s eta 0:00:01\r",
      "\u001b[K     |████▌                           | 51kB 2.1MB/s eta 0:00:01\r",
      "\u001b[K     |█████▍                          | 61kB 2.5MB/s eta 0:00:01\r",
      "\u001b[K     |██████▎                         | 71kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████▏                        | 81kB 3.2MB/s eta 0:00:01\r",
      "\u001b[K     |████████                        | 92kB 3.6MB/s eta 0:00:01\r",
      "\u001b[K     |█████████                       | 102kB 2.7MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▉                      | 112kB 2.7MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▊                     | 122kB 2.7MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▊                    | 133kB 2.7MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▋                   | 143kB 2.7MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▌                  | 153kB 2.7MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▍                 | 163kB 2.7MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▎                | 174kB 2.7MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▏               | 184kB 2.7MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████               | 194kB 2.7MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████              | 204kB 2.7MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▉             | 215kB 2.7MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▊            | 225kB 2.7MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▋           | 235kB 2.7MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▌          | 245kB 2.7MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▌         | 256kB 2.7MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▍        | 266kB 2.7MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▎       | 276kB 2.7MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▏      | 286kB 2.7MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████      | 296kB 2.7MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████     | 307kB 2.7MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▉    | 317kB 2.7MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▊   | 327kB 2.7MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▋  | 337kB 2.7MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▌ | 348kB 2.7MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▍| 358kB 2.7MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 368kB 2.7MB/s \n",
      "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.17.4)\n",
      "Collecting regex\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/8e/cbf2295643d7265e7883326fb4654e643bfc93b3a8a8274d8010a39d8804/regex-2019.11.1-cp36-cp36m-manylinux1_x86_64.whl (643kB)\n",
      "\u001b[K     |████████████████████████████████| 645kB 45.5MB/s \n",
      "\u001b[?25hCollecting sacremoses\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1f/8e/ed5364a06a9ba720fddd9820155cc57300d28f5f43a6fd7b7e817177e642/sacremoses-0.0.35.tar.gz (859kB)\n",
      "\u001b[K     |████████████████████████████████| 860kB 42.8MB/s \n",
      "\u001b[?25hCollecting sentencepiece\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/3d/efb655a670b98f62ec32d66954e1109f403db4d937c50d779a75b9763a29/sentencepiece-0.1.83-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
      "\u001b[K     |████████████████████████████████| 1.0MB 35.5MB/s \n",
      "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from transformers) (4.28.1)\n",
      "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.10.18)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.0)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.0)\n",
      "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.2.1)\n",
      "Requirement already satisfied: botocore<1.14.0,>=1.13.18 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.13.18)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.4)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.9.11)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.18->boto3->transformers) (0.15.2)\n",
      "Requirement already satisfied: python-dateutil<2.8.1,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.18->boto3->transformers) (2.6.1)\n",
      "Building wheels for collected packages: sacremoses\n",
      "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for sacremoses: filename=sacremoses-0.0.35-cp36-none-any.whl size=883999 sha256=a09706b00be9234241f9983d73041bdf45447f9ea17d9f750ccb02cf0f1f3c45\n",
      "  Stored in directory: /root/.cache/pip/wheels/63/2a/db/63e2909042c634ef551d0d9ac825b2b0b32dede4a6d87ddc94\n",
      "Successfully built sacremoses\n",
      "Installing collected packages: regex, sacremoses, sentencepiece, transformers\n",
      "Successfully installed regex-2019.11.1 sacremoses-0.0.35 sentencepiece-0.1.83 transformers-2.2.1\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 114
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 46684,
     "status": "ok",
     "timestamp": 1575466361867,
     "user": {
      "displayName": "Daniel Cuiñas Vázquez",
      "photoUrl": "",
      "userId": "11552885780691803973"
     },
     "user_tz": -60
    },
    "id": "eAcudZbiTuX-",
    "outputId": "7dab724f-1ea5-464d-d6b1-837ac82f52f5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p style=\"color: red;\">\n",
       "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
       "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
       "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
       "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 231508/231508 [00:00<00:00, 4550338.48B/s]\n",
      "100%|██████████| 492/492 [00:00<00:00, 252922.85B/s]\n",
      "100%|██████████| 267967963/267967963 [00:04<00:00, 56366420.60B/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "from transformers import DistilBertModel, DistilBertTokenizer, AdamW\n",
    "\n",
    "\n",
    "BERT_MODEL='distilbert-base-uncased'\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(BERT_MODEL)\n",
    "model = DistilBertModel.from_pretrained(BERT_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 46672,
     "status": "ok",
     "timestamp": 1575466361872,
     "user": {
      "displayName": "Daniel Cuiñas Vázquez",
      "photoUrl": "",
      "userId": "11552885780691803973"
     },
     "user_tz": -60
    },
    "id": "8-e9ELyCUK2i",
    "outputId": "9351f192-9a9a-4bd0-afe9-b8b9069e982e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] [SEP] [PAD] [UNK]\n"
     ]
    }
   ],
   "source": [
    "init_token = tokenizer.cls_token\n",
    "eos_token = tokenizer.sep_token\n",
    "pad_token = tokenizer.pad_token\n",
    "unk_token = tokenizer.unk_token\n",
    "\n",
    "print(init_token, eos_token, pad_token, unk_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 46657,
     "status": "ok",
     "timestamp": 1575466361876,
     "user": {
      "displayName": "Daniel Cuiñas Vázquez",
      "photoUrl": "",
      "userId": "11552885780691803973"
     },
     "user_tz": -60
    },
    "id": "1TGF0QDhVB9I",
    "outputId": "73c1932a-9dce-41b5-82d3-c2af440df6cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101 102 0 100\n"
     ]
    }
   ],
   "source": [
    "init_token_idx = tokenizer.convert_tokens_to_ids(init_token)\n",
    "eos_token_idx = tokenizer.convert_tokens_to_ids(eos_token)\n",
    "pad_token_idx = tokenizer.convert_tokens_to_ids(pad_token)\n",
    "unk_token_idx = tokenizer.convert_tokens_to_ids(unk_token)\n",
    "\n",
    "print(init_token_idx, eos_token_idx, pad_token_idx, unk_token_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 46595,
     "status": "ok",
     "timestamp": 1575466361879,
     "user": {
      "displayName": "Daniel Cuiñas Vázquez",
      "photoUrl": "",
      "userId": "11552885780691803973"
     },
     "user_tz": -60
    },
    "id": "fFcHqLJxVDSV",
    "outputId": "ea8838e0-ac92-493e-f968-ab2564bf0444"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n"
     ]
    }
   ],
   "source": [
    "max_input_length = tokenizer.max_model_input_sizes[BERT_MODEL]\n",
    "\n",
    "print(max_input_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7uthpTH6VNO_"
   },
   "outputs": [],
   "source": [
    "def tokenize_and_cut(sentence):\n",
    "    tokens=\"[CLS] \"+sentence+\" [SEP]\"\n",
    "    tokens = tokenizer.tokenize(tokens) \n",
    "    tokens = tokens[:max_input_length-2]\n",
    "    return tokens\n",
    "\n",
    "def pad_features(reviews_ints, seq_length):\n",
    "    ''' Return features of review_ints, where each review is padded with 0's \n",
    "        or truncated to the input seq_length.\n",
    "    '''\n",
    "    \n",
    "    # getting the correct rows x cols shape\n",
    "    features = np.zeros((len(reviews_ints), seq_length), dtype=int)\n",
    "\n",
    "    # for each review, I grab that review and \n",
    "    for i, row in enumerate(reviews_ints):\n",
    "        features[i, :len(row)] = np.array(row)[:seq_length]\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VV8acAFNVS9l"
   },
   "outputs": [],
   "source": [
    "tokenized_texts = [tokenize_and_cut(sent) for sent in reviews_split]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7j2MRwcPVUnI"
   },
   "outputs": [],
   "source": [
    "input_ids=[tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8lP2DA3iVWEQ"
   },
   "outputs": [],
   "source": [
    "max_len=200\n",
    "\n",
    "input_ids=pad_features(input_ids, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tsLjrWzVkSzo"
   },
   "outputs": [],
   "source": [
    "attention_masks = [[float(i>0) for i in ii] for ii in input_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 150154,
     "status": "ok",
     "timestamp": 1575466465617,
     "user": {
      "displayName": "Daniel Cuiñas Vázquez",
      "photoUrl": "",
      "userId": "11552885780691803973"
     },
     "user_tz": -60
    },
    "id": "LnkV50LjVXqL",
    "outputId": "7c04cda4-2e47-4deb-eb12-b32d81b7cd2a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tFeature Shapes:\n",
      "Train set: \t\t(20000, 200) \n",
      "Validation set: \t(2500, 200) \n",
      "Test set: \t\t(2500, 200)\n"
     ]
    }
   ],
   "source": [
    "split_frac = 0.8\n",
    "\n",
    "## split data into training, validation, and test data (features and labels, x and y)\n",
    "\n",
    "split_idx = int(len(input_ids)*split_frac)\n",
    "tr_inputs, remaining_inputs = input_ids[:split_idx], input_ids[split_idx:]\n",
    "tr_tags, remaining_tags = encoded_labels_[:split_idx], encoded_labels_[split_idx:]\n",
    "tr_masks, remaining_masks = attention_masks[:split_idx], attention_masks[split_idx:]\n",
    "\n",
    "test_idx = int(len(remaining_inputs)*0.5)\n",
    "val_inputs, test_inputs = remaining_inputs[:test_idx], remaining_inputs[test_idx:]\n",
    "val_tags, test_tags = remaining_tags[:test_idx], remaining_tags[test_idx:]\n",
    "val_masks, test_masks = remaining_masks[:test_idx], remaining_masks[test_idx:]\n",
    "\n",
    "## print out the shapes of your resultant feature data\n",
    "print(\"\\t\\t\\tFeature Shapes:\")\n",
    "print(\"Train set: \\t\\t{}\".format(tr_inputs.shape), \n",
    "      \"\\nValidation set: \\t{}\".format(val_inputs.shape),\n",
    "      \"\\nTest set: \\t\\t{}\".format(test_inputs.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kUYhkq10Y7Bc"
   },
   "outputs": [],
   "source": [
    "tr_inputs = torch.tensor(tr_inputs)\n",
    "val_inputs = torch.tensor(val_inputs)\n",
    "test_inputs=torch.tensor(test_inputs)\n",
    "\n",
    "tr_tags = torch.tensor(tr_tags)\n",
    "val_tags = torch.tensor(val_tags)\n",
    "test_tags=torch.tensor(test_tags)\n",
    "\n",
    "tr_masks = torch.tensor(tr_masks)\n",
    "val_masks = torch.tensor(val_masks)\n",
    "test_masks=torch.tensor(test_masks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6zvfZAt1Y7ND"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE=64\n",
    "\n",
    "train_data = TensorDataset(tr_inputs, tr_masks, tr_tags)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=BATCH_SIZE)\n",
    "\n",
    "valid_data = TensorDataset(val_inputs, val_masks, val_tags)\n",
    "valid_sampler = SequentialSampler(valid_data)\n",
    "valid_dataloader = DataLoader(valid_data, sampler=valid_sampler, batch_size=BATCH_SIZE)\n",
    "\n",
    "test_data = TensorDataset(test_inputs, test_masks, test_tags)\n",
    "test_sampler = SequentialSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b0Lbh5ffa05_"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DistilBert_FFNN(nn.Module):\n",
    "    def __init__(self, bert):\n",
    "        super(DistilBert_FFNN, self).__init__()\n",
    "        self.bert=bert\n",
    "        self.fc1=nn.Linear(768,100)\n",
    "        self.fc2=nn.Linear(100,1)\n",
    "        self.drop=nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, text, mask):\n",
    "        with torch.no_grad():\n",
    "            last_hidden_states=self.bert(text, attention_mask=mask)[0][:,0,:]\n",
    "        out=self.drop(F.relu(self.fc1(last_hidden_states)))\n",
    "        out=F.relu(self.fc2(out))\n",
    "        return out.squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y6TwEamZc8e_"
   },
   "outputs": [],
   "source": [
    "net=DistilBert_FFNN(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8SEln9LBeF3U"
   },
   "outputs": [],
   "source": [
    "for name, param in net.named_parameters():                \n",
    "    if name.startswith('bert'):\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 149947,
     "status": "ok",
     "timestamp": 1575466465633,
     "user": {
      "displayName": "Daniel Cuiñas Vázquez",
      "photoUrl": "",
      "userId": "11552885780691803973"
     },
     "user_tz": -60
    },
    "id": "7p4JTE7KdCgu",
    "outputId": "319e83be-5c69-46de-b783-9c837fa6243c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 77,001 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(net):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 149938,
     "status": "ok",
     "timestamp": 1575466465635,
     "user": {
      "displayName": "Daniel Cuiñas Vázquez",
      "photoUrl": "",
      "userId": "11552885780691803973"
     },
     "user_tz": -60
    },
    "id": "M4HBaIuydDej",
    "outputId": "2281b04b-6283-4e4d-dd3e-2e14ff2e6aa8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc1.weight\n",
      "fc1.bias\n",
      "fc2.weight\n",
      "fc2.bias\n"
     ]
    }
   ],
   "source": [
    "for name, param in net.named_parameters():                \n",
    "    if param.requires_grad:\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sshblqaIeQqi"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.0001) #cuidado con el lr. Si lr=0.003 no aprende res..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jGwVNOCyeU5_"
   },
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 155407,
     "status": "ok",
     "timestamp": 1575466471133,
     "user": {
      "displayName": "Daniel Cuiñas Vázquez",
      "photoUrl": "",
      "userId": "11552885780691803973"
     },
     "user_tz": -60
    },
    "id": "ZYdrisXxeXH8",
    "outputId": "2797f1ac-2ad9-4804-b05b-237425c11f9a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "net = net.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YQxpgB5XeYwI"
   },
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "\n",
    "    #round predictions to the closest integer\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float() #convert into float for division \n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JnOimJWQgREu"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1W3HJlN2e_Rv"
   },
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for text, mask, labs in iterator:\n",
    "        text=text.cuda()\n",
    "        mask=mask.cuda()\n",
    "        labs=labs.cuda()\n",
    "        labs=labs.float()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        predictions = model(text, mask)\n",
    "        \n",
    "        loss = criterion(predictions, labs)\n",
    "        \n",
    "        acc = binary_accuracy(predictions, labs)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S6V64ggQfogG"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for text, mask, labs in iterator:\n",
    "            text=text.cuda()\n",
    "            mask=mask.cuda()\n",
    "            labs=labs.cuda()\n",
    "            labs=labs.float()\n",
    "\n",
    "            predictions = model(text, mask)\n",
    "            \n",
    "            loss = criterion(predictions, labs)\n",
    "            \n",
    "            acc = binary_accuracy(predictions, labs)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-3uTw1F3fqdn"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2935085,
     "status": "ok",
     "timestamp": 1575472111048,
     "user": {
      "displayName": "Daniel Cuiñas Vázquez",
      "photoUrl": "",
      "userId": "11552885780691803973"
     },
     "user_tz": -60
    },
    "id": "nOW8uUgbfs__",
    "outputId": "183ded96-8bc0-4ea1-fab1-159f6a725387"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 4m 41s\n",
      "\tTrain Loss: 0.644 | Train Acc: 63.49%\n",
      "\t Val. Loss: 0.594 |  Val. Acc: 77.03%\n",
      "Epoch: 02 | Epoch Time: 4m 41s\n",
      "\tTrain Loss: 0.579 | Train Acc: 75.23%\n",
      "\t Val. Loss: 0.563 |  Val. Acc: 78.59%\n",
      "Epoch: 03 | Epoch Time: 4m 41s\n",
      "\tTrain Loss: 0.561 | Train Acc: 78.27%\n",
      "\t Val. Loss: 0.556 |  Val. Acc: 78.16%\n",
      "Epoch: 04 | Epoch Time: 4m 41s\n",
      "\tTrain Loss: 0.553 | Train Acc: 79.55%\n",
      "\t Val. Loss: 0.550 |  Val. Acc: 81.60%\n",
      "Epoch: 05 | Epoch Time: 4m 41s\n",
      "\tTrain Loss: 0.548 | Train Acc: 80.45%\n",
      "\t Val. Loss: 0.547 |  Val. Acc: 80.74%\n",
      "Epoch: 06 | Epoch Time: 4m 41s\n",
      "\tTrain Loss: 0.547 | Train Acc: 80.84%\n",
      "\t Val. Loss: 0.545 |  Val. Acc: 81.29%\n",
      "Epoch: 07 | Epoch Time: 4m 41s\n",
      "\tTrain Loss: 0.543 | Train Acc: 81.18%\n",
      "\t Val. Loss: 0.543 |  Val. Acc: 81.76%\n",
      "Epoch: 08 | Epoch Time: 4m 41s\n",
      "\tTrain Loss: 0.542 | Train Acc: 81.50%\n",
      "\t Val. Loss: 0.543 |  Val. Acc: 81.41%\n",
      "Epoch: 09 | Epoch Time: 4m 41s\n",
      "\tTrain Loss: 0.540 | Train Acc: 81.53%\n",
      "\t Val. Loss: 0.540 |  Val. Acc: 82.19%\n",
      "Epoch: 10 | Epoch Time: 4m 41s\n",
      "\tTrain Loss: 0.539 | Train Acc: 81.74%\n",
      "\t Val. Loss: 0.541 |  Val. Acc: 81.88%\n",
      "Epoch: 11 | Epoch Time: 4m 41s\n",
      "\tTrain Loss: 0.540 | Train Acc: 81.79%\n",
      "\t Val. Loss: 0.540 |  Val. Acc: 82.19%\n",
      "Epoch: 12 | Epoch Time: 4m 41s\n",
      "\tTrain Loss: 0.538 | Train Acc: 81.88%\n",
      "\t Val. Loss: 0.540 |  Val. Acc: 81.99%\n",
      "Epoch: 13 | Epoch Time: 4m 41s\n",
      "\tTrain Loss: 0.537 | Train Acc: 82.08%\n",
      "\t Val. Loss: 0.539 |  Val. Acc: 82.03%\n",
      "Epoch: 14 | Epoch Time: 4m 41s\n",
      "\tTrain Loss: 0.536 | Train Acc: 81.95%\n",
      "\t Val. Loss: 0.538 |  Val. Acc: 82.23%\n",
      "Epoch: 15 | Epoch Time: 4m 41s\n",
      "\tTrain Loss: 0.536 | Train Acc: 82.00%\n",
      "\t Val. Loss: 0.538 |  Val. Acc: 82.30%\n",
      "Epoch: 16 | Epoch Time: 4m 41s\n",
      "\tTrain Loss: 0.535 | Train Acc: 82.00%\n",
      "\t Val. Loss: 0.538 |  Val. Acc: 81.95%\n",
      "Epoch: 17 | Epoch Time: 4m 41s\n",
      "\tTrain Loss: 0.534 | Train Acc: 82.45%\n",
      "\t Val. Loss: 0.537 |  Val. Acc: 82.38%\n",
      "Epoch: 18 | Epoch Time: 4m 41s\n",
      "\tTrain Loss: 0.534 | Train Acc: 82.42%\n",
      "\t Val. Loss: 0.540 |  Val. Acc: 82.38%\n",
      "Epoch: 19 | Epoch Time: 4m 41s\n",
      "\tTrain Loss: 0.534 | Train Acc: 82.37%\n",
      "\t Val. Loss: 0.537 |  Val. Acc: 82.42%\n",
      "Epoch: 20 | Epoch Time: 4m 41s\n",
      "\tTrain Loss: 0.533 | Train Acc: 82.46%\n",
      "\t Val. Loss: 0.536 |  Val. Acc: 82.73%\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 20\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train(net, train_dataloader, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(net, valid_dataloader, criterion)\n",
    "        \n",
    "    end_time = time.time()\n",
    "        \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "        \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(net.state_dict(), 'tut6-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 31636,
     "status": "ok",
     "timestamp": 1575472269823,
     "user": {
      "displayName": "Daniel Cuiñas Vázquez",
      "photoUrl": "",
      "userId": "11552885780691803973"
     },
     "user_tz": -60
    },
    "id": "8kZMX6PPfyGT",
    "outputId": "29c1cd85-10ec-4b14-b506-d18740f61e89"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.535 | Test Acc: 82.03%\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = evaluate(net, test_dataloader,criterion)\n",
    "\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q8aB-1Hlr4gm"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "name": "BERT+FFNN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
