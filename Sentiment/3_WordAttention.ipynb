{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eqQmGpGA6xFH"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# read data from text files\n",
    "with open('data/reviews_sample.txt', 'r') as f:\n",
    "    reviews = f.read()\n",
    "with open('data/labels_sample.txt', 'r') as f:\n",
    "    labels = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uOLDNKE_6QVp"
   },
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "\n",
    "# get rid of punctuation\n",
    "reviews = reviews.lower() # lowercase, standardize\n",
    "all_text = ''.join([c for c in reviews if c not in punctuation])\n",
    "\n",
    "# split by new lines and spaces\n",
    "reviews_split = all_text.split('\\n')\n",
    "all_text = ' '.join(reviews_split)\n",
    "\n",
    "# create a list of words\n",
    "words = all_text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d0O6CUXM6uXy"
   },
   "outputs": [],
   "source": [
    "# feel free to use this import \n",
    "from collections import Counter\n",
    "\n",
    "## Build a dictionary that maps words to integers\n",
    "counts = Counter(words)\n",
    "vocab = sorted(counts, key=counts.get, reverse=True)\n",
    "vocab_to_int = {word: ii for ii, word in enumerate(vocab, 1)}\n",
    "vocab_to_int['<pad>']=0\n",
    "\n",
    "int2vocab={i:w for w,i in vocab_to_int.items()}\n",
    "\n",
    "## use the dict to tokenize each review in reviews_split\n",
    "## store the tokenized reviews in reviews_ints\n",
    "reviews_ints = []\n",
    "for review in reviews_split:\n",
    "    reviews_ints.append([vocab_to_int[word] for word in review.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XEAEN6SM616e"
   },
   "outputs": [],
   "source": [
    "# 1=positive, 0=negative label conversion\n",
    "labels_split = labels.split('\\n')\n",
    "encoded_labels = np.array([1 if label == 'positive' else 0 for label in labels_split])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PdrBjtJrK1El"
   },
   "outputs": [],
   "source": [
    "non_zero_idx = [ii for ii, review in enumerate(reviews_ints) if len(review) != 0]\n",
    "\n",
    "# remove 0-length reviews and their labels\n",
    "reviews_ints = [reviews_ints[ii] for ii in non_zero_idx]\n",
    "encoded_labels = np.array([encoded_labels[ii] for ii in non_zero_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Cc1aFvln64Mc"
   },
   "outputs": [],
   "source": [
    "review_lengths = np.array(list(map(len,reviews_ints)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AWbPpr5L84vl"
   },
   "outputs": [],
   "source": [
    "def pad_features(reviews_ints, seq_length):\n",
    "    ''' Return features of review_ints, where each review is padded with 0's \n",
    "        or truncated to the input seq_length.\n",
    "    '''\n",
    "    \n",
    "    # getting the correct rows x cols shape\n",
    "    features = np.zeros((len(reviews_ints), seq_length), dtype=int)\n",
    "\n",
    "    # for each review, I grab that review and \n",
    "    for i, row in enumerate(reviews_ints):\n",
    "        features[i, :len(row)] = np.array(row)[:seq_length]\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1seckIQI84yV"
   },
   "outputs": [],
   "source": [
    "\n",
    "seq_length = max(review_lengths)\n",
    "features = pad_features(reviews_ints,seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QNAInDEFOAoo"
   },
   "outputs": [],
   "source": [
    "review_lengths = np.sum(features!=0,axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ml4DG7pj70gr"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 7191,
     "status": "ok",
     "timestamp": 1584313270692,
     "user": {
      "displayName": "Daniel Cuiñas Vázquez",
      "photoUrl": "",
      "userId": "11552885780691803973"
     },
     "user_tz": -60
    },
    "id": "ubGsP3ze846n",
    "outputId": "624f96a9-3b79-49e2-f71b-45c2ad654e4d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tFeature Shapes:\n",
      "Train set: \t\t(20000, 2514) \n",
      "Validation set: \t(2500, 2514) \n",
      "Test set: \t\t(2500, 2514)\n"
     ]
    }
   ],
   "source": [
    "split_frac = 0.8\n",
    "\n",
    "## split data into training, validation, and test data (features and labels, x and y)\n",
    "\n",
    "split_idx = int(len(features)*split_frac)\n",
    "train_x, remaining_x = features[:split_idx], features[split_idx:]\n",
    "train_y, remaining_y = encoded_labels[:split_idx], encoded_labels[split_idx:]\n",
    "train_length, remaining_length=review_lengths[:split_idx], review_lengths[split_idx:]\n",
    "\n",
    "test_idx = int(len(remaining_x)*0.5)\n",
    "val_x, test_x = remaining_x[:test_idx], remaining_x[test_idx:]\n",
    "val_y, test_y = remaining_y[:test_idx], remaining_y[test_idx:]\n",
    "val_legth, test_length = remaining_length[:test_idx], remaining_length[test_idx:]\n",
    "\n",
    "## print out the shapes of your resultant feature data\n",
    "print(\"\\t\\t\\tFeature Shapes:\")\n",
    "print(\"Train set: \\t\\t{}\".format(train_x.shape), \n",
    "      \"\\nValidation set: \\t{}\".format(val_x.shape),\n",
    "      \"\\nTest set: \\t\\t{}\".format(test_x.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "19Oc6GBp7CXb"
   },
   "outputs": [],
   "source": [
    "train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_length), torch.from_numpy(train_y))\n",
    "valid_data = TensorDataset(torch.from_numpy(val_x), torch.from_numpy(val_legth), torch.from_numpy(val_y))\n",
    "test_data = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_length), torch.from_numpy(test_y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "raetxkx0A-y1"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UGhwKqoz_jHt"
   },
   "outputs": [],
   "source": [
    "\n",
    "BATCH_SIZE = 64\n",
    "# make sure the SHUFFLE your training data\n",
    "train_loader = DataLoader(train_data, shuffle=True,batch_size=BATCH_SIZE)\n",
    "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(test_data, shuffle=True,batch_size=BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "98Z8Z0HkBE51"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence, PackedSequence\n",
    "\n",
    "class WordAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, n_layers, att_size, output_dim, dropout):\n",
    "        super(WordAttention, self).__init__()\n",
    "\n",
    "        self.emb = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        self.rnn = nn.GRU(embedding_dim,\n",
    "                          hidden_dim, \n",
    "                          num_layers=n_layers, \n",
    "                          bidirectional=True,\n",
    "                          dropout = 0 if n_layers < 2 else dropout,\n",
    "                          batch_first=True)\n",
    "        \n",
    "        self.att = nn.Linear(2 * hidden_dim, att_size)\n",
    "        \n",
    "        self.context_vector = nn.Linear(att_size, 1, bias=False)\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "\n",
    "    def forward(self, text, text_lengths):\n",
    "        #text = [batch size, sent len]\n",
    "\n",
    "        text=text.long()\n",
    "        text_lengths = text_lengths.long()\n",
    "\n",
    "        embedded = self.dropout(self.emb(text))  \n",
    "        \n",
    "        #embedded = [batch size, sent len, emb dim]\n",
    "        \n",
    "        packed_words = pack_padded_sequence(embedded, text_lengths, batch_first=True, enforce_sorted=False)\n",
    "        \n",
    "        packed_words, _ = self.rnn(packed_words)\n",
    "        \n",
    "        #packed_words.data = [n words, hid dim * num directions]\n",
    "        \n",
    "        att_w = torch.tanh(self.att(packed_words.data))\n",
    "        \n",
    "        #att_w = [n words, att size]\n",
    "            \n",
    "        att_w = self.context_vector(att_w).squeeze(1)\n",
    "        \n",
    "        #att_w = [n words]\n",
    "        \n",
    "        max_value = att_w.max()  # scalar, for numerical stability during exponent calculation\n",
    "        \n",
    "        att_w = torch.exp(att_w - max_value)\n",
    "        \n",
    "        #att_w = [n words]\n",
    "        \n",
    "        att_w, _ = pad_packed_sequence(PackedSequence(data=att_w,\n",
    "                                                      batch_sizes=packed_words.batch_sizes,\n",
    "                                                      sorted_indices=packed_words.sorted_indices,\n",
    "                                                      unsorted_indices=packed_words.unsorted_indices),\n",
    "                                       batch_first=True)  \n",
    "        \n",
    "        #att_w = [batch size, max(text_lengths)]\n",
    "        \n",
    "        word_alphas = att_w / torch.sum(att_w, dim=1, keepdim=True)\n",
    "        \n",
    "        #word_alphas = [batch size, max(text_lengths)]\n",
    "        \n",
    "        sentences, _ = pad_packed_sequence(packed_words,\n",
    "                                           batch_first=True)\n",
    "        \n",
    "        #sentences = [batch size, max(text_lengths), hid dim * num directions ]\n",
    "        \n",
    "        sentences = sentences * word_alphas.unsqueeze(2)\n",
    "        \n",
    "        #sentences = [batch size, max(text_lengths), hid dim * num directions ]\n",
    "        \n",
    "        sentences = sentences.sum(dim=1)\n",
    "        \n",
    "        #sentences = [batch size, hid dim * num directions]\n",
    "        \n",
    "        output = self.fc(sentences)\n",
    "        \n",
    "        #output = [batch size, output dim]\n",
    "        \n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9oYVWH0XBGkC"
   },
   "outputs": [],
   "source": [
    "INPUT_DIM = len(vocab_to_int)\n",
    "EMBEDDING_DIM = 100\n",
    "OUTPUT_DIM = 1\n",
    "HIDDEN_DIM = 100\n",
    "N_LAYERS = 1\n",
    "ATT_DIM = 100\n",
    "DROPOUT = 0.5\n",
    "\n",
    "model=WordAttention(INPUT_DIM, \n",
    "                    EMBEDDING_DIM, \n",
    "                    HIDDEN_DIM, \n",
    "                    N_LAYERS, \n",
    "                    ATT_DIM,\n",
    "                    OUTPUT_DIM,\n",
    "                    DROPOUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 7119,
     "status": "ok",
     "timestamp": 1584313270695,
     "user": {
      "displayName": "Daniel Cuiñas Vázquez",
      "photoUrl": "",
      "userId": "11552885780691803973"
     },
     "user_tz": -60
    },
    "id": "I42_JQotBdIf",
    "outputId": "c659d03d-436e-4591-8f7c-e4ef26b7b2c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 7,548,901 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VBiXNow2Bfnn"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DFXtoYDnBhWf"
   },
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bGB7ke1kBjrf"
   },
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "\n",
    "    #round predictions to the closest integer\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float() #convert into float for division \n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "my6h7j7WBlcM"
   },
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for text, text_lengths, label in iterator:\n",
    "        text, text_lengths, label=text.cuda(), text_lengths.cuda(), label.cuda()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "      \n",
    "        predictions = model(text, text_lengths).squeeze(1)\n",
    "        \n",
    "        loss = criterion(predictions, label.float())\n",
    "        \n",
    "        acc = binary_accuracy(predictions, label.float())\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_lcZGBiNBqJO"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for text, text_lengths, label in iterator:\n",
    "            text, text_lengths, label=text.cuda(), text_lengths.cuda(), label.cuda()\n",
    "            \n",
    "            predictions = model(text, text_lengths).squeeze(1)\n",
    "            \n",
    "            loss = criterion(predictions, label.float())\n",
    "            \n",
    "            acc = binary_accuracy(predictions, label.float())\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cv3c5gpyBq96"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 252250,
     "status": "ok",
     "timestamp": 1584313620130,
     "user": {
      "displayName": "Daniel Cuiñas Vázquez",
      "photoUrl": "",
      "userId": "11552885780691803973"
     },
     "user_tz": -60
    },
    "id": "hxt5pJZPBzPI",
    "outputId": "cf5bb55a-52a1-418d-b8a7-da0c017037cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 0m 48s\n",
      "\tTrain Loss: 0.569 | Train Acc: 69.61%\n",
      "\t Val. Loss: 0.527 |  Val. Acc: 76.05%\n",
      "Epoch: 02 | Epoch Time: 0m 49s\n",
      "\tTrain Loss: 0.418 | Train Acc: 81.03%\n",
      "\t Val. Loss: 0.460 |  Val. Acc: 82.93%\n",
      "Epoch: 03 | Epoch Time: 0m 49s\n",
      "\tTrain Loss: 0.340 | Train Acc: 85.44%\n",
      "\t Val. Loss: 0.393 |  Val. Acc: 85.04%\n",
      "Epoch: 04 | Epoch Time: 0m 49s\n",
      "\tTrain Loss: 0.281 | Train Acc: 88.60%\n",
      "\t Val. Loss: 0.400 |  Val. Acc: 87.19%\n",
      "Epoch: 05 | Epoch Time: 0m 49s\n",
      "\tTrain Loss: 0.250 | Train Acc: 89.93%\n",
      "\t Val. Loss: 0.371 |  Val. Acc: 87.73%\n",
      "Epoch: 06 | Epoch Time: 0m 49s\n",
      "\tTrain Loss: 0.222 | Train Acc: 91.34%\n",
      "\t Val. Loss: 0.417 |  Val. Acc: 86.95%\n",
      "Epoch: 07 | Epoch Time: 0m 49s\n",
      "\tTrain Loss: 0.198 | Train Acc: 92.32%\n",
      "\t Val. Loss: 0.409 |  Val. Acc: 87.54%\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 20\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "counter = 0\n",
    "patience = 2\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train(model, train_loader, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_loader, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "      best_valid_loss = valid_loss\n",
    "      torch.save(model.state_dict(), 'tut2-model.pt')\n",
    "      counter = 0 \n",
    "    else:\n",
    "      counter += 1\n",
    "      if counter >= patience:\n",
    "          break\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 489,
     "status": "ok",
     "timestamp": 1584313620614,
     "user": {
      "displayName": "Daniel Cuiñas Vázquez",
      "photoUrl": "",
      "userId": "11552885780691803973"
     },
     "user_tz": -60
    },
    "id": "8a164kIdCViH",
    "outputId": "67b17949-955a-4777-ff3e-1427fe16766c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.405 | Test Acc: 86.88%\n"
     ]
    }
   ],
   "source": [
    "# model.load_state_dict(torch.load('tut2-model.pt'))\n",
    "test_loss, test_acc = evaluate(model, test_loader, criterion)\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOAgiEyVzYu/BvZTIslZLxC",
   "collapsed_sections": [],
   "name": "WordAttention2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
